---
title: "12-logit-appendix"
author: "mcc"
date: "4/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Logit-20 Training Using 20 Features

```{r 43,message=FALSE, warning=FALSE}
# Load Libraries
Libraries <- c("knitr", "readr", "tidyverse", "caret")
for (p in Libraries) { 
    library(p, character.only = TRUE)
}
```

```{r 44,include=FALSE}
opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE)
```

```{r 45}
# Import relevant data
c_m_TRANSFORMED <- read_csv("./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0", "1")),
                                             PID = col_skip(),
                                             TotalAA = col_skip()))
```

```{r 46, cache=TRUE}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)
training_set.1 <- c_m_TRANSFORMED[index, ]
```

- The `test.set.1` and `Class.test` data sets are not produced since the Logit run with 20 features was not deemed useful. The reason for its dismissal was that is contained extraneous features.

- The first training run is to determine if all 20 features (amino acids) are necessary for our logistic regression model.

```{r 47, cache=TRUE}
set.seed(1000)
start_time <- Sys.time()     # Start timer

# Create model, 10X fold CV repeated 5X
tcontrol <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5)

model_obj.1 <- train(Class ~ .,
                     data = training_set.1,
                     trControl = tcontrol,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()      # End timer
end_time - start_time       # Display time
```

### Logit-20 Summary #1

```{r 48, cache=TRUE}
summary(model_obj.1)
```

The Akaike information criterion (AIC)[^44] for model #1 is 699.72. This will be used later to compare the models generated to rate their ability to utilize the features best.
- The list of probabilities for the estimates leaves us with only **9 important features** to try re-modeling, R, H, P, C, E, Y, T, D, G.

[^44]:https://en.wikipedia.org/wiki/Akaike_information_criterion

## Logit-9 Training Using 9 Features

- This test uses **ONLY** 9 features: (R, H, P, C, E, Y, T, D, G)

```{r 49, cache=TRUE}
# Data import & handling
c_m_9aa <- read_csv("./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                    col_types = cols(Class = col_factor(levels = c("0", "1")),
                                     A = col_skip(), F = col_skip(),
                                     I = col_skip(), K = col_skip(),
                                     L = col_skip(), M = col_skip(),
                                     N = col_skip(), PID = col_skip(),
                                     Q = col_skip(), V = col_skip(),
                                     S = col_skip(), TotalAA = col_skip(),
                                     W = col_skip()))
```

```{r 410, cache=TRUE}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE)

training_set.2 <- c_m_9aa[ index, ]
test_set.2     <- c_m_9aa[-index, ]

Class_test.2 <- as.factor(test_set.2$Class)
```

```{r 411, cache=TRUE}
set.seed(1000)
start_time <- Sys.time()          # Start timer

# Create model, 10X fold CV repeated 5X
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           savePredictions = "final") # IMPORTANT: Saves predictions

model_obj.2 <- train(Class ~ .,
                     data = training_set.2,
                     trControl = fitControl,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()           # End timer
end_time - start_time            # Display time
```



### Logit-9 Summary

```{r 412, cache=TRUE}
summary(model_obj.2)
```



### Logit-9 Confusion Matrix

```{r 413, cache=TRUE}
Predicted_test_vals <- predict(model_obj.2, test_set.2[, -1])

confusionMatrix(Predicted_test_vals, Class_test.2, positive = "1")
```

- The Akaike information criterion (AIC) for model #2 is 708.96. This will be used later to compare the models generated to rate their ability to utilize the features best.
- The number of unique false-positives and false-negatives is 26.

### Obtain List of False Positives & False Negatives From Logit-9

```{r 414, cache=TRUE}
fp_fn_logit <- model_obj.2 %>% pluck("pred") %>% dplyr::filter(obs != pred)

# Write CSV in R
write.table(fp_fn_logit,
            file = "./00-data/03-ml_results/fp_fn_logit.csv",
            row.names = FALSE,
            na = "",
            col.names = TRUE,
            sep = ",")

nrow(fp_fn_logit) ## NOTE: NOT UNIQUE NOR SORTED
```

- The logistic regression second test produced 536 protein samples, which are either false-positives or false-negatives. The list of 536 proteins may have duplicates. Therefore they are NOT UNIQUE NOR SORTED.
