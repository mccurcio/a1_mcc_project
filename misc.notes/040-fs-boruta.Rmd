# Feature Selection Using 'Boruta'

## Summary

Dimensionality Reduction is highly valuable tool to elicit when using datasets of any size. Boruta is a features selection tool which uses


Let us look at a simple example of how the number of dimension in which our data resides can greatly effect the work needed to done in some cases. 

Imagine our data rests on a single dimension number line. In our imaginary dataset we may have 9 data poiints which may rest in 3 units measures, i.e. 9 data points / 3 units measures. Now let us consider that our dataset has a constant number of data points but *NOW* we plan to describe the dataset by using two dimensions. Our current dataset is now spread over 9 units square measures, leaving anywhere from 0 to 8 squares empty. If our dataset now resides in 3-dimensinal space, it is then possible to have 0 to 26 unit cubes empty.  

Depending on the data we are investigating, this may impact the work in several ways;

1. There may be an increase in empty blocks in our database. This is known as a sparse matrix.

It will then be posssible to have between 1 and 9 unit squares filled, or between 0 and 8 squares empty. Continuing to higher dimensionality datasets we see that the number of number cubes 



As many first year statistics students have no doubt learned to use linear regression and analysis of variance (ANOVA) 

As mentioned in the section of principal component analysis
We have already discussed feature extraction using  however there are many other methods for determining which 
FROM Boruta pdf, implementing a novel feature selection
algorithm for finding all relevant variables. The algorithm is designed as a wrapper around
a Random Forest classification algorithm. It iteratively removes the features which are
proved by a statistical test to be less relevant than random probes.

- Mention curse of dimensionality
- Random forest
- How to calc importance?


There are ten common fearture selection methods for R:
1. Boruta
2. Variable Importance from Machine Learning Algorithms
3. Lasso Regression
4. Step wise Forward and Backward Selection ***********
5. Relative Importance from Linear Regression
6. Recursive Feature Elimination (RFE) *******************
7. Genetic Algorithm
8. Simulated Annealing
9. Information Value and Weights of Evidence
10. DALEX Package
Conclusion

See:  
1. https://www.machinelearningplus.com/machine-learning/feature-selection/  
2. https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/  
3. https://www.datacamp.com/community/tutorials/feature-selection-R-boruta  
4. https://www.datasciencecentral.com/profiles/blogs/select-important-variables-using-boruta-algorithm
5. https://www.rdocumentation.org/packages/Boruta/versions/6.0.0/topics/Boruta  
6. https://pypi.org/project/Boruta/  

Reference:
Miron B. Kursa, Witold R. Rudnicki (2010). Feature Selection with the Boruta Package. Journal of Statistical Software, 36(11), p. 1-13. URL: https://www.jstatsoft.org/article/view/v036i11

```{r message=FALSE, warning=FALSE, include=FALSE}
## Libraries & Initialization

knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = FALSE)

Libraries = c("doMC", "Boruta")

for(p in Libraries){  # Install if not present
    if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
    library(p, character.only = TRUE)
}

## Import data & data handling
c_m_transformed <- read.csv("c_m_transformed.csv")
c_m_transformed <- c_m_transformed[, -c(2,3)]
Class <- as.factor(c_m_transformed$Class) # Convert ‘Class’ To Factor

```

### Perform Boruta search

```{r cache=TRUE}
set.seed(1000)
registerDoMC(cores = 3) # Start multi-processor mode
start_time <- Sys.time() # Start timer

boruta_model <- Boruta(Class ~ ., 
                       data = c_m_transformed,
                       p = 0.01,
                       mcAdj = TRUE,
                       doTrace = 1)  

registerDoSEQ()  # Stop multi-processor mode
end_time <- Sys.time()   # End timer
end_time - start_time    # Display elapsed time
```

### Display Model Results

- `boruta_model` object details

```{r}
names(boruta_model)
```

```{r eval=FALSE, include=FALSE}
## `boruta_model` final decision

boruta_model$finalDecision
```

## Variable Importance Scores

```{r}
roughFixMod <- TentativeRoughFix(boruta_model)

imps <- attStats(roughFixMod)

imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]

imps2[order(-imps2$meanImp), ]  # descending sort
```

### Plot variable importance

```{r echo=FALSE}
plot(boruta_model, 
     cex.axis = 1,
     las = 2, 
     ylim = c(-5,50),
     main = "Variable Importance Via Boruta (Bigger = Better)")
```

## Summary
