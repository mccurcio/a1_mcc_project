# Classification And Regression Trees - CART


## Summary

CART is developed by Breiman, Friedman, Olshen, & Stone in 1984 (Book - Classification and Regression Trees)

C5.0 uses Shannon's information gain

CART uses GINI index

https://techdifferences.com/difference-between-algorithm-and-pseudocode.html

Difference Between Algorithm and Pseudocode

Comparison Chart

 Pseudocode

| Basis for comparison | Algorithm | Pseudocode |
|:---------------------|:-------------------|:------------------|
| Comprehensibility    | Hard to understand | Easy to interpret |
| Uses | Complicated programming language | Combination of programming language and natural language |
| Debugging            | Moderate           | Simpler |
| Ease of construction | Complex            | Easier |

## Pseudocode

Inputs:

1. R: a set of non-target attributes,  
1. C: the target attribute,  
1. S: training data.

Output:

1. Returns a decision tree

Start:

1. Initialize to empty tree;
2. If $S$ is empty then {
        Return a single node failure value
   }
3. If $S$ is made only for the values of the same target then {
       Return a single node of this value
   }

4. If $R$ is empty then {
       Return a single node with value as the most common value of the target attribute values found in $S$
   }
5. Assign D = the attribute that has the largest Gain ($D, S$) from all the attributes of $R$.
6. Assign ${d_{j} = 1, 2, ..., m}$ = Attribute values of $D$

7. ${S_j: j = 1, 2, ..., m}$ = The subsets of $S$ respectively constituted of djrecords attribute value $D$.

8. Return a tree whose root is D and the arcs are labeled by {d1, d2, ..., dm} and going to sub-trees ID3 (R-{D}, C, S1), ID3 (R-{D} C, S2), .., ID3 (R-{D}, C, Sm)

End

**OR**

TAKEN FROM mitchell-dectrees.pdf

**CART Pseudocode:**  

**Input:**  
1. $(X, ~Y)$ is a labeled pair,  
    
where:  

$\langle X \rangle ~~ is~ a~ matrix~ \in R^n, ~~ \langle Y \rangle ~~ is~ a~ vector~ \in R^n$, 

$Y \leftarrow ~ label~y_i \in \{1, 2, 3, ...,C\}~~$ such that $~~f(x_i, x_j, ...) = ~~label~y_i$,

**Output:**  
1. node is a class which has properties values, childs, and next.  
2. root is first/top node in the decision tree.  

**Declare:**  
1. root = CART $(X, ~Y, ~root)$  
2. Entropy:  H$(x) = -p_{(+)} log_2 p_{(+)} ~-~ p_{(-)} log_2 p_{(-)}$

---

1. Input: $X=$ Matrix(m, n), $~~~y_i=$ Vector(y), $~~$root   
2. initialize node as a new node instance  
3. if row $x_i$ has only single classification $C$, then  
    + insert label $C$ into node  
    + return node  
4. if $x_i = null$, then   
    + insert dominant label in $x_i$ into node  
    + return node  
5. best_feature is an feature with maximum Entropy in $x_i$  
6. insert feature best_feature into node   
7. for $(v_i$ in values of best_feature)   
   + For example, protein category has 7 values: {Ctrl, Ery, Hcy, Hhe, Hmb, Lgb, Mgb}    
   + insert value $v_i$ as branch of node  
   + create $v_i[Rows, ~~]$ with rows that only contains value $v_i$  
8. if ($v_i[Rows, ~~] = null$, then)    
   + node branch ended by a leaf with value which is dominant label in $x_i$ 
   
   else    
   + $new_y$ = list of features $y_i$ with best_feature removed  
   + nextNode = next node connected by this branch  
   + nextNode = CART ($v_i[Rows, ~~]$, $new_y$, label, nextNode)   
   + return node  
End  

## Hyper-parameters

## Big O

## Model Training & Tuning

Libraries
```{r, message=FALSE, warning=FALSE}
Libraries = c("doMC", "caret", "rpart", "beepr")

for(p in Libraries){  # Install if not present
    if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
    library(p, character.only = TRUE)
}
```

Import data & data handling

```{r}
test_harness_paa <- read.csv("data/test_harness_paa.csv")
test_harness_paa <- test_harness_paa[, -c(2,3)]
Class <- as.factor(test_harness_paa$Class) # Convert ‘Class’ To Factor
```

Partition data - training / testing sets
```{r}
set.seed(1000)
index <- createDataPartition(test_harness_paa$Class, p = 0.8, list = FALSE)

training_set <- test_harness_paa[ index,]
test_set     <- test_harness_paa[-index,]

Class_test <- as.factor(test_set$Class)
```

### Run Training Set 
 
```{r cache=TRUE}
set.seed(1000)
registerDoMC(cores = 3) # Start multi-processor mode
start_time <- Sys.time() # Start timer

tcontrol <- trainControl(method = "repeatedcv",
                         number = 5, # 10X fold CV repeated 5X
                         repeats = 3,
                         allowParallel = TRUE)

model_list <- train(Class ~ .,
                    data = training_set,
                    methodList = "rpart",
                    trControl = tcontrol)

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ() # Stop multi-processor mode
```

### Display Model Results

Model Summary
```{r}
model_list
```

## Predictions - Run Test Set

 Predict new samples (Class_test)
 
```{r}
Predicted_test_vals <- predict(model_list, test_set[,-1])

length(Predicted_test_vals)
```

Quick Summary

```{r}
summary(Predicted_test_vals)
```

Confusion Matrix

```{r}
confusionMatrix(Predicted_test_vals, Class_test)
```



