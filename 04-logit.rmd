
# Logistic Regression For Binary Classification

```{r 41,include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

## Introduction

For individuals who have studied biology or biochemistry, Logistic Regression may be familiar as dose-response curves[^46], enzyme kinetic curves[^47], median lethal dose curve (LD-50)[^48] and the exponential growth curve, given a set of limited resources[^49]. 

[^46]:https://www.merckmanuals.com/professional/clinical-pharmacology/pharmacodynamics/dose-response-relationships
[^47]:https://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics
[^48]:https://en.wikipedia.org/wiki/Median_lethal_dose
[^49]:https://www.khanacademy.org/science/biology/ecology/population-growth-and-regulation/a/exponential-logistic-growth

When we discuss predictive modeling, Logistic Regression is used as a binary classifier. This is quite useful when qualitative or categorical values need to found using a set of input or independent variable(s) with the prediction being either absence or presence of a disease for example.


binary classifier that toggles between qualitative variables, often referred to as categorical values, or even the logical values of zero or one. 

Logistic regression (Logit) derives its name from its similarity to linear regression, even though in this case the formula does not produce a regression line but instead outputs a logical value, {0, 1}. Despite its name Logistic Regression is really a classifier, moving between two classes, zero and one, as we shall see below. 

"We have stated that linear regression is not appropriate in the case of a
qualitative response."

The input or independent variable(s) for Logit, may be a matrix ($X^{m ~x~ n}$) in the set of real numbers, ($X ~ \in ~ \Re$). While, the output of a Logistic Regression is represented by {0, 1}. Therefore, the output of Logit is NOT IN the set of real numbers, ($Y ~ \notin ~ \Re$).

\begin{equation}
f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ X < 0 \\ 1 ~~for~~ X \geq 0 \end{matrix} \right. ~~~where ~~X ~\in ~\Re
\end{equation}

Using Logistic Regression, we may calculate the presence or absence of a product or quality that we wish to model given a difficult situation where the transition is not clear.

In the figure below, the function's domain, $X \in \{-\infty$ to $\infty\}$, whereby its range is {0, 1}. In the figure, the *decision boundary* is $x ~=$ 0, denoted by the *red dotted line*. At the inflection point the curves range changes from *zero*, absence, to *one*, the presence of quality or item.

```{r 42,echo=FALSE, fig.align="center", fig.height=3.5}
x <- seq(-6, 6, 0.05)
y <- 1 / (1 + exp(-x))

plot(x, y,
type = "l",
main = "Logistic Curve",
ylim = c(-0.3, 1.3))
abline(h = 0, col = "blue")
abline(v = 0, col = "red", lty = 3, lwd = 3)
abline(h = 1, col = "blue", lty = 3)
text(-3.5, 0.48, cex = 1.75, "if x < 0 then y = 0")
text(3.5, 0.48, cex = 1.75, "if x >= 0 then y = 1")
```

The logistic growth curve is commonly denoted by:

\begin{equation}
f(x) ~=~ \frac{M}{1 + Ae^{-r(x - x_0)}}
\end{equation}

where $M$ is the curve's maximum value, $r$ is the maximum growth rate (also called the Malthusian parameter [^41]), $x_0$ is the midpoint of the curve, $A$ is the number of times that the initial population must double to reach $M$.[^42]

[^41]:https://en.wikipedia.org/wiki/Malthusian_growth_model
[^42]:https://en.wikipedia.org/wiki/Logistic_function

In the specific case of *Logistic Regression for Binary Classification* where we have a probability between 0 and 1, $M$, and $A$ take on the value one.

\begin{equation}
f(x) ~=~ \frac{1}{1 + e^{-(WX+b)}}
\end{equation}

Since the logistic equation is exponential, it is easier to work with the formula in terms of its odds or *log-odds*. Odds are the probabilities of success over failure denoted as $\Large \frac{p}{1-p}$ and more importantly, in this situation, log-odds are $ln \left (\frac{p}{1-p} \right )$.

Simply by using log-odds, logistic regression may be more easily expressed as a set of linear equations in x.[^43] Hence we can now go from linear regression to logistic regression.

[^43]:http://juangabrielgomila.com/en/logistic-regression-derivation/

Step #1:
\begin{equation}
Odds ~=~ \frac{Probability ~of ~Success}{Probability ~of ~Failure} ~=~ \frac{p}{1-p} ~=~ \left ( \frac{Pr(Y ~=~ 1|X)}{Pr(Y ~=~ 0|X)} \right )
\end{equation}


\begin{equation}
 =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}
\end{equation}

Step #2:
Substitute ($p$ for $Pr(y_i ~=~ 1|x_i)$) and ($1-p$ for $Pr(y_i ~=~ 0|x_i)$) and change notation to summation on the right hand side;

\begin{equation}
ln \left( \frac {p}{1-p} \right) =~ \sum_i^{k} \beta_i x_i
\end{equation}

Step #3:
Eliminate the natural log by taking the exponent on both sides;

\begin{equation}
\frac {p}{1-p} =~ exp \left ( \sum_i^{k} \beta_i x_i \right )
\end{equation}

Step #4:
Substitute $u = \sum_i^{k} \beta_i x_i$;

\begin{equation}
\frac {p}{1-p} =~ e^u
\end{equation}

Step #5:
Rearrange to solve for $\large p$;

\begin{equation}
p(u) ~=~ \frac{e^u}{1 + e^u}
\end{equation}


---



## Logit Conclusion

Logit is easy to implement and understand and can be used for feature selection.

Considering the table Logit Models, below, it is clear that model #2 with nine features best describes the better of the two models.

Akaike Information Criterion [^45]

[^45]:https://en.wikipedia.org/wiki/Akaike_information_criterion

\begin{equation}
AIC ~=~ 2 K ~-~ 2ln (\widehat{L})
\end{equation}

Where $ln (\widehat{L})$ is the log-likelihood estimate, $K$ is the number of parameters.

#### Two Logit Models {-}

| Model # | Features |  AIC   |
| :-----: | :------: | :----: |
|    1    |    20    | 699.72 |
|    2    |    9     | 708.96 |

Logit is a common machine learning method. It is easy to understand and explain. This supervised binary classification method is very useful for determining the importance of the features which can be applied. As we saw in Model#1, there were 11 features that had probabilities of the estimates used above the 5% threshold cut-off. In Model#2, only nine features were used to describe the model, and the AIC increased by 9.24.

The nine features which best described the logistic regression model were R, H, P, C, E, Y, T, D, G. If we compare this to the Boruta test carried out in the EDA, we find the overlap interesting.

#### Comparison of Boruta Vs Logit: Order of Importance {-}

|Test|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|
|:--:|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|Boruta|R|H|P|K|C|E|Y|T|S|A|V|U|I|F|D|G|N|L|M|Q|
|Logit-9|R|H|P|.|C|E|Y|T|.|.|.|.|.|.|D|G|.|.|.|.|

The first 7 out of 8 amino acid features are seen in the proper order, as described by the Boruta Random Forest model. This is confirmation that Logit can pick up the importance of features similar to Boruta.

Logit produced 536 proteins, which are false-negatives or false-positives. It should be noted that the 536 are NOT UNIQUE NOR SORTED. The number of unique FN/FP from the confusion matrix is 26. These proteins will be investigated further in the Outliers chapter, which compares these FN/FP proteins to the PCA outliers.

The two tests for Logit (using 20 then 9 features) is interesting. This shows that Logit is an alternatvie way of choosing the importance of features.  As can be seen in the table "*Comparison of Boruta Vs Logit: Order of Importance*" it was seen that the first seven features lined up very closely with the Boruta Random Forest feature order of importance.

