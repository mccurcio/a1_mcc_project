
# Logistic Regression For Binary Classification

```{r 41,include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

## Introduction

For individuals who have studied cell biology or biochemistry, logistic regression may be familiar as dose-response curves[^46], enzyme kinetic curves[^47], median lethal dose curve (LD-50)[^48] or even an exponential growth curve given limited resources[^49]. 

[^46]:https://www.merckmanuals.com/professional/clinical-pharmacology/pharmacodynamics/dose-response-relationships
[^47]:https://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics
[^48]:https://en.wikipedia.org/wiki/Median_lethal_dose
[^49]:https://www.khanacademy.org/science/biology/ecology/population-growth-and-regulation/a/exponential-logistic-growth

However, in the context of predictive modeling, Logistic Regression is used as a binary classifier that toggle between the logical values of zero or one.

Logistic regression (Logit) derives its name from its similarity to linear regression, as we shall see below. The input/independent variable for Logit is the set of real numbers, ($X ~ \in ~ \Re$). While, the output of a Logistic Regression is not represented by {0, 1}, ($Y ~ \notin ~ \Re$),

\begin{equation}
f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0 \\ 1 ~~for~~ x \geq 0 \end{matrix} \right. ~~~where ~~x ~\in ~\Re
\end{equation}

Using Logistic Regression, we may calculate the presence or absence of a product or quality that we wish to model given a difficult situation where the transition is not clear.

In the figure below, the function's domain, $X \in \{-\infty$ to $\infty\}$, whereby its range is {0, 1}. In the figure, the *decision boundary* is $x ~=$ 0, denoted by the *red dotted line*. At the inflection point the curves range changes from *zero*, absence, to *one*, the presence of quality or item.

```{r 42,echo=FALSE, fig.align="center", fig.height=3.5}
x <- seq(-6, 6, 0.05)
y <- 1 / (1 + exp(-x))

plot(x, y,
type = "l",
main = "Logistic Curve",
ylim = c(-0.3, 1.3))
abline(h = 0, col = "blue")
abline(v = 0, col = "red", lty = 3, lwd = 3)
abline(h = 1, col = "blue", lty = 3)
text(-3.5, 0.48, cex = 1.75, "if x < 0 then y = 0")
text(3.5, 0.48, cex = 1.75, "if x >= 0 then y = 1")
```

The logistic growth curve is commonly denoted by:

\begin{equation}
f(x) ~=~ \frac{M}{1 + Ae^{-r(x - x_0)}}
\end{equation}

where $M$ is the curve's maximum value, $r$ is the maximum growth rate (also called the Malthusian parameter [^41]), $x_0$ is the midpoint of the curve, $A$ is the number of times that the initial population must double to reach $M$.[^42]

[^41]:https://en.wikipedia.org/wiki/Malthusian_growth_model
[^42]:https://en.wikipedia.org/wiki/Logistic_function

In the specific case of *Logistic Regression for Binary Classification* where we have a probability between 0 and 1, $M$, and $A$ take on the value one.

\begin{equation}
f(x) ~=~ \frac{1}{1 + e^{-(WX+b)}}
\end{equation}

Since the logistic equation is exponential, it is easier to work with the formula in terms of its odds or *log-odds*. Odds are the probabilities of success over failure denoted as $\Large \frac{p}{1-p}$ and more importantly, in this situation, log-odds are $ln \left (\frac{p}{1-p} \right )$.

Simply by using log-odds, logistic regression may be more easily expressed as a set of linear equations in x.[^43] Hence we can now go from linear regression to logistic regression.

[^43]:http://juangabrielgomila.com/en/logistic-regression-derivation/

Step #1:
\begin{equation}
ln ~ \left ( \frac{Pr(y_i ~=~ 1|x_i)}{Pr(y_i ~=~ 0|x_i)} \right ) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}
\end{equation}

Step #2:
Substitute ($p$ for $Pr(y_i ~=~ 1|x_i)$) and ($1-p$ for $Pr(y_i ~=~ 0|x_i)$) and change notation to summation on the right hand side;

\begin{equation}
ln \left( \frac {p}{1-p} \right) =~ \sum_i^{k} \beta_i x_i
\end{equation}

Step #3:
Eliminate the natural log by taking the exponent on both sides;

\begin{equation}
\frac {p}{1-p} =~ exp \left ( \sum_i^{k} \beta_i x_i \right )
\end{equation}

Step #4:
Substitute $u = \sum_i^{k} \beta_i x_i$;

\begin{equation}
\frac {p}{1-p} =~ e^u
\end{equation}

Step #5:
Rearrange to solve for $\large p$;

\begin{equation}
p(u) ~=~ \frac{e^u}{1 + e^u}
\end{equation}

Step #6:
Incidentally, to find the probabilities, take the derivative of both sides using quotient rule;

\begin{equation}
p'(u) ~=~ \frac {(e^u)(1 + e^u) - (e^u)(e^u)}{(1 + e^u)^2}
\end{equation}

Step #7:
Simplify;

\begin{equation}
p'(u) ~=~ \frac {e^u}{(1 + e^u)^2}
\end{equation}

Step #8:
Separate out to produce two fractions;

\begin{equation}
p'(u) ~=~ \left ( \frac {e^u}{1 + e^u} \right ) \cdot \left ( \frac{1}{1 + e^u} \right )
\end{equation}

Step #9:
Substitute our previous success and failure variables back into place;

\begin{equation}
p'(u) ~=~ p(u) \cdot ( 1 - p(u))
\end{equation}

Now we can calculate the probabilities as well as the values for any given x value.

---



## Logit Conclusion

Logit is easy to implement and understand and can be used for feature selection.

Considering the table Logit Models, below, it is clear that model #2 with nine features best describes the better of the two models.

Akaike Information Criterion [^45]

[^45]:https://en.wikipedia.org/wiki/Akaike_information_criterion

\begin{equation}
AIC ~=~ 2 K ~-~ 2ln (\widehat{L})
\end{equation}

Where $ln (\widehat{L})$ is the log-likelihood estimate, $K$ is the number of parameters.

#### Two Logit Models {-}

| Model # | Features |  AIC   |
| :-----: | :------: | :----: |
|    1    |    20    | 699.72 |
|    2    |    9     | 708.96 |

Logit is a common machine learning method. It is easy to understand and explain. This supervised binary classification method is very useful for determining the importance of the features which can be applied. As we saw in Model#1, there were 11 features that had probabilities of the estimates used above the 5% threshold cut-off. In Model#2, only nine features were used to describe the model, and the AIC increased by 9.24.

The nine features which best described the logistic regression model were R, H, P, C, E, Y, T, D, G. If we compare this to the Boruta test carried out in the EDA, we find the overlap interesting.

#### Comparison of Boruta Vs Logit: Order of Importance {-}

|Test|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|
|:--:|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|Boruta|R|H|P|K|C|E|Y|T|S|A|V|U|I|F|D|G|N|L|M|Q|
|Logit-9|R|H|P|.|C|E|Y|T|.|.|.|.|.|.|D|G|.|.|.|.|

The first 7 out of 8 amino acid features are seen in the proper order, as described by the Boruta Random Forest model. This is confirmation that Logit can pick up the importance of features similar to Boruta.

Logit produced 536 proteins, which are false-negatives or false-positives. It should be noted that the 536 are NOT UNIQUE NOR SORTED. The number of unique FN/FP from the confusion matrix is 26. These proteins will be investigated further in the Outliers chapter, which compares these FN/FP proteins to the PCA outliers.

The two tests for Logit (using 20 then 9 features) is interesting. This shows that Logit is an alternatvie way of choosing the importance of features.  As can be seen in the table "*Comparison of Boruta Vs Logit: Order of Importance*" it was seen that the first seven features lined up very closely with the Boruta Random Forest feature order of importance.

