
The negative sign is simply a matter of convention. If one is interested in maximizing your performance then the negative will do nicely because it inverts the paraboloid making it an ascent otherwise use the no sign and one can carry out a minimization instead or gradient descent. 

The negative simply turns this parabolic function around so that now you're looking for a maximum and you can simply solve a maximum problem Ax Men. But your performance will be increasing. If we look at this in terms of an XY graph or a X being W1 and a why being W2 we would simply find that we have a contour map that is formed by the performance function. 


If we look at A specific situation where we are climbing To a maximum In a gradient descent one would simply Go move in every direction. And calculate the differences. Adding those differences together. To get the Final direction to climb for our gradient descent. If we were to do this, this would be a huge problem. This is a very expensive problem computationally. Actually, it is intractable problem because We would be trying to maximize. And number of neurons using this Exponentially Expanding Calculation of gradient descent Well, there are several ways to Look at this mathematically. And one way might be to 

Let's consider the simplest neural network of two neurons. We have a series of X has a vector of exes. Input into a Or transformed by x w a vector of W's 

If we consider this two neuron system we can see that this chain rule makes sense in a probabilistic manner as well.


Leading to P1 which is then an input into this activation function. Providing an output Of Y which then goes into our second. Summation Y times the vector of W-2s Producing P2 which is then fed into our second neuron or second activation energy activation function. Providing or producing the final desired output Z Vector Z 

So if we Look at this two neuron system. Well one alternate way to consider this is to look at the changes that are made along the way along each step of the way. So if we're interested in finding out what the change in Z is given the change of p 2 and the change of p 2 given W2 and the change of WW2 given y we can then set up a Set of partial derivatives. That explain or describe the system. Such that. 

The partial of P2 Divided by the partial W 2 times the partial of Z divided by the partial of P2 times the partial P divided by the partial of Z. In this formula 



We can break it apart so that we can calculate the partial of P divided by the partial Z looking at a performance function. We find that the partial of P divided by partial of Z is negative 1/2. Times 2 times Z DZ D minus Z Show proof as follows. 












---

BACK-PROP-LEARNING
AIMA3e
function BACK-PROP-LEARNING(examples, network) returns a neural network
https://github.com/aimacode/aima-pseudocode/blob/master/md/Back-Prop-Learning.md

---
