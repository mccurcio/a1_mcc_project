library(h2o)
library(rio)
library(doParallel)
library(viridis)
library(RColorBrewer)
library(tidyverse)
library(ggthemes)
library(knitr)
library(tidyverse)
library(caret)
library(caretEnsemble)
library(plotly)
library(lime)
library(plotROC)
library(pROC)

https://www.rpubs.com/mr148/316143


#h2o.init(nthreads=-1,enable_assertions = FALSE)
h2o.init(nthreads=-1)

localH2O = h2o.init(ip = 'localhost', port = 54321, nthreads = -1,max_mem_size = "8G")


# Calculate the number of cores
no_cores <- detectCores() - 1
cl<-makeCluster(no_cores)
registerDoParallel(cl)


Modeling

We use the h2o infracstructure to train our deep learning models and also perform the anomaly detection.


# convert data to H2OFrame
creditcard_h2o <- as.h2o(creditcard)


splits <- h2o.splitFrame(creditcard_h2o, 
                         ratios = c(0.6, 0.2), 
                         seed = 148)   #partition data into 60%, 20%, 20% chunks
train <- splits[[1]]
validation <- splits[[2]]
test <- splits[[3]]
outcome_name <- "Class"
features <- setdiff(colnames(train), outcome_name)



Autoencoders

An autoencoder is an artificial neural network used for unsupervised learning of efficient codings. The goal of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. The model will have to reduce the dimensionality of the input data (in this case, down to 5 nodes/dimensions). The autoencoder will learn the patterns in the credit card transactions to identify anomalies and similar transactions. Autoencoding reduces the feature space in order to distill the essential aspects of the data unlike most conventional deeplearning which blows up the feature space up to capture non-linearities and subtle interactions within the data. Autoencoding can also be seen as a non-linear alternative to PCA.
Activation and Loss Functions

The tanh function is a rescaled and shifted logistic function.Itâ€™s symmetry around 0 speeds up the training algorithm to converge faster.Another loss function avaialble in the h2o architecture is the rectified linear activation function. It has demonstrated high performance on image recognition tasks from practice , and is a more biologically accurate model of neuron activations.The third loss function available is Maxout which is a generalization of the Rectifier activation, where each neuron picks the larger output of k separate channels, each with its own weights and bias values.

We build our first model nelow. We train a deep learning model by setting the autoencoder equals to true. y should not be specified for autoencoders set to true.

 model_one = h2o.deeplearning(x = features, training_frame = train,
autoencoder = TRUE,
reproducible = TRUE,
 seed = 148,
 hidden = c(10,10,10), epochs = 100,activation = "Tanh",
validation_frame = test)


#model_id = " model_one")

We save the model to file to reuse later to avoid wasting time running each time.

h2o.saveModel(model_one, path="model_one", force = TRUE)

model_one <- h2o.loadModel("/Users/nanaakwasiabayieboateng/Documents/memphisclassesbooks/DataMiningscience/H20/model_one/DeepLearning_model_R_1507439493390_3791")
model_one

h2o.scoreHistory(model_one)%>%head()

#Convert to autoencoded representation
test_autoencoder <- h2o.predict(model_one, test)

train_features <- h2o.deepfeatures(model_one, train, layer = 2) %>%
  as.data.frame() %>%
  mutate(Class = as.vector(train[, 31]))


train_features%>%head()











