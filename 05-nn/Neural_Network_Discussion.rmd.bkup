---
title: "Neural Network Discussion"
author: "MCC"
date: "11/14/2019"
output: html_document

```{r}
library(knitr)
```

---

# Neural Network Discussion
---

If we are to discuss Neural Networks, we could start by considering several comparable systems that computers are trying to emulate by investigating the number of neurons in organisms from along earth's phylogenetic or evolutionary tree. See table 1.

### Table 1: Orgnaisms Vs Number of Neurons In Each[^1] {-}

Organism | Common Name | Approximate Number of Neurons
---------|-------------|------------------
C. elegans | roundworm | 302
Chrysaora fuscescens | jellyfish | 5,600
Apis linnaeus | honey bee | 960,000
Blattodea | family of cockroaches | 1,000,000
Mus musculus | mouse| 71,000,000
Felis silvetrestris |cat | 760,000,000
Canis lupus familiaris | dog | 2,300,000,000
Homo sapien sapien | human | 100,000,000,000

[^1]:https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons

Although this table portrays a powerful picture of the computing power of man and his/her evolutionary ancestors there is one other number worth noting. The table above does not describe the inter-connectivity between each neuron. It is currently thought that inter-connectivity of one neuron is less than 10^4 connections per neuron, thus resulting in a total of approximately 600 trillion synapses per human brain.[^2]

[^2]:Shepherd, G. M. (2004). The synaptic organization of the brain (5th ed.). USA: Oxford University Press, New York.

Although neurons have differing morphologies, the most common depiction with repsect to computer science discussions is the multipolar neuron, commonly found in the muscle wth a large dendritic tree and a long fibrous axon. See Figure 1.[^3]

[^3]:https://www.howstuffworks.com/

If we now consider the most complicated Neural Networks being used in computing, we find they contain only 100s of neurons per layer within 100s of layers, approximately 10^4 nodes total. So called dense layers in a neural network is one where each node or perceptron is connected with 100's inputs and outputs . This is a far cry from the 10^11 neurons inter-connected by 10^4 synapses per neuron in humans. 
knitr::include_graphics("pix/basicneurontypes.jpg")
![Figure 1: Four Ideograms Of Neurons](pix/basicneurontypes.jpg)


If we are to discuss *our* new discovery in terms of throughput we would find that it could be diagrammed in four sections.

![Nueron Model #1](pix/one.neuron.system.png)

Cell contains potassium or sodium and when the cell discharges the calcium is sent down the Axon. Which then signals vesicles in the Axons tip To release chemicals into the synaptic gap between axon and dendrite and the signal from one neuron to the next is made. If we look at this mathematically. Please see we can model this fairly simply. Such that the dendrites. Taken signals Calling down $X_1$ to $X_n$. Each signal may have a weight according to its importance. W12 W3 and those signals are then put into a summation function. This summation function then outputs its value to a threshold box. The threshold box or activation functional And you have a binary in and out binary signal coming in. Through the X's being waited through W's Send it to the summation function and finally to the threshold box. Where it decides to send a signal or not? Neurons work at all, and I think I'd fashion. The activation energy that was The activation function that We currently use. More commonly use was developed by Paul Werbos in 1974. The activation function that we currently use is a sigmoidal function.

$$\large f(x) ~=~ \frac{1}{1 + e^{-\beta x + c}}$$

where f of x equals 1 over 1 plus e to the minus bias (T) where Alpha can be a Lena Combination of Values X x's and W's 

$$\large \overrightarrow{z} = f \left( \overrightarrow{x}, \overrightarrow{w}, \overrightarrow{T} \right)$$

If we consider the brain for a moment. In a very simple sense what we have is a series of inputs. Our census taken signals from the eyes touch other areas. And send it to the brain where it is transformed and then the output Is a series of Z's a series of 

Probabilities are a series of Values that will call $\Large z$. 

The function Z or the Check that be Value of Z is simply a function of a vector of exes. , a vector of WS 

$$\large \overrightarrow{z} = f \left( \overrightarrow{x}, \overrightarrow{w} \right)$$

Sent through this activation sigmoid function. So in a sense if you have several neurons in a row, you have the input of f 1 Being an output of Z Z being fed into the second neuron. The output of that is being fed to the third neuron and so on and so forth. So you have this nested function nested set of functions Z equals F of 4 times F of 3 times F of 2 times F of 1 

$$\Large \overrightarrow{z} ~=~ \Large f^3 \left( \large f^2 \left( f^1 \left( \overrightarrow{x}, \overrightarrow{w}\right) \right) \right)$$

If we consider. If we desire to maximize or Model this performance With the function we might come up with something like 

Performance function equals negative the max magnitude of the vector of D minus the vector of Z magnitude squared. This is no more that a slightly modified mean squared error (MSE).
$$\large \mathbf{P} = -  \| \overrightarrow{d} - \overrightarrow{z} \|^2$$
The negative sign is simply a matter of convention. If one is interested in maximizing your performance then the negative will do nicely because it inverts the paraboloid making it an ascent otherwise use the no sign and one can carry out a minimization instead or gradient descent. 

The negative simply turns this parabolic function around so that now you're looking for a maximum and you can simply solve a maximum problem Ax Men. But your performance will be increasing. If we look at this in terms of an XY graph or a X being W1 and a why being W2 we would simply find that we have a contour map that is formed by the performance function. 


If we look at A specific situation where we are climbing To a maximum In a gradient descent one would simply Go move in every direction. And calculate the differences. Adding those differences together. To get the Final direction to climb for our gradient descent. If we were to do this, this would be a huge problem. This is a very expensive problem computationally. Actually, it is intractable problem because We would be trying to maximize. And number of neurons using this Exponentially Expanding Calculation of gradient descent Well, there are several ways to Look at this mathematically. And one way might be to 

Let's consider the simplest neural network of two neurons. We have a series of X has a vector of exes. Input into a Or transformed by x w a vector of W's 

If we consider this two neuron system we can see that this chain rule makes sense in a probailistic manner as well.

$$p(X) = \prod_{i=1}^n p( x_i | x_1, ..., x_n)$$


![Simple Neural Netowrk](pix/two.neuron.system.png)
Partial derivatives of Performance function
$$\frac{\delta Perf}{\delta X} = \frac{\delta Perf}{\delta z} \cdot \frac{\delta z}{\delta P_2} \cdot \frac{\delta P_2}{\delta w_2} \cdot \frac{\delta w_2}{\delta Y} \cdot \frac{\delta Y}{\delta P_1} \cdot \frac{\delta  P_1}{\delta w_1} \cdot \frac{\delta w_1}{\delta X}$$

$$= \frac{ \left\{ -\frac{1}{2} \Large \| d - z \|^2 \right\}} {\delta z} \cdot \frac{\delta~  \{ [1+e^{-w_2}]^{-1} \} }{\delta P_2} \cdot \frac{\delta P_2}{\delta w_2} \cdot \frac{\delta w_2}{\delta Y} \cdot \frac{\delta Y}{\delta P_1} \cdot \frac{\delta  P_1}{\delta w_1} \cdot \frac{\delta w_1}{\delta X}$$

Using the Performance function, P:
$$\mathbf{P} ~= - \frac{1}{2} \Large \| \overrightarrow{d} - \overrightarrow{z} \|^2$$



Leading to P1 which is then an input into this activation function. Providing an output Of Y which then goes into our second. Summation Y times the vector of W-2s Producing P2 which is then fed into our second neuron or second activation energy activation function. Providing or producing the final desired output Z Vector Z 

So if we Look at this two neuron system. Well one alternate way to consider this is to look at the changes that are made along the way along each step of the way. So if we're interested in finding out what the change in Z is given the change of p 2 and the change of p 2 given W2 and the change of WW2 given y we can then set up a Set of partial derivatives. That explain or describe the system. Such that. 

The partial of P2 Divided by the partial W 2 times the partial of Z divided by the partial of P2 times the partial P divided by the partial of Z. In this formula 

Partial derivatives of Performance function
$$\frac{\delta P}{\delta X} = \frac{1}{1} \cdot \frac{1}{1} \cdot \frac{1}{1} \cdot \frac{1}{1}$$

We can break it apart so that we can calculate the partial of P divided by the partial Z looking at a performance function. We find that the partial of P divided by partial of Z is negative 1/2. Times 2 times Z DZ D minus Z Show proof as follows. 
