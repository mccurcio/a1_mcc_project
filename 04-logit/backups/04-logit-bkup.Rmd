# Logistic Regression Model For Binary Classification

## Introduction (FIX)

IMPORTMNT: https://stats.stackexchange.com/questions/108995/interpreting-residual-and-null-deviance-in-glm-r)

For students which have not come across this equation, it is clearer if the exponential curve is shown side by side with the sigmoidal curve,

```{r}
x = seq(-6, 6, 0.05)
y = 1/(1 + exp(-x))

plot(x, y,
     type = "l",
     main = "Logistic Sigmoidal Curve",
     ylim = c(-0.3, 1.3))
abline(v=0, h=0, col="red")
abline(h=0, col="red")
abline(h=1, col="green")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
## Load libraries
Libraries = c("knitr", "doMC", "ggplot2", "readr", "caret", "dplyr", "tidyverse")

for (i in Libraries) { 
    library(i, character.only = TRUE) 
}
```

```{r echo=FALSE}
## Introductory/Explanatory Material
set.seed(1000)
x1 = c(rnorm(20, -3, 1), rnorm(20, 3, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- cbind(x1, y1)

plot(categorical_1, 
     ylim = c(-0.3, 1.3),
     xlim = c(-6, 6),
     col = "blue", 
     main = "Categorical Data Wrongly Plotted With Linear Regression", 
     ylab = "0 = NO ;  1 = YES",
     xlab = "X-Values")
#abline(a=0, b = 0.2, col = "red")
```


```{r}
x1 = c(rnorm(200, 3, 1), rnorm(200, 7, 1) )
y1 = c(rep(0, times=200), rep(1, times=200))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
                   data = categorical_1,
                   family = binomial(link = "logit"))

dummy_model
```

```{r}
plot(categorical_1, 
     ylim = c(-0.3, 1.3),
     col = "blue", 
     main = "Categorical Plotted With Logistic Regression", 
     ylab = "Answer: 0 = NO, 1 = YES",
     xlab = "X-Values")
```

If we simplify the right hand side and consider it the formula for the dependent variable we get;

$$ Log ~Odds :~~ ln \left( \frac {p}{1-p} \right) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}$$

$$ln \left( \frac {p_i}{1 - p_i} \right) = \sum_{i=0}^n  \beta x_i  ~+~ \alpha$$

If we rearrange the top log odds ratio so that we can determine the probability we find:
$$Pr(Protein~ is~ myoglobin) = \frac {exp (Y^*)} {exp (Y^*) + 1} = \frac {e^{Y^*}} {e^{Y^*} + 1}$$

As we can see the above formula for the probability satisfies the domain being (0 <= p <= 1).

## Interpretation of the coefficients for logistic regression

If we are looking for the midpoint for the logistic regression we can calculate

$$0.5 = \frac {e^{Y^*}} {e^{Y^*} + 1}$$

$$0.5 \cdot (e^{Y^*} + 1) =  e^{Y^*}$$

$$0.5 \cdot e^{Y^*} + 0.5 =  e^{Y^*}$$

$$0.5 =  0.5 \cdot e^{Y^*}$$

$$1 =  e^{Y^*}$$

$$ln 1 =  Y^*$$

$$Y^* = 0$$

Therefore when $Y^* = 0$ we find that the logistic regression give a probability of 0.5, or 50%.

With the average protein we find, See EDA section 1.x.x ??????????????????????????

```{r}
Y = 13.0 -54.5*0.0824 -39.9*0.049 -30.0*0.0619 + 82.2*0.0289-38.1*0.0383 -82.9*0.0382 -15.8*0.0619 -56.6*0.0484 -31.4*0.0364
Y
```
$$Pr =  \frac {e^{-2.41375}} {e^{-2.41375} + 1}$$
```{r}
Pr = (exp(Y)) / (exp(Y) + 1)
Pr
```

Therefore the average protein has only an 8.21 % chance of being myoglobin using this model.



## Logit Model #1 w/ 20 Features - Train #1

Import data & data handling
```{r}
setwd("~/Dropbox/a1_mcc_project/04-logit/")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1"))))

c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
```

Data  & partitioning - Train #1

- Split `c_m_20aa` data into training set (80%) and testing set (20%). 
- The percentage of myoglobin proteins versus controls in each set must be same in training and testing as they are for the entire dataset.
- This is handled by the`caret` library used here.
```{r}
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE) # Partition 80/20

train_set.1 <- c_m_20aa[ Index, ]
test_set.1  <- c_m_20aa[-Index, ]
```

### Logit Model #1 Train using 20 features
```{r, cache = TRUE}
fitControl <- trainControl(method = "repeatedcv", 
                           repeats = 5, 
                           number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # use multi-core
start_time <- Sys.time() # Start timer

model_logit.1 <- train(Class ~ .,
                       data = train_set.1,
                       trControl = fitControl,
                       method = "glm",
                       family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # stop multi-core
```

```{r}
model_logit.1
```


### Summary #1
```{r}
summary(model_logit.1)
```

### McFadden's pseudo $R^2$ calculations (ref:https://www.youtube.com/watch?v=C4N3_XJJ-jU&t=601s)
(REf:Frank E. Harrell, Jr.
Regression Modeling
Strategies
With Applications to Linear Models,
Logistic and Ordinal Regression,
and Survival Analysis
Second Edition
Springer International Publishing Switzerland 2015
deviance, Page 236, 449, 487, 516
ISBN 978-3-319-19425-7 (eBook)

---

ll.null <- logit$null.deviance / -2  
ll.proposed <- model_logit.1$deviance / -2  

Pseudo_R2 <- (ll.null - ll.proposed) / ll.null  
adjusted R 2  
```{r echo=FALSE}
ll.null <- model_logit.1$finalModel$null.deviance / -2

ll.proposed <- model_logit.1$finalModel$deviance / -2

adjusted_R2 <- (ll.null - ll.proposed) / ll.null

adjusted_R2
```



### Predicted Model #1
$$ln \left( \frac {p}{1-p} \right) = 18.038 -56.325 ~ [C] - 44.223 ~ [D] -29.599 ~ [G] + 83.914 ~ [H]$$
$$- 10.224 ~ [I] -51.151 [P] -94.063 ~ [R] -19.986 ~ [S] -49.619 ~ [T] -37.129 ~ [Y]$$

## Model Logit #2 w/ 9 Features

- Using only features: (C, D, G, H, P, R, S, T, Y)

Data import, handling & data partitioning
```{r}
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1"))))
# Remove TotalAA, PID & 11 features
c_m_9aa <- c_m_transformed[, -c(2,3,4,7,8,11,12,13,14,15,17,21,22)]  

Class <- as.factor(c_m_9aa$Class) # Convert ‘Class’ To Factor

set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE)
train_set.2 <- c_m_9aa[ Index, ]
test_set.2  <- c_m_9aa[-Index, ]

Class.train_set.2 <- as.factor(train_set.2$Class)
Class.test_set.2 <- as.factor(test_set.2$Class)
```

### Train #2 Logit w 9 Features
```{r, cache = TRUE}
fitControl <- trainControl(method = "repeatedcv", 
                           repeats = 5, 
                           number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

model_logit.2 <- train(Class ~ .,
                       data = train_set.2,
                       trControl = fitControl,
                       method = "glm",
                       family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

### Summary #2
```{r}
summary(model_logit.2)
```

### McFadden's pseudo $R^2$ calculations (ref:https://www.youtube.com/watch?v=C4N3_XJJ-jU&t=601s)

---

(REf:Frank E. Harrell, Jr.
Regression Modeling
Strategies
With Applications to Linear Models,
Logistic and Ordinal Regression,
and Survival Analysis
Second Edition
Springer International Publishing Switzerland 2015
deviance, Page 236, 449, 487, 516
ISBN 978-3-319-19425-7 (eBook)

---

Gareth James • Daniela Witten • Trevor Hastie
Robert Tibshirani
An Introduction to
Statistical Learning
with Applications in R
ISBN 978-1-4614-7138-7 (eBook)
Springer Science+Business Media New York 2013 (Corrected at 8 th printing 2017)
---

ll.null <- logit$null.deviance / -2  
ll.proposed <- model_logit.2$deviance / -2  

Pseudo_R2 <- (ll.null - ll.proposed) / ll.null  

```{r echo=FALSE}
ll.null <- model_logit.2$finalModel$null.deviance / -2

ll.proposed <- model_logit.2$finalModel$deviance / -2

adjusted_R2 <- (ll.null - ll.proposed) / ll.null

adjusted_R2
```



### Predicted Model #2
$$ln \left( \frac {p}{1 - p} \right) = 11.9 -52.2 [C] -38.4 [D] -28.3 [G] + 84.8 [H] -38.1 [P] -82.9 [R] -15.8 [S] -56.6 [T] -31.4 [Y]$$

## Collection of Outliers

### *Collection of Outliers* uses the complete dataset of 2340 proteins.  
- This is to gather the largest population of false negatives and positives for examination.  
- `savePredictions = "final"` in `trainControl` command saves predictions for later collection.  
```{r, cache = TRUE}
fitControl <- trainControl(method = "cv",
                           number = 5,
                           savePredictions = "final")
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

model_logit.3 <- train(Class ~ .,
                       data = c_m_9aa,
                       trControl = fitControl,
                       method = "glm",
                       family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

### `pluck("pred")` - Separate predictions where (obs != pred)
```{r}
outlier_sample <- model_logit.3 %>% 
                  pluck("pred") %>% 
                  dplyr::filter(obs != pred)

# out_of_sample
head(outlier_sample, n = 4)
```

### Inner-Join FN/FP row number to percent AAC
```{r}
fn_fp_list <- c_m_9aa %>% mutate(rowIndex = 1:nrow(c_m_9aa)) %>% 
                          inner_join(outlier_sample, by = "rowIndex")

head(fn_fp_list, n = 4)
```

```{r}
length(fn_fp_list)

fn_fp_list
```

## Confusion Matrix 

- ? `confusionMatrix(data = model_logit.3$pred, reference = model_logit.3$obs`)
```{r}
confusionMatrix(model_logit.3)
# print(results)
```



## Conclusion

model.1
   Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  657.72  on 1852  degrees of freedom
AIC: 699.72

model.2
    Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  701.04  on 1863  degrees of freedom
AIC: 721.04


Outliers------------------

Logit is good for parameter importance


