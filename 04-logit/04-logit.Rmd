# Logistic Regression For Binary Classification

## Introduction

For students who have studied cell biology and biochemistry, logistic regression may already be familiar. It may also be known as a *sigmoidal curve*, *dose-response curve*, *4-parameter fit*, a *median lethal dose curve* (LD-50) or even an exponential growth curve given limited resources. However to some biologists it is not generally thought of in terms of the decision boundary for binary classification.

In its simplest form we can use the formula given the Logistic / Sigmoidal Curve in the figure above.

$$f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0.5 \\ 1 ~~for~~ x \geq 0.5 \end{matrix} \right.$$

```{r echo=FALSE, fig.align="center"}
x = seq(-6, 6, 0.05)
y = 1/(1 + exp(-x))

plot(x, y,
     type = "l",
     main = "Logistic / Sigmoidal Curve",
     ylim = c(-0.3, 1.3))
abline(v = 0, h=0, col = "red")
abline(h = 0, col = "red")
```

The logistic curve can be denoted by:
    
$$f(x) ~=~ \Large \frac{M}{1 + e^{-r(t - t_0)}}$$

where $M$ is maximum value, $r$ is the maximum growth rate (sometimes called the Malthusian parameter), $t_0$ is the midpoint of the curve.[^41]

[^41]:https://en.wikipedia.org/wiki/Malthusian_growth_model

In linear regression we assume that this mean may be expressed as an equation linear in x (or some transformation of x or Y) such as
$$E(Y | x) = \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}$$
However with dichotomous data the conditional mean must be greater than of equal to zero and less than or equal to 1.

$$ln \left( \frac {p}{1-p} \right) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}$$





```{r message=FALSE, warning=FALSE}
## Load Libraries
Libraries <- c("doMC", "knitr", "readr", "tidyverse", "caret")

for (p in Libraries) {  # Install Library if not present
    if (!require(p, character.only = TRUE)) { install.packages(p)
        }
    library(p, character.only = TRUE)
}
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Import data & data handling
c_m_TRANSFORMED <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1")), 
                                             PID = col_skip(), TotalAA = col_skip()))
```

Note: Exploratory Data Analysis (EDA) has been carried out on the file `c_m_TRANSFORMED.csv`.  It can be found in the EDA chapter.*******(link to EDA)

```{r}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)

training_set.1 <- c_m_TRANSFORMED[index,]
#test_set.1     <- c_m_TRANSFORMED[-index,] # NOT needed since this run is not kept
#Class_test <- as.factor(test_set.1$Class) # NOT needed since this run is not kept
```

The `test.set.1` and `Class.test` data sets are not produced due to the fact that the Logit run with 20 features was not deamed useful.  The  reason for it dismissal was that is contained features which were extraneous.

## Logit Training #1 using 20 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-processor mode
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
tcontrol <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5)
                         
model_obj.1 <- train(Class ~ .,
                     data = training_set.1,
                     trControl = tcontrol,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ() # Stop multi-processor mode
```

### `glm/binomial`: Quick Summary

```{r}
model_obj.1
```

## `glm/binomial`: Summary #1

```{r}
summary(model_obj.1)
```

## Logit Training #2 using 9 Features

- Using **ONLY** features: (C, D, G, H, P, R, S, T, Y)

Data import & handling
```{r}
c_m_9aa <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
    col_types = cols(A = col_skip(), Class = col_factor(levels = c("0", "1")),
                     E = col_skip(), F = col_skip(),
                     I = col_skip(), K = col_skip(),
                     L = col_skip(), M = col_skip(),
                     N = col_skip(), PID = col_skip(),
                     Q = col_skip(), TotalAA = col_skip(),
                     V = col_skip(), W = col_skip()))
```

Partition data into training and testing sets
```{r}
set.seed(1000)
index <- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE)

training_set.2 <- c_m_9aa[ index, ]
test_set.2     <- c_m_9aa[-index, ]

Class_test.2 <- as.factor(test_set.2$Class)
```

## `glm/binomial`: Training #2 Logit with 9 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
fitControl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5,
                         savePredictions = "final") # IMPORTANT: Saves predictions

model_obj.2 <- train(Class ~ .,
                       data = training_set.2,
                       trControl = fitControl,
                       method = "glm",
                       family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

## `glm/binomial` Summary #2

```{r}
summary(model_obj.2)
```

## `glm/binomial`: Predict test_set.2

```{r, cache=TRUE}
Predicted_test_vals <- predict(model_obj.2, test_set.2[, -1])

summary(Predicted_test_vals)
```

## `glm/binomial`: Confusion Matrix #2

```{r}
confusionMatrix(Predicted_test_vals, Class_test.2, positive = "1")
```

## Obtain False Positives & False Negatives

```{r}
fp_fn_logit <- model_obj.2 %>% pluck("pred") %>% dplyr::filter(obs != pred)

# Write CSV in R
write.table(fp_fn_logit,
            file = "../20-outliers/fp_fn_logit.csv",
            row.names = FALSE,
            na = "",
            col.names = TRUE,
            sep = ",")

head(fp_fn_logit)
```

## Conclusion

model.1
   Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  657.72  on 1852  degrees of freedom
AIC: 699.72

model.2
    Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  701.04  on 1863  degrees of freedom
AIC: 721.04


Outliers------------------

Logit is good for parameter importance

