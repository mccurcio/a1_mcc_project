# Logistic Regression Model For Binary Classification

## Introduction

```{r message=FALSE, warning=FALSE}
## Load Libraries
rm(list = ls())
Libraries = c("doMC", "knitr", "readr", "tidyverse", "caret")

for(p in Libraries){  # Install Library if not present
    if(!require(p, character.only = TRUE)) { install.packages(p) }
    library(p, character.only = TRUE)
}
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

Import data & data handling
```{r}
c_m_TRANSFORMED <- read_csv("../00-data/aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1")), 
                                             PID = col_skip(), TotalAA = col_skip()))
#View(c_m_TRANSFORMED)
```

Note: Exploratory Data Analysis (EDA) has been carried out on the file `c_m_TRANSFORMED.csv`.  It's EDA can be found in chapter 3: EDA.

Partition data into training and testing sets
```{r}
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)

training_set.1 <- c_m_TRANSFORMED[ index,]
#test_set.1     <- c_m_TRANSFORMED[-index,] # NOT needed since this run is not kept

#Class_test <- as.factor(test_set.1$Class) # NOT needed since this run is not kept
```

The `test.set.1` and `Class.test` data sets are not produced due to the fact that the Logit run with 20 features was not deamed useful.  The  reason for it dismissal was that is contained features which were extraneous.


## Logit Training #1 using 20 Features
```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-processor mode 
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
tcontrol <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5)
                         
model_obj.1 <- train(Class ~ .,
                     data = training_set.1,
                     trControl = tcontrol,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ() # Stop multi-processor mode 
```

### `glm/binomial`: Quick Summary
```{r}
model_obj.1
```

## `glm/binomial`: Summary #1
```{r}
summary(model_obj.1)
```

## Logit Training #2 using 9 Features

- Using **ONLY** features: (C, D, G, H, P, R, S, T, Y)

Data import & handling
```{r}
c_m_9aa <- read_csv("../00-data/aac_dpc_values/c_m_TRANSFORMED.csv", 
    col_types = cols(A = col_skip(), Class = col_factor(levels = c("0", "1")), 
                     E = col_skip(), F = col_skip(), 
                     I = col_skip(), K = col_skip(), 
                     L = col_skip(), M = col_skip(), 
                     N = col_skip(), PID = col_skip(), 
                     Q = col_skip(), TotalAA = col_skip(), 
                     V = col_skip(), W = col_skip()))
#View(c_m_TRANSFORMED)
```

Partition data into training and testing sets
```{r}
set.seed(1000)
index <- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE)

training_set.2 <- c_m_9aa[ index,]
test_set.2     <- c_m_9aa[-index,]

Class_test.2 <- as.factor(test_set.2$Class)
```

## `glm/binomial`: Training #2 Logit with 9 Features
```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
fitControl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5,
                         savePredictions = "final") # IMPORTANT: Saves predictions for retreival

model_obj.2 <- train(Class ~ .,
                       data = training_set.2,
                       trControl = fitControl,
                       method = "glm",
                       family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

## `glm/binomial` Summary #2
```{r}
summary(model_obj.2)
```

## `glm/binomial`: Predict test_set.2
```{r, cache=TRUE}
Predicted_test_vals <- predict(model_obj.2, test_set.2[, -1])

summary(Predicted_test_vals)
```

## `glm/binomial`: Confusion Matrix #2
```{r}
confusionMatrix(Predicted_test_vals, Class_test.2, positive = "1")
```

## Obtain False Positives & False Negatives
```{r}
fp_fn_logit <- model_obj.2 %>% pluck("pred") %>% dplyr::filter(obs != pred)

# Write CSV in R
write.table(fp_fn_logit, 
            file = "../20-outliers/fp_fn_logit.csv", 
            row.names = FALSE, 
            na = "", 
            col.names = TRUE, 
            sep=",")

head(fp_fn_logit)
```



## Conclusion

model.1
   Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  657.72  on 1852  degrees of freedom
AIC: 699.72

model.2
    Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  701.04  on 1863  degrees of freedom
AIC: 721.04


Outliers------------------

Logit is good for parameter importance


