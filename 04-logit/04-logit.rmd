# Logistic Regression For Binary Classification

## Introduction

For students who have studied cell biology and biochemistry, logistic regression may already be familiar. It may also be known as a *sigmoidal curve*, *dose-response curve*, *4-parameter fit*, a *median lethal dose curve* (LD-50) or even an exponential growth curve is given limited resources. However, it can also be thought of a binary classifier that can toggle between off and on, zero or one.

Logistic regression derives its name from the similarity to linear regression, for reasons you will see below. In this circumstance it does not calculate a dependent variable $(y)$ as regression is normally thought to do but instead, Logistic Regression is used for classification between two states as follows.

$$f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0 \\ 1 ~~for~~ x \geq 0 \end{matrix} \right.$$

In this simple form we can use the generation of a zero or one as a binary or logical value, hence logistic, zero indicating the absence of some quality or item and one indicating its presence.

The domain ($x$) of our logistic equation may be ($-\infty$ to $\infty$), but at the **decision boundary** of $x ~=$ 0 (for this example) our system changes from *zero*, absence, to *one*, the presence of a quality or item. In the Logistic/Sigmoidal Curve figure below, the **decision boundary** is denoted by the *red dotted line*.

```{r echo=FALSE, fig.align="center"}
x <- seq(-6, 6, 0.05)
y <- 1 / (1 + exp(-x))

plot(x, y,
     type = "l",
     main = "Logistic / Sigmoidal Curve",
     ylim = c(-0.3, 1.3))
abline(h = 0, col = "blue")
abline(v = 0, col = "red", lty = 3, lwd = 3)
abline(h = 1, col = "blue", lty = 3)
text(-3.5, 0.48, cex = 1.75, "if x < 0 then y = 0")
text(3.5, 0.48, cex = 1.75, "if x >= 0 then y = 1")
```

The logistic growth curve is commonly denoted by:

$$f(x) ~=~ \frac{M}{1 + Ae^{-r(x - x_0)}}$$

where $M$ is the curve's maximum value, $r$ is the maximum growth rate (also called the Malthusian parameter[^41]), $x_0$ is the midpoint of the curve, $A$ is the number of times that the initial population must double to reach $M$.[^42]

[^41]:https://en.wikipedia.org/wiki/Malthusian_growth_model
[^42]:https://en.wikipedia.org/wiki/Logistic_function

In the specific case of *Logistic Regression for Binary Classification* where we have a probability between 0 and 1, $M$ and $A$ take on the value one.

---

Since the logistic equation is exponential it is easier to consider working with the formula in terms of the odds or log odds. Odds are the probabilities of success over failure denoted as $\Large \frac{p}{1-p}$ or log-odds as $\large ln \left (\frac{p}{1-p} \right )$.

In logistic regression, we find that the log-odds may be expressed as a set of linear equations in x. This is simply a transformation of the exponential curve to make it linear. The set of linear equations is similar to the idea inherent in Linear Regression.

$$ln ~ \left ( \frac{Pr(y_i ~=~ 1|x_i)}{Pr(y_i ~=~ 0|x_i)} \right ) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}$$

Substituting $p$ for $Pr(y_i ~=~ 1|x_i)$ and $1-p$ for $Pr(y_i ~=~ 0|x_i)$ we have:

$$ln \left( \frac {p}{1-p} \right) =~ \sum_i^{k} \beta_i x_i$$

Eliminate the natural log by taking the exponent on both sides:

$$\large \frac {p}{1-p} =~ exp \left ( \sum_i^{k} \beta_i x_i \right )$$

$$\large \frac {p}{1-p} =~ e^u ~~:~~ where ~~ u = \sum_i^{k} \beta_i x_i$$

Rearrange to solve for $\large p$ we find:

$$\large p(u) ~=~ \frac{e^u}{1 + e^u}$$

Take the derivative of both sides using quotient rule:

$$p'(u) ~=~ \frac {(e^u)(1 + e^u) - (e^u)(e^u)}{(1 + e^u)^2}$$

Simplify:

$$p'(u) ~=~ \frac {e^u}{(1 + e^u)^2}$$

Separate out to produce two fractions:

$$p'(u) ~=~ \left ( \frac {e^u}{1 + e^u} \right ) \cdot \left ( \frac{1}{1 + e^u} \right )$$

Substitute success and failure variables back into place.

$$p'(u) ~=~ p(u) \cdot ( 1 - p(u))$$
[^43]

[^43]:http://juangabrielgomila.com/en/logistic-regression-derivation/

```{r message=FALSE, warning=FALSE}
# Load Libraries
Libraries <- c("doMC", "knitr", "readr", "tidyverse", "caret")

for (p in Libraries) {  # Install Library if not present
    if (!require(p, character.only = TRUE)) { install.packages(p) }
    library(p, character.only = TRUE)
}
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Import data & data handling
c_m_TRANSFORMED <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0", "1")),
                                             PID = col_skip(),
                                             TotalAA = col_skip()))
```

Note: Exploratory Data Analysis (EDA) has been carried out on the file `c_m_TRANSFORMED.csv`.  It can be found in the EDA chapter.*******(link to EDA)

```{r}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)

training_set.1 <- c_m_TRANSFORMED[index, ]
#test_set.1     <- c_m_TRANSFORMED[-index,] # NOT needed since this run is not kept
#Class_test <- as.factor(test_set.1$Class) # NOT needed since this run is not kept
```

The `test.set.1` and `Class.test` data sets are not produced since that the Logit run with 20 features was not deemed useful. The reason for its dismissal was that is contained extraneous features.

## Logit Training #1 using 20 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-processor mode
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
tcontrol <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5)

model_obj.1 <- train(Class ~ .,
                     data = training_set.1,
                     trControl = tcontrol,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()  # End timer
end_time - start_time   # Display time
registerDoSEQ() # Stop multi-processor mode
```

## Logit Results #1

```{r}
summary(model_obj.1)
```

The Akaike information criterion (AIC) for model #1 is 699.72. This will be used later to compare the models generated to rate their ability to best utilize the features.
- The list of probabilities for the estimates leaves us with only **9 important features** to try re-modeling, R, H, P, C, E, Y, T, D, G.

## Logit Training #2 using 9 Features

- Using **ONLY** features: (R, H, P, C, E, Y, T, D, G)

```{r}
# Data import & handling
c_m_9aa <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                     col_types = cols(Class = col_factor(levels = c("0", "1")),
                                      A = col_skip(),
                                      F = col_skip(),
                                      I = col_skip(),
                                      K = col_skip(),
                                      L = col_skip(),
                                      M = col_skip(),
                                      N = col_skip(),
                                      PID = col_skip(),
                                      Q = col_skip(),
                                      V = col_skip(),
                                      S = col_skip(),
                                      TotalAA = col_skip(),
                                      W = col_skip()))
```

```{r}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE)

training_set.2 <- c_m_9aa[ index, ]
test_set.2     <- c_m_9aa[-index, ]

Class_test.2 <- as.factor(test_set.2$Class)
```

### Logit Training #2 with 9 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           savePredictions = "final") # IMPORTANT: Saves predictions

model_obj.2 <- train(Class ~ .,
                     data = training_set.2,
                     trControl = fitControl,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

## Logit Summary #2

```{r}
summary(model_obj.2)
```

## Logit Confusion Matrix #2

```{r, cache=TRUE}
Predicted_test_vals <- predict(model_obj.2, test_set.2[, -1])

confusionMatrix(Predicted_test_vals, Class_test.2, positive = "1")
```

- The Akaike information criterion (AIC) for model #2 is 708.96. This will be used later to compare the models generated to rate their ability to best utilize the features.
- The number of unique false-positives and false-negatives is 26.

## Obtain List of False Positives & False Negatives

```{r}
fp_fn_logit <- model_obj.2 %>% pluck("pred") %>% dplyr::filter(obs != pred)

# Write CSV in R
write.table(fp_fn_logit,
            file = "../00-data/03-ml_results/fp_fn_logit.csv",
            row.names = FALSE,
            na = "",
            col.names = TRUE,
            sep = ",")

nrow(fp_fn_logit) ## NOTE: NOT UNIQUE NOR SORTED
```

    536 proteins listed although they are NOT UNIQUE NOR SORTED.

## Conclusion

Logit is easy to implement and understand and can be used for parameter importance measurements.

Considering the Table Logit Models, below, it is clear that model #2 with 9 features best describes the better of the two models.

Akaike Information Criterion:

$$AIC ~=~ 2 K ~-~ ln (\widehat{L})$$

Where $ln (\widehat{L})$ is the log-likelihood, $K$ is the number of parameters.

| Model # | Features |  AIC   |
| :-----: | :------: | :----: |
|    1    |    20    | 699.72 |
|    2    |    9     | 708.96 |

Logit is a common machine learning method. It is easy to understand and explain. This supervised binary classification method is very useful for determining the importance of the features which can be applied. As we saw in Model#1 there were 11 features that had probabilities of the estimates used above the 5% threshold cut-off. In Model#2 only 9 features were used to describe the model and the AIC increased by 9.24.

The nine features which best described the logistic regression model were R, H, P, C, E, Y, T, D, G. If we compare this to the Boruta test carried out in the EDA we find the overlap interesting.

**Test Model / Order of Importance (From Left To Right)**

| Test Model | | | | | | | | | | | | | | | | | | | | |
| :--------: |:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|    Order   | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11| 12| 13| 14| 15| 16| 17| 18| 19| 20|
|   Boruta   | R   | H   | P   | K   | C   | E   | Y   | T   | S   | A   | V   | U   | I   | F   | D   | G   | N   | L   | M   | Q   |
|   Logit    | R   | H   | P   | .   | C   | E   | Y   | T   | .   | .   | .   | .   | .   |  .  | D   | G   | .   | .   | .   | .   |

The first 7 out of 8 amino acid features are seen in the proper order as described by the Boruta Random Forest model. This is confirmation that Logit can pick up the importance of features similar to Boruta.

Logit produced 536 proteins which are false-negatives or false-positives. It should be noted that the 536 are NOT UNIQUE NOR SORTED. The number of FN/FP from the confusion matrix is 26. These proteins will be investigated further in the Outliers section which compares these FN/FP proteins to the PCA outliers.

