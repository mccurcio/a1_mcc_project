# Logistic Regression For Binary Classification

## Introduction

For students who have studied cell biology and biochemistry, logistic regression may already be familiar. It may also be known as a *sigmoidal curve*, *dose-response curve*, *4-parameter fit*, a *median lethal dose curve* (LD-50) or even an exponential growth curve given limited resources. However it can also be thought of a binary classifier which can toggle between off and on, zero or one.

Logistic regression derives it name from the similarity to linear regression, for reasons you will see below. In this circumstance it does not calculate a dependent variable $(y)$ as a regression is normally thought to do but instead Logistic Regression is used for classification bewteen two states as follows.

$$f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0 \\ 1 ~~for~~ x \geq 0 \end{matrix} \right.$$

In this simple form we can use the generation of a zero or one as a binary or logical value, hence logistic, zero indicating the absence of some quality or item and one indicating its presence. 

The domain ($x$) of our logistic equation may be ($-\infty$ to $\infty$), but at the **decision boundary** of $x ~=$ 0 (for this example) our system changes from *zero*, absence, to *one*, the presence of a quality or item. In the Logistic/Sigmoidal Curve figure below, the **decision boundary** is denoted by the *red dotted line*.

```{r echo=FALSE, fig.align="center"}
x <- seq(-6, 6, 0.05)
y <- 1 / (1 + exp(-x))

plot(x, y,
     type = "l",
     main = "Logistic / Sigmoidal Curve",
     ylim = c(-0.3, 1.3))
abline(h = 0, col = "blue")
abline(v = 0, col = "red", lty = 3, lwd = 3)
abline(h = 1, col = "blue", lty = 3)
text(-3.5, 0.48, cex = 1.75, "if x < 0 then y = 0")
text(3.5, 0.48, cex = 1.75, "if x >= 0 then y = 1")
```

The logistic growth curve is commonly denoted by:
    
$$f(x) ~=~ \frac{M}{1 + Ae^{-r(x - x_0)}}$$

where $M$ is the curve's maximum value, $r$ is the maximum growth rate (also called the Malthusian parameter[^41]), $x_0$ is the midpoint of the curve, $A$ is the number of times that the initial population must double to reach $M$.[^42] 

[^41]:https://en.wikipedia.org/wiki/Malthusian_growth_model
[^42]:https://en.wikipedia.org/wiki/Logistic_function

In the specific case of *Logistic Regression for Binary Classification* where we have a probability between 0 and 1, $M$ and $A$ take on the value one.

---

Since the logistic equation is exponential it is easier to consider working with the formula in terms of the the odds or log odds. Odds are the probabilities of success over failure denoted as $\Large \frac{p}{1-p}$ or log odds = $\large ln \left (\frac{p}{1-p} \right )$.

In logistic regression we find that the log odds may be expressed as a set of linear equations in x. This is simply a transformation of exponential curve to make it linear. The set of linear equations is similar to the idea inherent in Linear Regression.

$$ln ~ \left ( \frac{Pr(y_i ~=~ 1|x_i)}{Pr(y_i ~=~ 0|x_i)} \right ) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}$$

Substituting $p$ for $Pr(y_i ~=~ 1|x_i)$ and $1-p$ for $Pr(y_i ~=~ 0|x_i)$ we have:

$$ln \left( \frac {p}{1-p} \right) =~ \sum_i^{k} \beta_i x_i$$

Eliminate the natural log by taking the exponent on both sides:

$$\large \frac {p}{1-p} =~ exp \left ( \sum_i^{k} \beta_i x_i \right )$$
 
$$\large \frac {p}{1-p} =~ e^u ~~:~~ where ~~ u = \sum_i^{k} \beta_i x_i$$

Rearrange to solve for $\large p$ we find:

$$\large p(u) ~=~ \frac{e^u}{1 + e^u}$$

Take the derivative of both sides using quotient rule:

$$p'(u) ~=~ \frac {(e^u)(1 + e^u) - (e^u)(e^u)}{(1 + e^u)^2}$$

Simplify:

$$p'(u) ~=~ \frac {e^u}{(1 + e^u)^2}$$

Separate out to produce two fractions:

$$p'(u) ~=~ \left ( \frac {e^u}{1 + e^u} \right ) \cdot \left ( \frac{1}{1 + e^u} \right )$$

Substitute success and failure variables back into place.

$$p'(u) ~=~ p(u) \cdot ( 1 - p(u))$$
[^43]

[^43]:http://juangabrielgomila.com/en/logistic-regression-derivation/

```{r message=FALSE, warning=FALSE}
# Load Libraries
Libraries <- c("doMC", "knitr", "readr", "tidyverse", "caret")

for (p in Libraries) {  # Install Library if not present
    if (!require(p, character.only = TRUE)) { install.packages(p) }
    library(p, character.only = TRUE)
}
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Import data & data handling
c_m_TRANSFORMED <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1")),
                                             PID = col_skip(),
                                             TotalAA = col_skip()))
```

Note: Exploratory Data Analysis (EDA) has been carried out on the file `c_m_TRANSFORMED.csv`.  It can be found in the EDA chapter.*******(link to EDA)

```{r}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)

training_set.1 <- c_m_TRANSFORMED[index, ]
#test_set.1     <- c_m_TRANSFORMED[-index,] # NOT needed since this run is not kept
#Class_test <- as.factor(test_set.1$Class) # NOT needed since this run is not kept
```

The `test.set.1` and `Class.test` data sets are not produced due to the fact that the Logit run with 20 features was not deamed useful. The reason for it dismissal was that is contained features which were extraneous.

## Logit Training #1 using 20 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-processor mode
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
tcontrol <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 5)

model_obj.1 <- train(Class ~ .,
                     data = training_set.1,
                     trControl = tcontrol,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()  # End timer
end_time - start_time   # Display time
registerDoSEQ() # Stop multi-processor mode
```

## Logit Results #1

```{r}
summary(model_obj.1)
```

## Logit Training #2 using 9 Features

- Using **ONLY** features: (C, D, E, G, H, P, R, T, Y)

```{r}
# Data import & handling
c_m_9aa <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                     col_types = cols(Class = col_factor(levels = c("0","1")),
                                      A = col_skip(),
                                      F = col_skip(),
                                      I = col_skip(),
                                      K = col_skip(),
                                      L = col_skip(),
                                      M = col_skip(),
                                      N = col_skip(),
                                      PID = col_skip(),
                                      Q = col_skip(),
                                      V = col_skip(),
                                      S = col_skip(),
                                      TotalAA = col_skip(),
                                      W = col_skip()))
```

```{r}
# Partition data into training and testing sets
set.seed(1000)
index <- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE)

training_set.2 <- c_m_9aa[ index, ]
test_set.2     <- c_m_9aa[-index, ]

Class_test.2 <- as.factor(test_set.2$Class)
```

### Logit Training #2 with 9 Features

```{r, cache = TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer

# Create model, 10X fold CV repeated 5X
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5,
                           savePredictions = "final") # IMPORTANT: Saves predictions

model_obj.2 <- train(Class ~ .,
                     data = training_set.2,
                     trControl = fitControl,
                     method = "glm",
                     family = "binomial")

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
```

## Logit Summary #2

```{r}
summary(model_obj.2)
```

## Logit Confusion Matrix #2

```{r, cache=TRUE}
Predicted_test_vals <- predict(model_obj.2, test_set.2[, -1])

confusionMatrix(Predicted_test_vals, Class_test.2, positive = "1")
```

## Obtain False Positives & False Negatives

```{r}
fp_fn_logit <- model_obj.2 %>% pluck("pred") %>% dplyr::filter(obs != pred)

# Write CSV in R
write.table(fp_fn_logit,
            file = "../20-outliers/fp_fn_logit.csv",
            row.names = FALSE,
            na = "",
            col.names = TRUE,
            sep = ",")

head(fp_fn_logit)
```

## Conclusion

model.1

AIC: 699.72

model.2
    Null deviance: 2593.68  on 1872  degrees of freedom
Residual deviance:  701.04  on 1863  degrees of freedom
AIC: 708.96


Logit is good for parameter importance

