c_m_20_PCA = prcomp(norm_c_m_20aa)
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
screeplot(c_m_20_PCA,
main = "Screeplot of c_m_20_PCA \n (Red Line Indicates Kaiser Rule, Eigenvalues = 1)",
npcs = 20,
type = "lines",
ylim = c(0, 7))
abline(h = 1, col = "red")
abline(v = 5, col = "green")
plot(cumsum(c_m_20_PCA$sdev^2 / sum(c_m_20_PCA$sdev^2)),
main = "Cumulative Proportion of Variance Vs Principle Component\n(Red line indicates p = 0.9)",
ylab = "% Variance(i) / Total Variance",
xlab = "Principal Component(i)",
ylim = c(0,1),
type="b")
abline(h = 0.9, col = "red")
abline(v = 12, col = "green")
autoplot(c_m_20_PCA,
data = c_m_transformed,
label.show.legend = F,
colour = 'Class',
shape = FALSE,
label = TRUE,
label.size = 4,
loadings = TRUE, loadings.label = TRUE, loadings.label.size = 7,
loadings.label.hjust = 2)
## Print Biplot1.annotated.png
## Biplot 1 for Conclusion discussion
png(filename = "Biplot1.annotated.png",
width = 6,
height = 6,
units = 'in',
res = 300)
autoplot(c_m_20_PCA,
data = c_m_transformed,
label.show.legend = F,
colour = 'Class',
shape = FALSE,
label = TRUE,
label.size = 4,
loadings = TRUE, loadings.label = TRUE, loadings.label.size = 7,
loadings.label.hjust = 2)
dev.off()
biplot(c_m_20_PCA,
cex = 0.75,
col = c("#bbbbbb", "#ff0000"),
cols <- c(1,2))
abline(v = 4.5, col = "blue", lw = 1)
abline(v = -15, col = "blue", lw = 1)
abline(h = 25, col = "blue", lw = 1)
abline(h = -24.5, col = "blue", lw = 1)
## Print 'Biplot.2pc1.pc2.png'
## Biplot 2 for Conclusion discussion
png(filename = "Biplot.2pc1.pc2.png",
width = 6,
height = 6,
units = 'in',
res = 300)
biplot(c_m_20_PCA,
cex = 0.75,
col = c("#bbbbbb", "#ff0000"),
cols <- c(1, 2))
abline(v = 4.5, col = "blue", lw = 1)
abline(v = -15, col = "blue", lw = 1)
abline(h = 25, col = "blue", lw = 1)
abline(h = -24.5, col = "blue", lw = 1)
dev.off()
outliers_PC1 <- which((c_m_20_PCA$x[, 1] > 5) | (c_m_20_PCA$x[, 1] < -5))
length(outliers_PC1)
outliers_PC2 <- which((c_m_20_PCA$x[, 2] > 3.5) | (c_m_20_PCA$x[, 2] < -3.5))
length(outliers_PC2)
total_pca_1_2_outliers <- union(outliers_PC1, outliers_PC2)
total_pca_1_2_outliers <- sort(total_pca_1_2_outliers)
remove <- c(182, 223, 236, 705)
mgb_pca_outliers <- setdiff(total_pca_1_2_outliers, remove)
length(mgb_pca_outliers)
mgb_pca_outliers
knitr::include_graphics("./Biplot1.best.annotated.png")
plot(cumsum(c_m_20_PCA$sdev^2 / sum(c_m_20_PCA$sdev^2)),
main = "Cumulative Proportion of Variance Vs Principle Component\n(Red line indicates p = 0.9, Green line indicates PC=12)",
ylab = "% Variance(i) / Total Variance",
xlab = "Principal Component(i)",
ylim = c(0,1),
type="b")
abline(h = 0.9, col = "red")
abline(v = 12, col = "green")
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = TRUE)
Libraries = c("knitr", "doMC", "ggplot2", "readr", "caret", "dplyr", "tidyverse")
for(p in Libraries){
library(p, character.only = TRUE)
}
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
setwd("~/Dropbox/a1_mcc_project/04-logit")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
setwd("~/Dropbox/a1_mcc_project/04-logit")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
setwd("~/Dropbox/a1_mcc_project/04-logit")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
x = seq(-1, 10 , 0.02)
y = 1/(1 + exp(-x+5))
plot(x, y,
type = "l",
main = "Logistic Profile",
ylim = c(-0.3, 1.3))
abline(v=0, h=0, col="red")
abline(h=0.5, col="green")
abline(h=1, col="green")
## Libraries
Libraries = c("doMC", "caret", "dplyr", "beepr", "ggplot2")
for(p in Libraries){  # Install if not present
if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
library(p, character.only = TRUE)
}
## Introductory/Explanatory Material
x1 = c(rnorm(20, 0, 1), rnorm(20, 5, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- cbind(x1, y1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
xlim = c(-2, 8),
col = "blue",
main = "Categorical Data Wrongly Plotted With Linear Regression",
ylab = "0 = NO, 1 = YES",
xlab = "X-Values")
abline(a=0, b = 0.2, col = "red")
x1 = c(rnorm(20, 3, 1), rnorm(20, 7, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
# get marginal effects
logit = seq(-3, 7, by = 0.1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
col = "blue",
main = "Categorical Plotted With Logistic Regression",
ylab = "Answer: 0 = NO, 1 = YES",
xlab = "X-Values")
Y = 13.0 -54.5*0.0824 -39.9*0.049 -30.0*0.0619 + 82.2*0.0289-38.1*0.0383 -82.9*0.0382 -15.8*0.0619 -56.6*0.0484 -31.4*0.0364
Y
Pr = (exp(Y)) / (exp(Y) + 1)
Pr
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = TRUE)
Libraries = c("knitr", "doMC", "ggplot2", "readr", "caret", "dplyr", "tidyverse")
for(p in Libraries){
library(p, character.only = TRUE)
}
setwd("~/Dropbox/a1_mcc_project/04-logit")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE) # Partition 80/20
train_set.1 <- c_m_20aa[ Index, ]
test_set.1  <- c_m_20aa[-Index, ]
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # use multi-core
start_time <- Sys.time() # Start timer
model_logit.1 <- train(Class ~ .,
data = train_set.1,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # stop multi-core
model_logit.1
summary(model_logit.1)
c_m_transformed <- read_csv("c_m_transformed.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
setwd("~/Dropbox/a1_mcc_project/04-logit/")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
x = seq(-1, 10 , 0.02)
y = 1/(1 + exp(-x+5))
plot(x, y,
type = "l",
main = "Logistic Profile",
ylim = c(-0.3, 1.3))
abline(v=0, h=0, col="red")
abline(h=0.5, col="green")
abline(h=1, col="green")
## Libraries
Libraries = c("doMC", "caret", "dplyr", "beepr", "ggplot2")
for(p in Libraries){  # Install if not present
if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
library(p, character.only = TRUE)
}
## Introductory/Explanatory Material
x1 = c(rnorm(20, 0, 1), rnorm(20, 5, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- cbind(x1, y1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
xlim = c(-2, 8),
col = "blue",
main = "Categorical Data Wrongly Plotted With Linear Regression",
ylab = "0 = NO, 1 = YES",
xlab = "X-Values")
abline(a=0, b = 0.2, col = "red")
x1 = c(rnorm(20, 3, 1), rnorm(20, 7, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
# get marginal effects
logit = seq(-3, 7, by = 0.1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
col = "blue",
main = "Categorical Plotted With Logistic Regression",
ylab = "Answer: 0 = NO, 1 = YES",
xlab = "X-Values")
Y = 13.0 -54.5*0.0824 -39.9*0.049 -30.0*0.0619 + 82.2*0.0289-38.1*0.0383 -82.9*0.0382 -15.8*0.0619 -56.6*0.0484 -31.4*0.0364
Y
Pr = (exp(Y)) / (exp(Y) + 1)
Pr
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = TRUE)
Libraries = c("knitr", "doMC", "ggplot2", "readr", "caret", "dplyr", "tidyverse")
for(p in Libraries){
library(p, character.only = TRUE)
}
setwd("~/Dropbox/a1_mcc_project/04-logit/")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE) # Partition 80/20
train_set.1 <- c_m_20aa[ Index, ]
test_set.1  <- c_m_20aa[-Index, ]
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # use multi-core
start_time <- Sys.time() # Start timer
model_logit.1 <- train(Class ~ .,
data = train_set.1,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # stop multi-core
model_logit.1
summary(model_logit.1)
c_m_transformed <- read_csv("c_m_transformed.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
# Remove TotalAA, PID & 11 features
c_m_9aa <- c_m_transformed[, -c(2,3,4,7,8,11,12,13,14,15,17,21,22)]
Class <- as.factor(c_m_9aa$Class) # Convert ‘Class’ To Factor
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE)
train_set.2 <- c_m_9aa[ Index, ]
test_set.2  <- c_m_9aa[-Index, ]
Class.train_set.2 <- as.factor(train_set.2$Class)
Class.test_set.2 <- as.factor(test_set.2$Class)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 5)
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer
model_logit.2 <- train(Class ~ .,
data = train_set.2,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
summary(model_logit.2)
fitControl <- trainControl(method = "cv",
number = 5,
savePredictions = "final")
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer
model_logit.3 <- train(Class ~ .,
data = c_m_9aa,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
outlier_sample <- model_logit.3 %>%
pluck("pred") %>%
dplyr::filter(obs != pred)
# out_of_sample
head(outlier_sample, n = 4)
fn_fp_list <- c_m_9aa %>% mutate(rowIndex = 1:nrow(c_m_9aa)) %>%
inner_join(outlier_sample, by = "rowIndex")
head(fn_fp_list, n = 4)
confusionMatrix(model_logit.3)
# print(results)
x = seq(-1, 10 , 0.02)
y = 1/(1 + exp(-x+5))
plot(x, y,
type = "l",
main = "Logistic Profile",
ylim = c(-0.3, 1.3))
abline(v=0, h=0, col="red")
abline(h=0.5, col="green")
abline(h=1, col="red")
## Introductory/Explanatory Material
x1 = c(rnorm(20, 0, 1), rnorm(20, 5, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- cbind(x1, y1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
xlim = c(-2, 8),
col = "blue",
main = "Categorical Data Wrongly Plotted With Linear Regression",
ylab = "0 = NO;  1 = YES",
xlab = "X-Values")
abline(a=0, b = 0.2, col = "red")
x1 = c(rnorm(20, 3, 1), rnorm(20, 7, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
x1 = c(rnorm(20, 3, 1), rnorm(20, 7, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
# get marginal effects
logit = seq(-3, 7, by = 0.1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
col = "blue",
main = "Categorical Plotted With Logistic Regression",
ylab = "Answer: 0 = NO, 1 = YES",
xlab = "X-Values")
x1 = c(rnorm(200, 3, 1), rnorm(200, 7, 1) )
y1 = c(rep(0, times=200), rep(1, times=200))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
# get marginal effects
logit = seq(-3, 7, by = 0.1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
col = "blue",
main = "Categorical Plotted With Logistic Regression",
ylab = "Answer: 0 = NO, 1 = YES",
xlab = "X-Values")
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
dummy_model
x1 = c(rnorm(200, 3, 1), rnorm(200, 7, 1) )
y1 = c(rep(0, times=200), rep(1, times=200))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
dummy_model
plot(categorical_1,
ylim = c(-0.3, 1.3),
col = "blue",
main = "Categorical Plotted With Logistic Regression",
ylab = "Answer: 0 = NO, 1 = YES",
xlab = "X-Values")
summary(model_logit.1)
x = seq(-10, 10 , 0.05)
y = 1/(1 + exp(-x))
plot(x, y,
type = "l",
main = "Logistic Sigmoidal Curve",
ylim = c(-0.3, 1.3))
abline(v=0, h=0, col="red")
abline(h=0, col="red")
abline(h=1, col="green")
ll.null <- model_logit.1$null.deviance / -2
ll.proposed <- model_logit.1$deviance / -2
Pseudo_R2 <- (ll.null - ll.proposed) / ll.null
Pseudo_R2
setwd("~/Dropbox/a1_mcc_project/04-logit/")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
## Libraries
Libraries = c("knitr", "doMC", "ggplot2", "readr", "caret", "dplyr", "tidyverse")
for (i in Libraries) {
library(i, character.only = TRUE)
}
## Introductory/Explanatory Material
set.seed(1000)
x1 = c(rnorm(20, -3, 1), rnorm(20, 3, 1) )
y1 = c(rep(0, times=20), rep(1, times=20))
categorical_1 <- cbind(x1, y1)
plot(categorical_1,
ylim = c(-0.3, 1.3),
xlim = c(-6, 6),
col = "blue",
main = "Categorical Data Wrongly Plotted With Linear Regression",
ylab = "0 = NO ;  1 = YES",
xlab = "X-Values")
#abline(a=0, b = 0.2, col = "red")
x1 = c(rnorm(200, 3, 1), rnorm(200, 7, 1) )
y1 = c(rep(0, times=200), rep(1, times=200))
categorical_1 <- as.data.frame(cbind(x1, y1))
dummy_model <- glm(y1 ~ x1,
data = categorical_1,
family = binomial(link = "logit"))
dummy_model
setwd("~/Dropbox/a1_mcc_project/04-logit/")
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
c_m_20aa <- c_m_transformed[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_20aa$Class)      # Convert ‘Class’ To Factor
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE) # Partition 80/20
train_set.1 <- c_m_20aa[ Index, ]
test_set.1  <- c_m_20aa[-Index, ]
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # use multi-core
start_time <- Sys.time() # Start timer
model_logit.1 <- train(Class ~ .,
data = train_set.1,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # stop multi-core
model_logit.1
summary(model_logit.1)
ll.null <- model_logit.1$null.deviance / -2
ll.proposed <- model_logit.1$deviance / -2
Pseudo_R2 <- (ll.null - ll.proposed) / ll.null
Pseudo_R2
model_logit.1$null.deviance
names(model_logit.1)
names(model_logit.1$coefnames)
model_logit.1$coefnames
model_logit.1$finalModel
names(model_logit.1$finalModel)
names(model_logit.1$finalModel$null.deviance)
model_logit.1$finalModel$null.deviance
model_logit.1$finalModel$deviance
ll.null <- model_logit.1$finalModel$null.deviance / -2
ll.proposed <- model_logit.1$finalModel$deviance / -2
Pseudo_R2 <- (ll.null - ll.proposed) / ll.null
Pseudo_R2
c_m_transformed <- read_csv("c_m_TRANSFORMED.csv",
col_types = cols(Class = col_factor(levels = c("0","1"))))
# Remove TotalAA, PID & 11 features
c_m_9aa <- c_m_transformed[, -c(2,3,4,7,8,11,12,13,14,15,17,21,22)]
Class <- as.factor(c_m_9aa$Class) # Convert ‘Class’ To Factor
set.seed(1000)
Index <- createDataPartition(Class, p = 0.8, list = FALSE)
train_set.2 <- c_m_9aa[ Index, ]
test_set.2  <- c_m_9aa[-Index, ]
Class.train_set.2 <- as.factor(train_set.2$Class)
Class.test_set.2 <- as.factor(test_set.2$Class)
fitControl <- trainControl(method = "repeatedcv",
repeats = 5,
number = 10)
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-core
start_time <- Sys.time() # Start timer
model_logit.2 <- train(Class ~ .,
data = train_set.2,
trControl = fitControl,
method = "glm",
family = "binomial")
end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()          # Stop multi-core
summary(model_logit.2)
ll.null <- model_logit.2$finalModel$null.deviance / -2
ll.proposed <- model_logit.2$finalModel$deviance / -2
adjusted_R2 <- (ll.null - ll.proposed) / ll.null
adjusted_R2
shrinkage_R2 = 1 - (1 - adjusted_R2)((1873-1)/(1873-9-1))
shrinkage_R2 <- 1 - (1 - adjusted_R2)((1873-1)/(1873-9-1))
shrinkage_R2 <- 1 - (1 - adjusted_R2)*((1873-1)/(1873-9-1))
shrinkage_R2
shrinkage_R2 <- 1 - (1 - adjusted_R2)*((1873-1)/(1873-9-1))
shrinkage_R2
shrinkage_R2_20 <- 1 - (1 - adjusted_R2)*((1873-1)/(1873-20-1))
shrinkage_R2_20
length(fn_fp_list, n = 4)
length(fn_fp_list)
fn_fp_list
View(dummy_model)
