---
title: "Use Autoencoder as a Feature Engineering Technique" 
subtitle: "For Killing Time at the Hospital"
---

Hands-On Machine Learning with R
* https://bradleyboehmke.github.io/HOML/

* https://rpubs.com/chidungkt/449797

* [Investigation and improvement of multi-layer perceptron neural networks for credit scoring](https://www.sciencedirect.com/science/article/pii/S0957417414007726)

# Title: Autoencoders TEST

## Introduction

```{r message=FALSE, warning=FALSE}
## Load Libraries
Libraries = c("doMC", "knitr", "readr", "tidyverse", "magrittr", "caret", "h2o")
for(p in Libraries){
    library(p, character.only = TRUE)
}
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Import data & data handling
```{r}
setwd("~/Dropbox/a1_mcc_project/05-autoe-nn")
c_m_TRANSFORMED <- read_csv("../00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv",
                            col_types = cols(Class = col_factor(levels = c("0","1")), 
                                             PID = col_skip(), TotalAA = col_skip()))
#View(c_m_TRANSFORMED)
```

Partition data into training and testing sets
```{r}
set.seed(1000)
index <- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE)

training_set <- c_m_TRANSFORMED[ index,]
test_set     <- c_m_TRANSFORMED[-index,]

Class_train <- as.factor(training_set$Class)
Class_test <- as.factor(test_set$Class)
```

# Default Random Forest

```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
# Activate h2o package for deep learning: 
h2o.init(nthreads = 3)   # max_mem_size = "16g")
#h2o.no_progress() # Progress Bar

# Convert to H2o Frame:
train <- as.h2o(training_set)
test <- as.h2o(test_set)

# Identify inputput and output: 
y <- Class_train
x <- setdiff(names(train), y)

# Train Default Random Forest: 
pure_nn <- h2o.randomForest(x = x, y = y, 
                            training_frame = train,
                            nfolds = 10, 
                            stopping_rounds = 5, 
                            stopping_metric = "AUC", 
                            seed = 29)


# Function collects results from cross-validation: 

results_df <- function(h2o_model) {
  h2o_model@model$cross_validation_metrics_summary %>% 
    as.data.frame() %>% 
    select(-mean, -sd) %>% 
    t() %>% 
    as.data.frame() %>% 
    mutate_all(as.character) %>% 
    mutate_all(as.numeric) %>% 
    select(Accuracy = accuracy, 
           AUC = auc, 
           Precision = precision, 
           Specificity = specificity, 
           Recall = recall, 
           Logloss = logloss) %>% 
    return()
  }


# Function presents results by graph: 

vis_results <- function(df_results) {
  df_results %>% 
    gather(Metrics, Values) %>% 
    ggplot(aes(Metrics, Values, fill = Metrics, color = Metrics)) +
    geom_boxplot(alpha = 0.3, show.legend = FALSE) + 
    facet_wrap(~ Metrics, scales = "free") + 
    scale_y_continuous(labels = scales::percent) + 
    theme_minimal() + 
    labs(x = NULL, y = NULL, 
         title = "Model Performance Based on Cross Validation")
}


pure_nn %>% results_df() %>% summary()
pure_nn %>% results_df() %>% vis_results()

```

 Accuracy  79%. 

# Autoencoder as a Feature Engineering Technique

# Build a autoencoder(Version 1)

```{r, fig.fullwidth=TRUE, fig.height=8, fig.width=12}
autoencoder <- h2o.deeplearning(x = x,
                                training_frame = train, 
                                autoencoder = TRUE, 
                                seed = 29, 
                                hidden = c(10, 20, 61), 
                                epochs = 30, 
                                activation = "Tanh")

train_autoen <- h2o.predict(autoencoder, train) %>% 
                as.data.frame() %>% 
                mutate(Class = df_train$Class) %>% 
                as.h2o()

test_autoen <- h2o.predict(autoencoder, test) %>% 
               as.data.frame() %>% 
               mutate(Class = df_test$Class) %>% 
               as.h2o()

nn_autoen_layers1 <- h2o.randomForest(x = setdiff(colnames(train_autoen), "Class"), 
                                      y = y, 
                                      training_frame = train_autoen,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 1000)


#  Use Autoencoder as Feature Engineering Method (Version 2)

train_features_l2 <- h2o.deepfeatures(autoencoder, train, layer = 2) %>%
                     as.data.frame() %>%
                     mutate(Class = df_train$Class) %>% 
                     as.h2o()


test_features_l2 <- h2o.deepfeatures(autoencoder, test, layer = 2) %>%
                    as.data.frame() %>%
                    mutate(Class = df_test$Class) %>% 
                    as.h2o()   


nn_autoen_layers2 <- h2o.randomForest(x = setdiff(colnames(train_features_l2), "Class"), 
                                      y = y, 
                                      training_frame = train_features_l2,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 29)

#  Use Autoencoder as Feature Engineering Method (Version 3)

train_features_l3 <- h2o.deepfeatures(autoencoder, train, layer = 3) %>%
                     as.data.frame() %>%
                     mutate(Class = df_train$Class) %>% 
                     as.h2o()


test_features_l3 <- h2o.deepfeatures(autoencoder, test, layer = 3) %>%
                    as.data.frame() %>%
                    mutate(Class = df_test$Class) %>% 
                    as.h2o()


nn_autoen_layers3 <- h2o.randomForest(x = setdiff(colnames(train_features_l3), "Class"), 
                                      y = y, 
                                      training_frame = train_features_l3,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 29)

#==========================
#  Compare between models
#==========================

do.call("bind_rows", lapply(list(pure_nn,
                                 nn_autoen_layers1, 
                                 nn_autoen_layers2, 
                                 nn_autoen_layers3), results_df)) -> df_com

df_com %<>% mutate(Model = rep(c("Original", "Layer1", "20Var", "61Var"), each = 10, time = 1))

theme_set(theme_minimal())

df_com %>% gather(a, b, -Model) %>% 
           ggplot(aes(Model, b, fill = Model, color = Model)) + 
           geom_boxplot(alpha = 0.3) + 
           scale_y_continuous(labels = scales::percent) + 
           facet_wrap(~ a, scales = "free") + 
           labs(x = NULL, y = NULL, title = "Model Performance")

df_com %>% group_by(Model) %>% 
           summarise_each(funs(mean), Accuracy, AUC, Logloss, Precision, Recall, Specificity) %>% 
           arrange(-Accuracy) %>% 
           mutate_if(is.numeric, function(x) {round(x, 3)}) %>% 
           knitr::kable()
```

Feature Engineering. 

# Optimal Threshold

```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
evaluate_func <- function(thre) {
    lapply(1:10, function(x) {
        set.seed(x)
        id <- createDataPartition(y = df_test$Class, p = 0.5, list = FALSE)
        test_df <- df_test[id, ]
  
        du_bao_prob <- h2o.predict(pure_nn, test_df %>% as.h2o()) %>% 
                       as.data.frame() %>% 
                       pull(Bad)
    
       du_bao <- case_when(du_bao_prob >= thre ~ "Bad", 
                        du_bao_prob < thre ~ "Good") %>% as.factor()
       cm <- confusionMatrix(du_bao, test_df$Class, positive = "Bad")
    
       bg_gg <- cm$table %>% 
                as.vector() %>% 
                matrix(ncol = 4) %>% 
                as.data.frame() %>% 
                rename(TP = V1, FN = V2, FP = V3, TN = V4)
       
       kq <- c(cm$overall, cm$byClass) 
       ten <- kq %>% as.data.frame() %>% row.names()
       
       kq %>% as.vector() %>% matrix(ncol = 18) %>% as.data.frame() -> all_df
       
       names(all_df) <- ten
       all_df <- bind_cols(all_df, bg_gg)
       return(all_df)
    })
  }

system.time(so_sanh_list <- lapply(seq(0.05, 0.8, by = 0.05), eval_fun))

so_sanh_df <- do.call("bind_rows", so_sanh_list) 

so_sanh_df %<>% mutate(Threshold = lapply(seq(0.05, 0.8, by = 0.05), 
                                          function(x) {rep(x, 10)}) %>% unlist())

theme_set(theme_minimal())
so_sanh_df %>% group_by(Threshold) %>% 
               summarise_each(funs(median), Accuracy, Kappa, Sensitivity, Specificity) %>% 
               gather(Metric, b, -Threshold) %>% 
               ggplot(aes(Threshold, b, color = Metric)) + 
               geom_line() + 
               geom_point(size = 3) + 
               scale_y_continuous(labels = scales::percent) + 
               theme(panel.grid.minor = element_blank()) + 
               scale_x_continuous(breaks = seq(0.05, 0.8, by = 0.05)) + 
                                  labs(y = "Accuracy Rate", 
                                       title = "Variation of Classifier's Metrics by Threshold")
```

 (Sensitivity)Good (Specificity) Accuracy 0.7. 


```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
my_cm_com_dl <- function(thre) {
  du_bao_prob <- h2o.predict(pure_nn, test) %>% as.data.frame() %>% pull(Bad)
  du_bao <- case_when(du_bao_prob >= thre ~ "Bad", 
                      du_bao_prob < thre ~ "Good") %>% as.factor()
  cm <- confusionMatrix(du_bao, df_test$Class, positive = "Bad")
  return(cm)
  
}

my_threshold <- c(0.10, 0.25, 0.5, 0.7)
results_list_dl <- lapply(my_threshold, my_cm_com_dl)


vis_detection_rate_dl <- function(x) {
  
  results_list_dl[[x]]$table %>% as.data.frame() -> m
  rate <- round(100*m$Freq[1] / sum(m$Freq[c(1, 2)]), 2)
  acc <- round(100*sum(m$Freq[c(1, 4)]) / sum(m$Freq), 2)
  acc <- paste0(acc, "%")
  
  m %>% 
    ggplot(aes(Reference, Freq, fill = Prediction)) +
    geom_col(position = "fill") + 
    scale_fill_manual(values = c("#e41a1c", "#377eb8"), name = "") + 
    theme(panel.grid.minor.y = element_blank()) + 
    theme(panel.grid.minor.x = element_blank()) + 
    scale_y_continuous(labels = scales::percent) + 
    labs(x = NULL, y = NULL, 
         title = paste0("Detecting Bad Cases when Threshold = ", my_threshold[x]), 
         subtitle = paste0("Detecting Rate for Bad Cases: ", rate, "%", ", ", "Accuracy: ", acc))
  }

gridExtra::grid.arrange(vis_detection_rate_dl(1), 
                        vis_detection_rate_dl(2), 
                        vis_detection_rate_dl(3), 
                        vis_detection_rate_dl(4))
```






