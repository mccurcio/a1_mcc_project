---
title: "Use Autoencoder as a Feature Engineering Technique" 
subtitle: "For Killing Time at the Hospital"

output:
  html_document: 
    code_download: true
    # code_folding: hide
    highlight: pygments
    # number_sections: yes
    theme: "flatly"
    toc: TRUE
    toc_float: TRUE
---

https://rpubs.com/chidungkt/449797

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

[Investigation and improvement of multi-layer perceptron neural networks for credit scoring](https://www.sciencedirect.com/science/article/pii/S0957417414007726)


![](C:/Users/Zbook/Documents/result_german.png)

# Default Random Forest

```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
#=================================
#  Stage 1: Data Pre-processing
#=================================

# Clear workspace: 
rm(list = ls())

# Load packages and data: 
library(tidyverse)
library(magrittr)
library(caret)
data("GermanCredit")

# Split data: 
set.seed(1)
id <- createDataPartition(y = GermanCredit$Class, p = 0.8, list = FALSE)
df_train <- GermanCredit[id, ] # For training
df_test <- GermanCredit[-id, ] # For testing


# Activate h2o package for deep learning: 
library(h2o)
h2o.init(nthreads = -1, max_mem_size = "16g")
h2o.no_progress()

# Convert to H2o Frame:
train <- as.h2o(df_train)
test <- as.h2o(df_test)

# Identify inputput and output: 
y <- "Class"
x <- setdiff(names(train), y)

# Train Default Random Forest: 
pure_nn <- h2o.randomForest(x = x, y = y, 
                            training_frame = train,
                            nfolds = 10, 
                            stopping_rounds = 5, 
                            stopping_metric = "AUC", 
                            seed = 29)


# Function collects results from cross-validation: 

results_df <- function(h2o_model) {
  h2o_model@model$cross_validation_metrics_summary %>% 
    as.data.frame() %>% 
    select(-mean, -sd) %>% 
    t() %>% 
    as.data.frame() %>% 
    mutate_all(as.character) %>% 
    mutate_all(as.numeric) %>% 
    select(Accuracy = accuracy, 
           AUC = auc, 
           Precision = precision, 
           Specificity = specificity, 
           Recall = recall, 
           Logloss = logloss) %>% 
    return()
  }


# Function presents results by graph: 

vis_results <- function(df_results) {
  df_results %>% 
    gather(Metrics, Values) %>% 
    ggplot(aes(Metrics, Values, fill = Metrics, color = Metrics)) +
    geom_boxplot(alpha = 0.3, show.legend = FALSE) + 
    facet_wrap(~ Metrics, scales = "free") + 
    scale_y_continuous(labels = scales::percent) + 
    theme_minimal() + 
    labs(x = NULL, y = NULL, 
         title = "Model Performance Based on Cross Validation")
}


pure_nn %>% results_df() %>% summary()
pure_nn %>% results_df() %>% vis_results()

```

 Accuracy  79%. 

# Autoencoder as a Feature Engineering Technique



```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}

# Buil a autoencoder: 

autoencoder <- h2o.deeplearning(x = x,
                                training_frame = train, 
                                autoencoder = TRUE, 
                                seed = 29, 
                                hidden = c(10, 20, 61), 
                                epochs = 30, 
                                activation = "Tanh")

#============================================================
#  Use Autoencoder as Feature Engineering Method (Version 1)
#============================================================


train_autoen <- h2o.predict(autoencoder, train) %>% 
  as.data.frame() %>% 
  mutate(Class = df_train$Class) %>% 
  as.h2o()

test_autoen <- h2o.predict(autoencoder, test) %>% 
  as.data.frame() %>% 
  mutate(Class = df_test$Class) %>% 
  as.h2o()


nn_autoen_layers1 <- h2o.randomForest(x = setdiff(colnames(train_autoen), "Class"), 
                                      y = y, 
                                      training_frame = train_autoen,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 29)



#============================================================
#  Use Autoencoder as Feature Engineering Method (Version 2)
#============================================================

train_features_l2 <- h2o.deepfeatures(autoencoder, train, layer = 2) %>%
  as.data.frame() %>%
  mutate(Class = df_train$Class) %>% 
  as.h2o()


test_features_l2 <- h2o.deepfeatures(autoencoder, test, layer = 2) %>%
  as.data.frame() %>%
  mutate(Class = df_test$Class) %>% 
  as.h2o()   


nn_autoen_layers2 <- h2o.randomForest(x = setdiff(colnames(train_features_l2), "Class"), 
                                      y = y, 
                                      training_frame = train_features_l2,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 29)


#============================================================
#  Use Autoencoder as Feature Engineering Method (Version 3)
#============================================================

train_features_l3 <- h2o.deepfeatures(autoencoder, train, layer = 3) %>%
  as.data.frame() %>%
  mutate(Class = df_train$Class) %>% 
  as.h2o()


test_features_l3 <- h2o.deepfeatures(autoencoder, test, layer = 3) %>%
  as.data.frame() %>%
  mutate(Class = df_test$Class) %>% 
  as.h2o()


nn_autoen_layers3 <- h2o.randomForest(x = setdiff(colnames(train_features_l3), "Class"), 
                                      y = y, 
                                      training_frame = train_features_l3,
                                      nfolds = 10, 
                                      stopping_rounds = 5, 
                                      stopping_metric = "AUC", 
                                      seed = 29)

#==========================
#  Compare between models
#==========================

do.call("bind_rows", 
        lapply(list(pure_nn, 
                    nn_autoen_layers1, 
                    nn_autoen_layers2, 
                    nn_autoen_layers3), results_df)) -> df_com


df_com %<>% mutate(Model = rep(c("Original", "Layer1", "20Var", "61Var"), each = 10, time = 1))

theme_set(theme_minimal())
df_com %>% 
  gather(a, b, -Model) %>% 
  ggplot(aes(Model, b, fill = Model, color = Model)) + 
  geom_boxplot(alpha = 0.3) + 
  scale_y_continuous(labels = scales::percent) + 
  facet_wrap(~ a, scales = "free") + 
  labs(x = NULL, y = NULL, title = "Model Performance")


df_com %>% 
  group_by(Model) %>% 
  summarise_each(funs(mean), Accuracy, AUC, Logloss, Precision, Recall, Specificity) %>% 
  arrange(-Accuracy) %>% 
  mutate_if(is.numeric, function(x) {round(x, 3)}) %>% 
  knitr::kable()

```

 Feature Engineering. 

# Optimal Threshold


```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
eval_fun <- function(thre) {
  lapply(1:10, function(x) {
    
    set.seed(x)
    id <- createDataPartition(y = df_test$Class, p = 0.5, list = FALSE)
    test_df <- df_test[id, ]
  
    du_bao_prob <- h2o.predict(pure_nn, test_df %>% as.h2o()) %>% 
      as.data.frame() %>% 
      pull(Bad)
    
    du_bao <- case_when(du_bao_prob >= thre ~ "Bad", 
                        du_bao_prob < thre ~ "Good") %>% as.factor()
    cm <- confusionMatrix(du_bao, test_df$Class, positive = "Bad")
    
    bg_gg <- cm$table %>% 
      as.vector() %>% 
      matrix(ncol = 4) %>% 
      as.data.frame() %>% 
      rename(TP = V1, FN = V2, FP = V3, TN = V4)
  
    
    kq <- c(cm$overall, cm$byClass) 
    ten <- kq %>% as.data.frame() %>% row.names()
    
    kq %>% 
      as.vector() %>% 
      matrix(ncol = 18) %>% 
      as.data.frame() -> all_df
    
    names(all_df) <- ten
    all_df <- bind_cols(all_df, bg_gg)
    return(all_df)
  })
}




system.time(so_sanh_list <- lapply(seq(0.05, 0.8, by = 0.05), eval_fun))

so_sanh_df <- do.call("bind_rows", so_sanh_list) 

so_sanh_df %<>% 
  mutate(Threshold = lapply(seq(0.05, 0.8, by = 0.05), function(x) {rep(x, 10)}) %>% unlist())

theme_set(theme_minimal())
so_sanh_df %>% 
  group_by(Threshold) %>% 
  summarise_each(funs(median), Accuracy, Kappa, Sensitivity, Specificity) %>% 
  gather(Metric, b, -Threshold) %>% 
  ggplot(aes(Threshold, b, color = Metric)) + 
  geom_line() + 
  geom_point(size = 3) + 
  scale_y_continuous(labels = scales::percent) + 
  theme(panel.grid.minor = element_blank()) + 
  scale_x_continuous(breaks = seq(0.05, 0.8, by = 0.05)) + 
  labs(y = "Accuracy Rate", 
       title = "Variation of Classifier's Metrics by Threshold")
```

 (Sensitivity)Good (Specificity) Accuracy 0.7. 




```{r, fig.fullwidth = TRUE, fig.height=8, fig.width=12}
my_cm_com_dl <- function(thre) {
  du_bao_prob <- h2o.predict(pure_nn, test) %>% as.data.frame() %>% pull(Bad)
  du_bao <- case_when(du_bao_prob >= thre ~ "Bad", 
                      du_bao_prob < thre ~ "Good") %>% as.factor()
  cm <- confusionMatrix(du_bao, df_test$Class, positive = "Bad")
  return(cm)
  
}

my_threshold <- c(0.10, 0.25, 0.5, 0.7)
results_list_dl <- lapply(my_threshold, my_cm_com_dl)


vis_detection_rate_dl <- function(x) {
  
  results_list_dl[[x]]$table %>% as.data.frame() -> m
  rate <- round(100*m$Freq[1] / sum(m$Freq[c(1, 2)]), 2)
  acc <- round(100*sum(m$Freq[c(1, 4)]) / sum(m$Freq), 2)
  acc <- paste0(acc, "%")
  
  m %>% 
    ggplot(aes(Reference, Freq, fill = Prediction)) +
    geom_col(position = "fill") + 
    scale_fill_manual(values = c("#e41a1c", "#377eb8"), name = "") + 
    theme(panel.grid.minor.y = element_blank()) + 
    theme(panel.grid.minor.x = element_blank()) + 
    scale_y_continuous(labels = scales::percent) + 
    labs(x = NULL, y = NULL, 
         title = paste0("Detecting Bad Cases when Threshold = ", my_threshold[x]), 
         subtitle = paste0("Detecting Rate for Bad Cases: ", rate, "%", ", ", "Accuracy: ", acc))
  }


gridExtra::grid.arrange(vis_detection_rate_dl(1), 
                        vis_detection_rate_dl(2), 
                        vis_detection_rate_dl(3), 
                        vis_detection_rate_dl(4))

```






