---
title: "autoencoder"
author: "MCC"
date: "12/26/2019"
output: html_document
---

https://api.rpubs.com/yoompubs/476942

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(keras)      # package for deep learning
library(readr)      # package for reading file
library(dplyr)      # package for preprocessing
library(purrr)      # package for preprocessing
library(knitr)      # package for report generation
library(kableExtra) # package for report generation
library(caret)      # package for machine learning technique
library(yardstick)  # package for measuring model performance
library(stringr)    # package for splitting string
#library(imbalance)  # package for oversampling
#library(ROSE)       # package for oversampling
library(pROC)       # package for ROC
#library(AppliedPredictiveModeling)
library(GGally)
library(ggplot2)
library(rgl)
library(RColorBrewer)
#library(foreign)
library(ks)
library(mclust)
```


```{r}
## Split the data into train, valid, and test.
X_train_list <- list()
X_valid_list <- list()
X_test_list <- list()
y_train_list <- list()
y_valid_list <- list()
y_test_list <- list()
for(seedind in 1:10) {
  set.seed(2019 + seedind)
  trainvalid_index <- createDataPartition(y_dat, p = 0.8, list = FALSE)
  
  X_trainvalid <- X_dat2[trainvalid_index,] %>% as.matrix()
  y_trainvalid <- y_dat[trainvalid_index]
  
  X_test <- X_dat2[-trainvalid_index,] %>% as.matrix()
  y_test <- y_dat[-trainvalid_index]
  
  train_index <- createDataPartition(y_trainvalid, p = 0.75, list = FALSE)
  
  X_train <- X_trainvalid[train_index,]
  y_train <- y_trainvalid[train_index]
  
  X_valid <- X_trainvalid[-train_index,]
  y_valid <- y_trainvalid[-train_index]
  
  ## Center and scale continuous variables.
  
  prepro <- preProcess(X_train, c("range", "zv"))
  X_train2 <- predict(prepro, X_train)
  X_valid2 <- predict(prepro, X_valid)
  X_test2 <- predict(prepro, X_test)
  
  numevar <- c("FVC", "FEV1", "Age")
  catevar <- colnames(X_train2)[!(colnames(X_train2) %in% numevar)]
  
  X_train_list[[seedind]] <- X_train2
  X_valid_list[[seedind]] <- X_valid2
  X_test_list[[seedind]]  <- X_test2
  y_train_list[[seedind]] <- y_train
  y_valid_list[[seedind]] <- y_valid
  y_test_list[[seedind]]  <- y_test
}
```


```{r}
len_layer2 <- floor(1 + sqrt(ncol(X_train2)))
len_layer1 <- floor(1 + sqrt(len_layer2))

library(keras)
use_session_with_seed(2019)
```


```{r}
input_nume <- layer_input(c(dim(X_train2)[2]))

pred_nume <- input_nume %>% 
  layer_dense(len_layer2) %>%
  layer_activation("selu") %>%
  layer_dense(len_layer1) %>%
  layer_activation("selu") %>%
  layer_dense(len_layer2) %>%
  layer_activation("selu") %>%
  layer_dense(dim(X_train2)[2]) %>%
  layer_activation("sigmoid")

model_nume <- keras_model(input_nume, pred_nume)

summary(model_nume)
```


```{r}
model_nume %>% compile(
  optimizer = optimizer_adam(0.001),
  loss = "mean_squared_error"
  # loss = "mean_absolute_error"
  # loss = "logcosh"
)

history_nume <- model_nume %>% fit(X_train2[y_train==0,], X_train2[y_train==0,],
                                   batch_size = 4096,
                                   validation_data = list(X_valid2[y_valid==0,], X_valid2[y_valid==0,]),
                                   epochs = 10000,
                                   verbose = 0, 
                                   callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 2000, restore_best_weights = TRUE),
                                                    callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 500)))

print(history_nume)
```


```{r}
# plot(history_nume)

layer_weight <- keras::get_weights(model_nume)

pred_nume2 <- input_nume %>% 
  layer_dense(len_layer2) %>%
  layer_activation("selu") %>%
  layer_dense(len_layer1)

model_nume2 <- keras_model(input_nume, pred_nume2)
model_nume2 %>% set_weights(layer_weight)

ae_hidden_train <- predict(model_nume2, X_train2)

p1 <- ggpairs(data.frame(ae_hidden_train), mapping = aes(color = as.factor(y_train)),
              lower = list(continuous = wrap("points", alpha = 0.7)),
              diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
              upper = list(continuous = wrap("cor", size = 3)), title = "Hidden layer of Training set")
# Correlation matrix plot
p2 <- ggcorr(data.frame(ae_hidden_train), label = TRUE, label_round = 2)
g2 <- ggplotGrob(p2)
colors <- g2$grobs[[6]]$children[[3]]$gp$fill
# Change background color to tiles in the upper triangular matrix of plots
idx <- 1
p <- ncol(ae_hidden_train)
for (k1 in 1:(p-1)) {
  for (k2 in (k1+1):p) {
    plt <- getPlot(p1,k1,k2) +
      theme(panel.background = element_rect(fill = colors[idx], color="white"),
            panel.grid.major = element_line(color = colors[idx]))
    p1 <- putPlot(p1,plt,k1,k2)
    idx <- idx+1
  }
}
p1
```

```{r}
ae_hidden_test <- predict(model_nume2, X_test2)

p1 <- ggpairs(data.frame(ae_hidden_test), mapping = aes(color = as.factor(y_test)),
              lower = list(continuous = wrap("points", alpha = 0.7)),
              diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
              upper = list(continuous = wrap("cor", size = 3)), title = "Hidden layer of Test set")
# Correlation matrix plot
p2 <- ggcorr(data.frame(ae_hidden_test), label = TRUE, label_round = 2)
g2 <- ggplotGrob(p2)
colors <- g2$grobs[[6]]$children[[3]]$gp$fill
# Change background color to tiles in the upper triangular matrix of plots
idx <- 1
p <- ncol(ae_hidden_test)
for (k1 in 1:(p-1)) {
  for (k2 in (k1+1):p) {
    plt <- getPlot(p1,k1,k2) +
      theme(panel.background = element_rect(fill = colors[idx], color="white"),
            panel.grid.major = element_line(color = colors[idx]))
    p1 <- putPlot(p1,plt,k1,k2)
    idx <- idx+1
  }
}
p1
```













