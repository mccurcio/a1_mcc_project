# Exploratory Data Analysis Of TRANSFORMED `c_m_RAW_ACC.csv`

NOTE: This next section investigates the EDA of the **TRANSFORMED** data.  The *Transformed* data was derived from `c_m_RAW_ACC.csv` where the amino acids C, F, I were transformed using a squareroot function in order to reduce the skewness of these samples and avoid modeling problems arising from high skewness.

### Import libraries
```{r message=FALSE}
Libraries = c("knitr", "readr", "RColorBrewer", "corrplot", "doMC", "Boruta")

for (i in Libraries) { 
    library(i, character.only = TRUE) 
}

opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = FALSE)
```

### Import data

```{r message=FALSE, warning=FALSE}
c_m_TRANSFORMED <- read_csv("/home/mcc/Dropbox/a1_mcc_project/00-data/aac_dpc_values/c_m_TRANSFORMED.csv")
Class <- as.factor(c_m_TRANSFORMED$Class)
```

### Visually inspect files; 

- Use command line interface followed by the command `less`,
- or Use by RStudio's `view` command,
- check for binary instead of ASCII and bad Unicode.

### Inspect dataframe structure, `str()` 

- 'Class' and numerical data are correct.
```{r}
str(c_m_TRANSFORMED)
```

### Check data `head` & `tail`

```{r}
head(c_m_TRANSFORMED, n=2)
```
```{r}
tail(c_m_TRANSFORMED, n=2)
```

### Check data types

```{r}
is.data.frame(c_m_TRANSFORMED)
class(c_m_TRANSFORMED$Class)        # Col 1
class(c_m_TRANSFORMED$TotalAA)      # Col 2
class(c_m_TRANSFORMED$PID)          # Col 3
class(c_m_TRANSFORMED$A)            # Col 4
```

### Check dataframe dimensions

```{r}
dim(c_m_TRANSFORMED)
```

### Check for missing values

- **No missing values found.**
```{r}
apply(is.na(c_m_TRANSFORMED), 2, which) 

# sapply(c_m_TRANSFORMED, function(x) sum(is.na(x))) # Sum up NA by columns
# c_m_TRANSFORMED[rowSums(is.na(c_m_TRANSFORMED)) != 0,]     # Show rows where NA's is not zero
```

### Count number of polypeptides per group

- Number of polypeptides per `Class`: `Class 0` = Control, `Class 1` = Myoglobin
```{r}
class_table <- table(c_m_TRANSFORMED$Class)
class_table
```

### Numerical summary of features

```{r}
summary(c_m_TRANSFORMED)
```

## Visualize With Descriptive Statistics

### Scatter plot of means of *Myoglobin-Control* amino acid composition `c_m_TRANSFORMED` dataframe

Formulas for mean:
$$E[X] = \sum_{i=1}^n x_i p_i ~~; ~~~~~~ \bar x = \frac {1}{n} \sum_{i=1}^n x_i$$

- This plot shows the means for each feature (column-means) in the dataset. The means represent the ungrouped or total of all proteins (where n=2340) versus AA type.

```{r}
AA_ave <- colMeans(c_m_TRANSFORMED[,4:23])
plot(AA_ave,
     main = "Plot of Column-Means of % Composition Vs Amino Acid",
     ylab = "% Composition",
     xlab = "Amino Acid",
     sub = "(Note: The red line at 0.1 is simply an arbitrary marker)",
     ylim = c(0, 0.3),
     type = "b",
     xaxt = "n")
axis(1, at = 1:20, labels = names(c_m_TRANSFORMED[,4:23]))
```

### Grouped barchart of amino acid vs. protein category

Pseudo-code:  
A-1. Subset 7 protein groups, [Control:Ctrl, Myoglobin:Mgb] & Grand-Mean of both sets  
A-2. Determine column means for each protein class  
A-3. Calculate percentage values  
A-4. Produce Grouped Bar Plot   

```{r, cache=TRUE}
# A-1
ctrl_set <- c_m_TRANSFORMED[ which(c_m_TRANSFORMED$Class == '0'),]
mgb_set  <- c_m_TRANSFORMED[ which(c_m_TRANSFORMED$Class == '1'),]

# A-2
ctrl_means <- apply(ctrl_set[, 4:23], 2, mean)
mgb_means  <- apply(mgb_set[, 4:23], 2, mean)
grand_mean <- apply(c_m_TRANSFORMED[, 4:23], 2, mean)

# A-3
data = data.frame(ctrl_means, mgb_means, grand_mean)
percent_aa = as.matrix(t(100*data))

# A-4
barplot(percent_aa, 
        main = "Mean % A.A.Composition Of 3 Protein Groupings",
        ylab = "% AA Composition", 
        ylim = c(0, 30),
        col = colorRampPalette(brewer.pal(4,"Blues"))(3), 
        legend = T,
        beside = T)
```

### Means of percent amino acid composition of control & myoglobin categories

```{r, message=FALSE, warning=FALSE, cache=TRUE}
data2 = data.frame(ctrl_means, mgb_means)
percent_aa2 = as.matrix(t(100*data2))

barplot(percent_aa2, 
        ylim = c(0, 30),
        main = "Mean % A.A.Composition Of Control & Myoglobin",
        ylab = "% AA Composition", 
        col = colorRampPalette(brewer.pal(4,"Blues"))(2), 
        legend = T,
        beside = T)
```

### Boxplots of grand-means of overall amino acid composition

```{r, message=FALSE, warning=FALSE}
boxplot(c_m_TRANSFORMED[,4:23], 
        main = "Boxplots of Feature Columns, % AAC Vs Amino Acid",
        ylab = "% AAC",
        xlab = "Amino Acid",
        las = 1)
```

### Boxplots of amino acid compositions for control (only)

```{r}
boxplot(ctrl_set[, 4:23], 
        main = "Boxplots of Control, % AAC Vs Amino Acid",
        ylab = "% AAC",
        xlab = "Amino Acid",
        las = 1)
```

### Boxplots of amino acid compositions for myoglobin (only)

```{r}
boxplot(mgb_set[, 4:23],
        main = "Boxplot of Myoglobin, % AAC Vs Amino Acid",
        ylab = "% AAC",
        xlab = "Amino Acid",
        las = 1)
```

### Boxplots of Length of Polypeptides For Myoglobin, Control & Combined

```{r}
ctrl_totalaa <- ctrl_set[,2]
mgb_totalaa  <- mgb_set[,2]
grand_totalaa <- c_m_TRANSFORMED[,2]

data = c(ctrl_totalaa, mgb_totalaa, grand_totalaa)

boxplot(data, 
        ylim = c(0, 5000),
        main = "Boxplot: Length of Polypeptides Vs Control, Myoglobin & Combined",
        ylab = "Length of Polypeptides",
        xaxt = "n",
        las = 2)
axis(1, at=1:3, labels = c("Control", "Myoglobin", "Combined"))
```

### Plot of normalized standard deviations / coefficient of variance (CV)

Standard deviations are sensitive to scale. Therefore I compare the normalized standard deviations. This normalized standard deviation is more commonly called coefficient of variation (CV).

$$CV = \frac {\sigma (x)} {E [|x|]} ~~~ where ~~~ \sigma(x) \equiv \sqrt{ E[x - \mu]^2 }$$

$$CV ~~=~~ \frac{1}{\bar x} \cdot \sqrt{ \frac{1}{n-1} \sum^n_{i=1} (x_i - \bar x)^2}$$

```{r}
AA_var_norm <- (apply(c_m_TRANSFORMED[, 4:23], 2, sd)) / AA_ave
plot(AA_var_norm,
     main = "Plot of Coefficient Of Variance (CV) Vs 20 Std AA",
     sub = "(Note: Two largest values shown in red.)",
     ylab = "Coefficient Of Variance (CV)",
     xlab = "Amino Acid",
     ylim = c(0, 1.5),
     type = "b",
     xaxt = "n")
axis(1, at = 1:20, labels = names(c_m_TRANSFORMED[, 4:23]))
text(x = 2, y = 1.4, label=" C=1.24", col='red')
text(x = 19, y = 1.1, label="W=0.946", col='red')
```
```{r}
AA_var_norm
```

### Skewness of distributions

$$Skewness ~= E\left[ \left( \frac{X - \mu}{\sigma(x)} \right)^3 \right] ~~~~ where ~~~~ \sigma(x) \equiv \sqrt{  E[x - \mu]^2 }$$

$$Skewness ~= \frac { \frac{1}{n} \sum^n_{i=1} (x_i - \bar x)^3 } { \left( \sqrt{ \frac{1}{n-1} \sum^n_{i=1} (x_i - \bar x)^2 } \right) ^ {3}}$$

- Skewness values for each A.A. by Class

```{r}
AA_skewness <- (apply(c_m_TRANSFORMED[, 4:23], 2, e1071::skewness))
plot(AA_skewness,
     main = "Plot of Skewness Vs Amino Acids",
     ylab = "Skewness",
     xlab = "Amino Acid",
     type = "b",
     ylim = c(-1, 3),
     xaxt = "n")
axis(1, at = 1:20, labels = names(c_m_TRANSFORMED[, 4:23]))
abline(h = 2.0, col = "red")
text(x = 2, y = 2.8, label=" C=2.5", col='red')

```

### QQ Plots of 20 amino acids

```{r}
AA = c("A", "C", "D", "E", "F", "G", "H", "I", "K", "L", 
       "M", "N", "P", "Q", "R", "S", "T", "V", "W", "Y")
for (i in 4:23) {
    qqnorm(c_m_TRANSFORMED[[i]], main = AA[[i-3]])
    qqline(c_m_TRANSFORMED[[i]], col = "red")
}
```

### Determine coefficients of correlation

An easily interpretable test, is correlation 2D-plot for investigating multicollinearity or feature reduction. It is clear that fewer attributes "means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they are measuring the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions".[^11]

[^11]:"Applied Predictive Modeling", Max Kuhn and Kjell Johnson, Springer Publishing, 2018, P.43

Pearson's correlation coefficient: 
$$\rho_{x,y} = \frac {E \left[(X - \mu_x)(X - \mu_y) \right]} {\sigma_x \sigma_y}$$

$$r_{xy} = \frac {\sum^n_{i=1} (x_i - \bar x)(y_1 - \bar y)} { {\sqrt {\sum^n_{i=1} (x_i - \bar x)^2 }} {\sqrt {\sum^n_{i=1} (y_i - \bar y)^2 }} }$$

```{r message=FALSE, warning=FALSE}
c_m_corr_mat = cor(c_m_TRANSFORMED[, c(2, 4:23)], 
                   method = "p") # "p": Pearson test for continous variables

corrplot(abs(c_m_corr_mat),
         title = "Correlation Plot Of AAC Features", 
         method = "square", 
         type = "lower",
         tl.pos = "d",
         cl.lim = c(0, 1),
         addgrid.col = "lightgrey", 
         cl.pos = "b", # Color legend position bottom.
         order = "FPC", # "FPC" = first principal component order.
         mar = c(1, 2, 1, 2),
         tl.col = "black")
```

```{r}
c_m_corr_mat["T", "N"]
c_m_corr_mat["Y", "A"]
```

### Boruta - dimensionality reduction

```{r}
c_m_class_20 <- c_m_TRANSFORMED[, -c(2,3)]  # Remove TotalAA & PID
Class <- as.factor(c_m_class_20$Class) # Convert ‘Class’ To Factor
```

**Perform Boruta search**

NOTE: *mcAdj = TRUE*: If True, multiple comparisons  will be adjusted using the Bonferroni method to calculate p-values. Therefore, $p_i \leq \frac {\alpha} {m}$ where $\alpha$ is the desired p-value and $m$ is the total number of null hypotheses. 
```{r cache=TRUE}
set.seed(1000)
registerDoMC(cores = 3)  # Start multi-processor mode
start_time <- Sys.time() # Start timer

boruta_output <- Boruta(Class ~ ., 
                        data = c_m_class_20[,-1],
                        mcAdj = TRUE, # See Note above.
                        doTrace = 1) # doTrace = 1, represents non-verbose mode.

registerDoSEQ()  # Stop multi-processor mode
end_time <- Sys.time()   # End timer
end_time - start_time    # Display elapsed time
```

```{r}
#names(boruta_output)
```

**Plot variable importance**

```{r}
plot(boruta_output, 
     cex.axis = 1,
     las = 2, 
     ylim = c(-5,50),
     main = "Variable Importance (Bigger=Better)")
```

**Variable importance scores**

```{r message=FALSE}
roughFixMod <- TentativeRoughFix(boruta_output)
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
meanImps <- imps2[order(-imps2$meanImp), ]  # descending sort

knitr::kable(meanImps,
             full_width = F, 
             position = "left",
             caption  = 'Mean Importance Scores & Decision')
```


Plot importance history

```{r}
plotImpHistory(boruta_output)
```

**Conclusion for Boruta random forest test**

All features are important. None should be dropped.


## Conclusions For EDA ??????????????????????????????????

Due to prior subject knowledge in the area of utilizing AAC and DPC, it was determined that initially the machine learning modeling should be carried out with the single amino acid percent composition. This is primarily due to the fact that many of the di-peptide percent compositions had skews greater than 2 and some reaching as high as 40. This is most likely due to the lack of large sample sizes used.

It was also found that three amino acids from the single amino acid percent composition should be transformed by using the square root function. The square root transformation lowered the skewness from greater than 2 in all cases to {-0.102739 $\leq$ skew after transformation $\leq$ 0.3478132}.

| Protein | Initial skewness | Skew after square root transform |
|:-------:|:----------------:|:--------------------------------:|
| C | 2.538162 | 0.3478132 |
| F | 2.128118 | -0.102739 |
| I | 2.192145 | 0.2934749 |

