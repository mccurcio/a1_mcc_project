# K-Means

```{r, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      cache.lazy = FALSE)
```

## Summary

Clustering is a fundamental technique in exploratory data analysis and pattern discovery, aimed at extracting underlying structure.[^1] It is efficient and it runs quickly even on older machinery. The methodology is simple therefore the results are not generally complicated to understand. 

[^1]:Bioinformatics - The Machine Learning Approach, P. Baldi, S. Brunak, MIT Press, 2001

K-means is an [Unsupervised learning](https://www.datacamp.com/courses/unsupervised-learning-in-r)[^2] algorithm.  therefore it only requires two items as inputs; 

[^2]:https://www.datacamp.com/courses/unsupervised-learning-in-r

1) a matrix/array or vector of features (X) with no Y variable needed,  
2) a value for K which represents the number of clusters for inspection.

As the name suggests, K-means determines the K number of means that your data points are centered around.  For instance; 

- Let K = { 1, 2, 3, …, $C_K$ }  a Natural counting number
- E = { $e_1, e_2, e_3, ..., e_n$ }, the set entities to be clustered  
- Where: E  is a matrix size (m x n) $\in R^n$, m observations (rows) and n features (columns) 

For example, Let K = 2, and E  is a matrix size (100 x 2), m observations and n features.

Imagine a two spheres in a 2-D space. Therefore, It is possible to have 25 points within the volume of the sphere$(K=1)$ and 75 points within the volumes of the sphere $(K=2)$. The two spheres have centers, also known as centroids. If the 2 spheres do not touch or overlap than it is easy to see that the 25 points within sphere $(K=1)$ are closer to the centroid of the sphere then the remaining points. 


it attempts to cluster or group your entities into K clusters around the mean values which best describe the Ki cluster.

The data may be presented as a (m x n) dimensional array of features with rows of observations with no corresponding labels or Y values. With the base R package it is possible to load simple comma separated values (.csv), or tab-separated values (.tsv)


The second input, the K value, determines how many clusters one intends to test your dataset against.  A set of 


The K value can be determined empirically or when the researcher has prior knowledge a specific value can be chosen. Many times it is common to not know the number of possible clusters given the complexity of your datasets.  A multivariate dataset is difficult to graph all the permutations of if you are limited to 2-D graphics. Therefore it is standard procedure to implement a small search for range find the best value(s) of K. There may be hidden structure with the data that researchers are not aware of.

For example, the dataset for this set of experiments/studies contains 7 groupings of proteins; 

1. erythrocruorin, 
1. hemerythrin, 
1. hemocyanin, 
1. hemoglobin, 
1. leghemoglobin, 
1. myoglobin and 
1. a control group of human proteins that do not contain oxygen binding sites within.

My goal is to investigate how well K-means clustering can do at discerning the 7 groups apart.  


The algorithm does not look for identical features that describe all the entities or data points but instead looks for similarities. As the name suggests it seeks


K-means is an Unsupervised learning algorithm. The data that is used for input contains only feature values


 K-means is valued for many reasons;

1. K-means runs very quickly on most computers even those with older hardware,
1. Its results are easy to comprehend and draw quick and insightful conclusions from despite its simplicity,
1. wish to categorize or even gain some basic insights regarding there data.

K-Means strengths are:

1. Easy to use; does not require a high degree of computing power such that K-Means can be run on old hardware.
1. The mathematics is easily understandable which happens to make the results easily understandable too.
1. The algorithm is computationally efficient with a Big O proportional to; number of samples, number of dimensions, number of iteration


Clustering has been a useful tool for classification of multivariate observations since 1967 by MacQueen. It was recognized as easily programmed and computationally economical. 

It is known as a polythetic methodology, meaning it may have many items in common with each other, but not all properties in common. Therefore it clusters items based on their similarity and not how identical the items are.  So an item is similar to another by how close it is to another, known as "similarity grouping."  For example, using Roland Fishers iris data species are similar to each other due to the fact that the petals are between x and y cm in length. 

Conversely, a monothetic cluster method means that the objects all have at least one thing in common. For example, a grouping may be all male with beards and hats.

One tendency of the K-means algorithm is it provides a hard border or boundary of its classification observations.  If we consider the diagram of two centroids on a 2-dimensional plane we can easily see that the boundary between the two groups is the perpendicular bisector connecting the centroids.  As we can see an observation point is either closer to one center than the other, it is never both. This is known as a hard boundary that separates the clusters.

**ADD Diagram of two centroids on plane and perpendicular bisector.**


[^1]:MacQueen, J. B. (1967), Some Methods for classification and Analysis of Multivariate Observations, Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, 1. University of California Press. pp. 281–297.

[^2]:Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006

---

K-Means is the minimization of a distance measure to k-number of centroids or centers.  

Given a set of N objects or entities with M-dimensions and K number of clusters  the objective is to minimize the expected loss. This loss is an error measurement in terms of the sum of squared errors. A simple yet very useful tool.

$SSE(C) ~=~ \sum_{i=1}^k \sum_{x_j \in C_i} {\Vert x_j - \mu_i \Vert}^2$

- Where the algorithm first sums up the differences of the sample points to the closest centroid and then for all the K centroids with sample points around them.

$C^* ~=~ argmin_{~C}~ \{ SSE(C) \}$

References  

* https://rpubs.com/kaz_yos/epi288-cluster  
* The Elements of Statistical Learning:   http://www-stat.stanford.edu/~tibs/ElemStatLearn/  
* CRAN Task View: http://cran.r-project.org/web/views/Cluster.html  
* https://rpubs.com/JanpuHou/298239  

Cluster analysis  

* K-means clustering  
* K-Medoid clustering  
* Expectation maximization (EM) clustering  
* Hierarchical clustering  


## K-Means Pseudocode 

**Input:**  

1. $E$ = {$e_1, e_2, e_3, ..., e_n$}, the set entities to be clustered  

- Where: $E$  is a matrix size (m x n) $\in R^n$, m observations (rows) and n features (columns)   

2. k = the number of clusters  

3. MaxIterations = the maximum number of iterations to run  
   

**Output:**  

1. $C$ = {1, 2, ..., k}, set of cluster centroids  

2. L = {$l(e)|e$ = 1, 2, ..., n}, Set of all cluster labels of set $E$.  

---

Steps:  

1. For every $C$ = {1, 2 ,..., k} Do # This loop chooses initial cluster centroids randomly.
    - $c_i \leftarrow e_j \in E$  
    - end For

2. For each $e_i \in E$ Do # Calculates distance from all points to centroids then choose nearest.
    - $l(e_i) \leftarrow argmin Distance(e_i, c_j)j \in$ {1,2,...,k}
    - end For  

3. Initialize variable 'Changed' $\leftarrow$ FALSE

4. initialize variable 'iter' $\leftarrow$ NULL

5. While (changed = FALSE) and (iterations <= MaxIterations) Do

   1. For each $c_i \in C$ Do
       - Update Cluster values($c_i$)
       - end For
   2. For $e_i \in E$ Do
       - minDistance $\leftarrow$ argminDistance$(e_i, ~c_j) ~ j ~ \in$ {1, 2, ..., k}
       - If minDistance $\leftarrow ~ \neq ~ l(e_i)$ then  
           - $l(e_i) \leftarrow minDistance  
           - Changed $\leftarrow$ TRUE  
           - end If  
       - end For  
   iter += 1   
      end While  

End Program


## Hyper-parameters

## Big O

The Big O( # iterations * # clusters * # observations * # of dimensions or features ) = O(I * C * N * D) 

Because this algorithm is linear in nature it is a very fast algorithm to run and even use with large datasets.

## Model Training & Tuning

Libraries

```{r, message=FALSE, warning=FALSE}
Libraries = c("doMC")

for(p in Libraries){  # Install if not present
    if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
    library(p, character.only = TRUE)
}
```

Import data & data handling

```{r}
test_harness_paa <- read.csv("test_harness_paa.csv")
test_harness_paa <- test_harness_paa[, -c(2,3)]
Class <- as.factor(test_harness_paa$Class) # Convert ‘Class’ To Factor
```

Partition Data - training & testing sets

```{r}
set.seed(1000)
index <- createDataPartition(test_harness_paa$Class, p = 0.8, list = FALSE)

training_set <- test_harness_paa[ index,]
test_set     <- test_harness_paa[-index,]
```

### Run Training Set 
 
```{r cache=TRUE}
set.seed(1000)
start_time <- Sys.time() # Start timer

vars <- names(test_harness_paa[,-1]) # Extract variables for analysis

num.clusters <- 1:10 # Try 1 to 20 clusters

test_harness_paa.Mclust <- Mclust(test_harness_paa[ ,vars]) # Number of clusters

end_time <- Sys.time() # End timer
end_time - start_time # Display time
```

### Display Model Results


## Predictions - Run Test Set

