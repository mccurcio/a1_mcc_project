# K-Nearest-Neighbor (KNN)

```{r, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      cache.lazy = FALSE)
```

## Summary

P.37, Note that (2.10) is a conditional probability: it is the probability
that Y = j, given the observed predictor vector x 0 . This very simple clas-
sifier is called the Bayes classifier.

p.39, K-nearest neighbors (KNN) classifier

KNN psuedo code for KNN, https://stackoverflow.com/questions/22796864/k-nearest-neighbor-pseudocode

http://www.scholarpedia.org/article/K-nearest_neighbor

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2243774/?page=1

Very Good:
https://dataaspirant.com/2016/12/23/k-nearest-neighbor-classifier-intro/

EXCELLENT Write up:
https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/

## Pseudocode

**Input:**  

1. $(X, ~Y)$ is a labeled pair,    
    
where:  
$X ~~ is~ a~ matrix~ size ~ (m, n) \in R^n$  (with m observations and n features),    

$Y ~is ~a ~matrix ~size ~(m, 1) ~\in \{1, 2, 3, ...,C\}$  (with m observations and 1 Label), 

such that $~f(x_i, x_j, ...) = ~~Y$,

**Output:**  

1. 

Procedure

1. Classify $(X, ~Y, ~x)~~$  where $X$ are the training data, $Y$ are the class labels, $x$ are unknown samples  

2. for $i = 1~~ to ~~m$   
       Compute distance $d(X_i, ~x)$  
       
3. Compute set $I$ containing indices for the k smallest distances, $d(X_i, ~x)$.  

4. Return the majority label for ${Y_i ~~where ~~i \in I}$  

**Distance Measurements,**$~~~~d(X_i, ~x)$

1. Euclidean Distance = $\sqrt{(x_{i} - y_{i})^2 }$

2. Manhattan Distance = $\sum_{i=1}^n { (x_{i} - y_{i}) }$

3. Hamming Distance = $\sum_{i=1}^n { (x_{i} - y_{i}) }$

   + Hamming distances are used for categorical variables.
   
   + This is somewhat similar to the Manhatten distance. However, if the value $(x_{i1} - x_{i2})$ are same, the distance D=0. Otherwise D=1.

4. Cosine similarity function = $\frac {X \cdot Y} {\sqrt {X \cdot Y}  \sqrt {X \cdot Y}}$

   + Where $X \cdot Y$ are the [dot products](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length) of the two matricies $X$ and $Y$.

## Hyper-parameters

## Big O

## Model Training & Tuning

Libraries
```{r, message=FALSE, warning=FALSE}
Libraries = c("doMC", "caret")

for(p in Libraries){  # Install if not present
    if(!require(p, character.only = TRUE)) { install.packages(p, dependencies = TRUE) }
    library(p, character.only = TRUE)
}
```

Import data, data handling & Partition data - training / testing sets

```{r}
test_harness_paa <- read.csv("data/test_harness_paa.csv")
test_harness_paa <- test_harness_paa[, -c(2,3)]

set.seed(1000)
index <- createDataPartition(test_harness_paa$Class, p = 0.8, list = FALSE)

training_set <- test_harness_paa[ index,]
test_set     <- test_harness_paa[-index,]

training_set[["Class"]] = factor(training_set[["Class"]])
test_set[["Class"]]  = factor(test_set[["Class"]])
```

### Run KNN Training Set 
 
```{r, cache = TRUE}
control <- trainControl(method = "repeatedcv",
	                     number = 5,
	                     repeats = 5)
grid <- expand.grid(k = seq(1, 11, 2))

registerDoMC(cores = 3)
start_time <- Sys.time() # Start timer

set.seed(1000)
modelknn <- train(Class ~ ., 
                  data = training_set, 
                  method = "knn", 
                  trControl = control,
                  preProcess = c("center", "scale"),
                  tuneGrid = grid)

end_time <- Sys.time()   # End timer
end_time - start_time    # Display time
registerDoSEQ()
```

### Model Summary

```{r}
modelknn
```

```{r}
names(modelknn)
```

```{r}
names(modelknn$results)
```


```{r}
plot(x = modelknn$results$k,
     y = modelknn$results$Accuracy,
     type = "b",
     ylim = c(0.75, 0.9))
```

### Predict Test Set

```{r}
Predicted_test_vals <- predict(modelknn, test_set[, -1])

length(Predicted_test_vals)
```

Quick Summary

```{r}
summary(Predicted_test_vals)
```

### Confusion Matrix

```{r}
confusionMatrix(Predicted_test_vals, 
                test_set[["Class"]])
```

## Summary






