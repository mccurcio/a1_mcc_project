[
["exploratory-data-analysis.html", "3 Exploratory Data Analysis 3.1 Introduction 3.2 Analysis of RAW data 3.3 Analysis of TRANSFORMED data 3.4 EDA Conclusion", " 3 Exploratory Data Analysis “Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.” 1 3.1 Introduction The experiment described herein involves taking groups of proteins from the Uniprot.org database and comparing how well different machine learning techniques do at separating the positive from the negative control grouping. In this circumstance, proteins from the myoglobin family are analyzed against randomly chosen human proteins, which are not related to hemoglobin or myoglobin. This work is to characterize the anomalous points derived from PCA and compare them to the false-positives and false-negatives generated from each of six machine learning approaches produces. For the sake of this paper anomalous points are defined as values greater than the absolute value of three times the standard deviation from of the first and second principal components. \\[\\begin{equation} Anomalous ~Points &gt; | 3 \\sigma | ~~~where~~~ \\sigma = \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2} \\end{equation}\\] Therefore the M.L techniques will be: Principal Component Analysis, Logistic Regression, SVM-Linear, SVM-polynomial, SVM-RBF, Neural Network. 3.1.1 Four-Step Analysis At this stage, data is inspected in a careful and structured way. Hence, I have chosen a four-step process: Hypothesize, Summarize, Visualize, Normalize. 3.1.2 Useful Guides for Exploratory Data Analysis The summarization of the amino acid dataset is based on a hybrid set of guidelines; NIST Handbook of Statistics,2 Exploratory Data Analysis With R by Roger Peng,3 Exploratory Data Analysis Using R by Ronald K. Pearson.4 3.1.3 Questions During EDA Although exploratory data analysis does not always have a formal hypothesis testing portion, I do, however, pose several questions concerning the structure, quality, and types of data. Do the independent variables of this study have large skewed distributions? 1.1 If skews are greater than 2.0, then can a transformation be used for normalization? 1.2 Determine what transformation to use? Can Feature Selection be used, and which procedures are appropriate? 2.1 Use the Random Forest technique known as Boruta5 for feature importance or reduction? 2.2 Will coefficients of correlation (R) find collinearity and reduce the number of features? 2.3 Will principal component analysis (PCA) be useful in finding hidden structures of patterns? 2.4 Can PCA be used successfully for Feature Selection? What is the structure of the data? 3.1 Is the data representative of the entire experimental space? 3.2 Is missing data an issue? 3.3 Does the data have certain biases, either known or unknown? 3.4 What relationships do we expect from these variables?6 3.2 Analysis of RAW data Raw data is considered: ./00-data/02-aac_dpc_values/c_m_RAW_AAC.csv # Import libraries, NO &quot;doMC&quot;, library(easypackages) libraries(&quot;knitr&quot;, &quot;readr&quot;, &quot;RColorBrewer&quot;, &quot;corrplot&quot;, &quot;Boruta&quot;, &quot;kableExtra&quot;) # Import RAW data c_m_RAW_AAC &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_RAW_AAC.csv&quot;) Class &lt;- as.factor(c_m_RAW_AAC$Class) 3.2.1 Visually inspect RAW data files Use the command-line interface followed by the command less. Check for binary instead of ASCII and bad Unicode. 3.2.2 Inspect RAW dataframe structure, str() ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2340 obs. of 23 variables: ## $ Class : num 0 0 0 0 0 0 0 0 0 0 ... ## $ TotalAA: num 226 221 624 1014 699 ... ## $ PID : chr &quot;C1&quot; &quot;C2&quot; &quot;C3&quot; &quot;C4&quot; ... ## $ A : num 0.2655 0.2081 0.0433 0.0661 0.0644 ... ## $ C : num 0 0 0.00962 0.01381 0.03577 ... ## $ D : num 0.00442 0.00452 0.04647 0.06114 0.02861 ... ## $ E : num 0.031 0.0271 0.0833 0.074 0.0472 ... ## $ F : num 0.00442 0.00452 0.02564 0.02959 0.06295 ... ## $ G : num 0.0708 0.0769 0.0817 0.07 0.0443 ... ## $ H : num 0 0 0.0176 0.0187 0.0157 ... ## $ I : num 0.00885 0.0181 0.03045 0.04734 0.0701 ... ## $ K : num 0.28761 0.27602 0.00962 0.12426 0.05579 ... ## $ L : num 0.0442 0.0452 0.0577 0.0888 0.1359 ... ## $ M : num 0.00442 0.00452 0.01442 0.02465 0.02289 ... ## $ N : num 0.0177 0.0136 0.0641 0.0355 0.0558 ... ## $ P : num 0.0841 0.0995 0.0449 0.0434 0.0472 ... ## $ Q : num 0.00442 0.00905 0.04327 0.03353 0.02861 ... ## $ R : num 0.0133 0.0181 0.1202 0.0325 0.0415 ... ## $ S : num 0.0575 0.0724 0.1875 0.0838 0.0787 ... ## $ T : num 0.0531 0.0633 0.0625 0.0414 0.0744 ... ## $ V : num 0.0442 0.0543 0.0385 0.0671 0.0458 ... ## $ W : num 0 0 0.00481 0.01282 0.00715 ... ## $ Y : num 0.00442 0.00452 0.01442 0.03156 0.0372 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Class = col_double(), ## .. TotalAA = col_double(), ## .. PID = col_character(), ## .. A = col_double(), ## .. C = col_double(), ## .. D = col_double(), ## .. E = col_double(), ## .. F = col_double(), ## .. G = col_double(), ## .. H = col_double(), ## .. I = col_double(), ## .. K = col_double(), ## .. L = col_double(), ## .. M = col_double(), ## .. N = col_double(), ## .. P = col_double(), ## .. Q = col_double(), ## .. R = col_double(), ## .. S = col_double(), ## .. T = col_double(), ## .. V = col_double(), ## .. W = col_double(), ## .. Y = col_double() ## .. ) 3.2.3 Check RAW data head &amp; tail head(c_m_RAW_AAC, n = 2) ## # A tibble: 2 x 23 ## Class TotalAA PID A C D E F G H I K L ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 226 C1 0.265 0 0.00442 0.0310 0.00442 0.0708 0 0.00885 0.288 0.0442 ## 2 0 221 C2 0.208 0 0.00452 0.0271 0.00452 0.0769 0 0.0181 0.276 0.0452 ## # … with 10 more variables: M &lt;dbl&gt;, N &lt;dbl&gt;, P &lt;dbl&gt;, Q &lt;dbl&gt;, R &lt;dbl&gt;, S &lt;dbl&gt;, T &lt;dbl&gt;, ## # V &lt;dbl&gt;, W &lt;dbl&gt;, Y &lt;dbl&gt; tail(c_m_RAW_AAC, n = 2) ## # A tibble: 2 x 23 ## Class TotalAA PID A C D E F G H I K L ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 335 M1123 0.0567 0.00299 0.0537 0.0716 0.0507 0.0507 0.0388 0.0776 0.0716 0.0687 ## 2 1 43 M1124 0.0698 0 0.116 0.116 0.0930 0.0465 0 0.0233 0.0233 0.0698 ## # … with 10 more variables: M &lt;dbl&gt;, N &lt;dbl&gt;, P &lt;dbl&gt;, Q &lt;dbl&gt;, R &lt;dbl&gt;, S &lt;dbl&gt;, T &lt;dbl&gt;, ## # V &lt;dbl&gt;, W &lt;dbl&gt;, Y &lt;dbl&gt; 3.2.4 Check RAW data types is.data.frame(c_m_RAW_AAC) ## [1] TRUE class(c_m_RAW_AAC$Class) # Col 1 ## [1] &quot;numeric&quot; class(c_m_RAW_AAC$TotalAA) # Col 2 ## [1] &quot;numeric&quot; class(c_m_RAW_AAC$PID) # Col 3 ## [1] &quot;character&quot; class(c_m_RAW_AAC$A) # Col 4 ## [1] &quot;numeric&quot; 3.2.5 Check RAW dataframe dimensions dim(c_m_RAW_AAC) ## [1] 2340 23 3.2.6 Check RAW for missing values No missing values found. apply(is.na(c_m_RAW_AAC), 2, which) ## integer(0) # sapply(c_m_RAW_AAC, function(x) sum(is.na(x))) # Sum up NA by columns # c_m_RAW_AAC[rowSums(is.na(c_m_RAW_AAC)) != 0,] # Show rows where NA&#39;s is not zero 3.2.7 Number of polypeptides per Class: Class 0 = Control, Class 1 = Myoglobin ## ## 0 1 ## 1216 1124 3.2.8 Numerical summary of RAW data ## Class TotalAA PID A C ## Min. :0.0000 Min. : 2.0 Length:2340 Min. :0.00000 Min. :0.000000 ## 1st Qu.:0.0000 1st Qu.: 109.8 Class :character 1st Qu.:0.05108 1st Qu.:0.000000 ## Median :0.0000 Median : 154.0 Mode :character Median :0.07364 Median :0.007034 ## Mean :0.4803 Mean : 353.8 Mean :0.07835 Mean :0.011970 ## 3rd Qu.:1.0000 3rd Qu.: 407.0 3rd Qu.:0.10261 3rd Qu.:0.020408 ## Max. :1.0000 Max. :4660.0 Max. :0.28000 Max. :0.159420 ## D E F G H ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.03401 1st Qu.:0.05435 1st Qu.:0.03801 1st Qu.:0.04544 1st Qu.:0.01324 ## Median :0.05195 Median :0.07143 Median :0.04545 Median :0.06394 Median :0.02297 ## Mean :0.04900 Mean :0.07451 Mean :0.05135 Mean :0.06193 Mean :0.02890 ## 3rd Qu.:0.06567 3rd Qu.:0.09091 3rd Qu.:0.05501 3rd Qu.:0.08625 3rd Qu.:0.04095 ## Max. :0.17647 Max. :0.50000 Max. :0.37500 Max. :0.36364 Max. :0.13333 ## I K L M N ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.04348 1st Qu.:0.05797 1st Qu.:0.07480 1st Qu.:0.01087 1st Qu.:0.01948 ## Median :0.05992 Median :0.08182 Median :0.09136 Median :0.01948 Median :0.04145 ## Mean :0.06839 Mean :0.08386 Mean :0.09313 Mean :0.01949 Mean :0.04228 ## 3rd Qu.:0.08216 3rd Qu.:0.12081 3rd Qu.:0.11688 3rd Qu.:0.02721 3rd Qu.:0.05788 ## Max. :0.50000 Max. :0.28761 Max. :0.25000 Max. :0.11111 Max. :0.12563 ## P Q R S T ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.02464 1st Qu.:0.02212 1st Qu.:0.01476 1st Qu.:0.04348 1st Qu.:0.03247 ## Median :0.03401 Median :0.03598 Median :0.03896 Median :0.05564 Median :0.05194 ## Mean :0.03825 Mean :0.03342 Mean :0.03818 Mean :0.06191 Mean :0.04838 ## 3rd Qu.:0.04772 3rd Qu.:0.04545 3rd Qu.:0.05370 3rd Qu.:0.06964 3rd Qu.:0.06522 ## Max. :0.20635 Max. :0.18182 Max. :0.24324 Max. :0.22619 Max. :0.18750 ## V W Y ## Min. :0.00000 Min. :0.000000 Min. :0.00000 ## 1st Qu.:0.04575 1st Qu.:0.001899 1st Qu.:0.01463 ## Median :0.05844 Median :0.011492 Median :0.02865 ## Mean :0.06512 Mean :0.012327 Mean :0.03644 ## 3rd Qu.:0.07405 3rd Qu.:0.017889 3rd Qu.:0.04564 ## Max. :0.20000 Max. :0.133333 Max. :0.14286 3.2.9 Visualize Descriptive Statistics using RAW Data Formulas for mean: \\[\\begin{equation} E[X] = \\sum_{i=1}^n x_i p_i ~~; ~~~~~~ \\bar x = \\frac {1}{n} \\sum_{i=1}^n x_i \\end{equation}\\] 3.2.10 Scatter plot of means of Myoglobin-Control amino acid composition of RAW Data This Scatter-plot shows the means for each feature (column-means) in the dataset. The means represent the ungrouped or total of all proteins (where n = 2340) versus AA type. # A-4 ### Grouped barchart of amino acid vs. protein category barplot(percent_aa, main = &quot;Mean % A.A.Composition Of 3 Protein Groupings&quot;, ylab = &quot;% AA Composition&quot;, ylim = c(0, 12), col = colorRampPalette(brewer.pal(4, &quot;Blues&quot;))(3), legend = T, beside = T) 3.2.11 Means of percent amino acid composition of control &amp; myoglobin categories, RAW data 3.2.12 Boxplots of grand-means of overall amino acid composition, RAW data 3.2.13 Boxplots of amino acid compositions for control (only), RAW data 3.2.14 Boxplots of amino acid compositions for myoglobin (only), RAW data 3.2.15 Boxplots Of Length Of Polypeptides For RAW Data 3.2.16 Plot Coefficient Of Variance For RAW Data Standard deviations are sensitive to scale. Therefore I compare the normalized standard deviations. This normalized standard deviation is more commonly called the coefficient of variation (CV). \\[\\begin{equation} CV = \\frac {\\sigma (x)} {E [|x|]} ~~~ where ~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} CV ~~=~~ \\frac{1}{\\bar x} \\cdot \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2} \\end{equation}\\] AA_var_norm ## A C D E F G H I K ## 0.6095112 1.2444944 0.5478540 0.4156102 0.5436243 0.5201625 0.7966296 0.6005962 0.4689544 ## L M N P Q R S T V ## 0.3215591 0.6529752 0.7352478 0.7383244 0.5752622 0.7680977 0.4948690 0.5830352 0.4420595 ## W Y ## 0.9461276 0.8461615 3.2.17 Skewness of distributions, RAW data \\[\\begin{equation} Skewness ~= E\\left[ \\left( \\frac{X - \\mu}{\\sigma(x)} \\right)^3 \\right] ~~~~ where ~~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} Skewness ~= \\frac { \\frac{1}{n} \\sum^n_{i=1} (x_i - \\bar x)^3 } { \\left( \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2 } \\right) ^ {3}} \\end{equation}\\] Skewness values for each A.A. are determined in totality. AA_skewness ## A C D E F G H ## 0.670502595 2.538162400 -0.058540442 1.782876260 2.128117638 0.091338300 1.135783661 ## I K L M N P Q ## 2.192145038 0.223433207 -0.172566877 0.744002991 0.633532783 1.493903282 0.306716333 ## R S T V W Y ## 1.241930812 1.448521897 -0.006075043 1.338971930 1.831047440 1.694362388 3.2.18 Determine coefficients of correlation, RAW data An easily interpretable test is a correlation 2D-plot for investigating multicollinearity or feature reduction. Fewer attributes “means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they measure the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions.” 7 Pearson’s correlation coefficient: \\[\\begin{equation} \\rho_{x,y} = \\frac {E \\left[(X - \\mu_x)(X - \\mu_y) \\right]} {\\sigma_x \\sigma_y} \\end{equation}\\] \\[\\begin{equation} r_{xy} = \\frac {\\sum^n_{i=1} (x_i - \\bar x)(y_1 - \\bar y)} { {\\sqrt {\\sum^n_{i=1} (x_i - \\bar x)^2 }} {\\sqrt {\\sum^n_{i=1} (y_i - \\bar y)^2 }} } \\end{equation}\\] c_m_corr_mat &lt;- cor(c_m_RAW_AAC[, c(2, 4:23)], method = &quot;p&quot;) # &quot;p&quot;: Pearson test for continous variables corrplot(abs(c_m_corr_mat), title = &quot;Correlation Plot Of AAC, RAW Data&quot;, method = &quot;square&quot;, type = &quot;lower&quot;, tl.pos = &quot;d&quot;, cl.lim = c(0, 1), addgrid.col = &quot;lightgrey&quot;, cl.pos = &quot;b&quot;, # Color legend position bottom. order = &quot;FPC&quot;, # &quot;FPC&quot; = first principal component order. mar = c(1, 2, 1, 2), tl.col = &quot;black&quot;) NOTE: Amino acids shown in First Principal Component order, top to bottom. Maximum value of Correlation between T &amp; N. ## [1] 0.7098085 According to Max Kuhn8, correlation coefficients need only be addressed if the |R| &gt;= 0.75. Therefore is no reason to consider multicollinearity. 3.2.19 Boruta Random Forest Test, RAW data It finds relevant features by comparing original attributes’ importance with importance achievable at random, estimated using their permuted copies (shadows). Miron Kursa 9 c_m_class_20 &lt;- c_m_RAW_AAC[, -c(2, 3)] # Remove TotalAA &amp; PID Class &lt;- as.factor(c_m_class_20$Class) # Convert &#39;Class&#39; To Factor NOTE: mcAdj = TRUE, If True, multiple comparisons will be adjusted using the Bonferroni method to calculate p-values. Therefore, \\(p_i \\leq \\large \\frac {\\alpha} {m}\\) where \\(\\alpha\\) is the desired p-value and \\(m\\) is the total number of null hypotheses. set.seed(1000) #registerDoMC(cores = 3) # Start multi-processor mode start_time &lt;- Sys.time() # Start timer boruta_output &lt;- Boruta(Class ~ ., data = c_m_class_20[, -1], mcAdj = TRUE, # See Note above. doTrace = 1) # doTrace = 1, represents non-verbose mode. #registerDoSEQ() # Stop multi-processor mode end_time &lt;- Sys.time() # End timer end_time - start_time # Display elapsed time ## Time difference of 29.70991 secs names(boruta_output) 3.2.20 Plot variable importance, RAW Data 3.2.21 Variable importance scores, RAW Data meanImp decision R 43.18824 Confirmed H 34.29757 Confirmed P 28.70225 Confirmed C 27.67710 Confirmed K 27.60808 Confirmed E 26.18884 Confirmed Y 22.85337 Confirmed T 21.67689 Confirmed S 21.43716 Confirmed A 20.53089 Confirmed N 20.09681 Confirmed V 18.77054 Confirmed I 18.76492 Confirmed F 18.31240 Confirmed D 17.64592 Confirmed G 16.15461 Confirmed W 15.74107 Confirmed L 15.27767 Confirmed M 14.82861 Confirmed Q 14.13939 Confirmed 3.2.22 Conclusion for Boruta random forest test, RAW Data All features are essential. None should be dropped. 3.2.23 Conclusions For EDA, RAW data Three amino acids (C, F, I) from the single amino acid percent composition were deemed problematic due to their skewness were greater than 2.0. This suggests that a transformation should be carried out to rectify this issue. Protein Skewness C, Cysteine 2.538162 F, Phenolalanine 2.128118 I, Isoleucine 2.192145 3.3 Analysis of TRANSFORMED data This EDA section is a reevaluation square root transformed, c_m_RAW_ACC.csv data set, hence called c_m_TRANSFORMED.csv. The \\(\\sqrt x_i\\) Transformed data is derived from c_m_RAW_ACC.csv where the amino acids C, F, I were transformed using a square root function. This transformation was done to reduce the skewness of these samples and avoid modeling problems arising from high skewness, as seen below. Amino Acid Initial skewness Skew After Square-Root Transformation C, Cysteine 2.538162 0.3478132 F, Phenolalanine 2.128118 -0.102739 I, Isoleucine 2.192145 0.2934749 # Import Transformed data c_m_TRANSFORMED &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;) Class &lt;- as.factor(c_m_TRANSFORMED$Class) 3.3.1 Check Transformed dataframe dimensions dim(c_m_TRANSFORMED) ## [1] 2340 23 3.3.2 Check Transformed for missing values apply(is.na(c_m_TRANSFORMED), 2, which) ## integer(0) No missing values found. 3.3.3 Count Transformed data for the number of polypeptides per class Number of polypeptides per Class: Class 0 = Control, Class 1 = Myoglobin ## ## 0 1 ## 1216 1124 3.3.4 Visualization Descriptive Statistics, TRANSFORMED data Formulas for mean: \\[\\begin{equation} E[X] = \\sum_{i=1}^n x_i p_i ~~; ~~~~~~ \\bar x = \\frac {1}{n} \\sum_{i=1}^n x_i \\end{equation}\\] 3.3.5 Scatter plot of means of Myoglobin-Control amino acid composition \\(\\sqrt x_i\\), TRANSFORMED data This plot shows the means for each feature (column-means) in the dataset. The means represent the ungrouped or total of all proteins (where n=2340) versus AA type. # A-4 ## Grouped barchart of $\\sqrt x_i$ Transformed amino acid vs. ## protein category data barplot(percent_aa, main = &quot;Mean % A.A.Composition, TRANSFORMED data&quot;, ylab = &quot;% AA Composition&quot;, ylim = c(0, 30), col = colorRampPalette(brewer.pal(4, &quot;Blues&quot;))(3), legend = T, beside = T) 3.3.6 Grouped bar chart of means for percent amino acid composition of Transformed Data; control &amp; myoglobin categories 3.3.7 Boxplots of grand-means of the overall amino acid composition of square-root transformed data 3.3.8 Boxplots of amino acid compositions for control (only) of square-root transformed data 3.3.9 Boxplots of amino acid compositions for myoglobin of square-root transformed Data(only), TRANSFORMED data 3.3.10 Boxplots Of Length Of Polypeptides Of Transformed Data; Myoglobin, Control &amp; Combined 3.3.11 Coefficient of Variance (CV), TRANSFORMED data Standard deviations are sensitive to scale. Therefore I compare the normalized standard deviations. This normalized standard deviation is more commonly called the coefficient of variation (CV). \\[\\begin{equation} CV = \\frac {\\sigma (x)} {E [|x|]} ~~~ where ~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} CV ~~=~~ \\frac{1}{\\bar x} \\cdot \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2} \\end{equation}\\] 3.3.12 Plot of Coefficient Of Variance (CV) AA_var_norm ## A C D E F G H I K ## 0.6095112 0.8729758 0.5478540 0.4156102 0.2815745 0.5201625 0.7966296 0.2999687 0.4689544 ## L M N P Q R S T V ## 0.3215591 0.6529752 0.7352478 0.7383244 0.5752622 0.7680977 0.4948690 0.5830352 0.4420595 ## W Y ## 0.9461276 0.8461615 3.3.13 Skewness of distributions, TRANSFORMED data \\[\\begin{equation} Skewness ~= E\\left[ \\left( \\frac{X - \\mu}{\\sigma(x)} \\right)^3 \\right] ~~~~ where ~~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} Skewness ~= \\frac { \\frac{1}{n} \\sum^n_{i=1} (x_i - \\bar x)^3 } { \\left( \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2 } \\right) ^ {3}} \\end{equation}\\] Skewness values for each A.A. by Class of square-root transformed data AA_skewness ## A C D E F G H ## 0.670502595 0.347813248 -0.058540442 1.782876260 -0.102739748 0.091338300 1.135783661 ## I K L M N P Q ## 0.293474879 0.223433207 -0.172566877 0.744002991 0.633532783 1.493903282 0.306716333 ## R S T V W Y ## 1.241930812 1.448521897 -0.006075043 1.338971930 1.831047440 1.694362388 3.3.14 Determine coefficients of correlation, TRANSFORMED data An easily interpretable test is a correlation 2D-plot for investigating multicollinearity or feature reduction. Fewer attributes “means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they measure the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions.” 10 Pearson’s correlation coefficient: \\[\\begin{equation} \\rho_{x,y} = \\frac {E \\left[(X - \\mu_x)(X - \\mu_y) \\right]} {\\sigma_x \\sigma_y} \\end{equation}\\] \\[\\begin{equation} r_{xy} = \\frac {\\sum^n_{i=1} (x_i - \\bar x)(y_1 - \\bar y)} { {\\sqrt {\\sum^n_{i=1} (x_i - \\bar x)^2 }} {\\sqrt {\\sum^n_{i=1} (y_i - \\bar y)^2 }} } \\end{equation}\\] c_m_corr_mat[&quot;T&quot;, &quot;N&quot;] ## [1] 0.7098085 No values in the correlation matrix meet the 0.75 cut off criteria for problems. 3.3.15 Boruta - Dimensionality Reduction, TRANSFORMED data Perform Boruta search NOTE: mcAdj = TRUE: If True, multiple comparisons will be adjusted using the Bonferroni method to calculate p-values. Therefore, \\(p_i \\leq \\frac {\\alpha} {m}\\) where \\(\\alpha\\) is the desired p-value and \\(m\\) is the total number of null hypotheses. set.seed(1000) #registerDoMC(cores = 3) # Start multi-processor mode start_time &lt;- Sys.time() # Start timer boruta_output &lt;- Boruta(Class ~ ., data = c_m_class_20[, -1], mcAdj = TRUE, # See Note above. doTrace = 1) # doTrace = 1, represents non-verbose mode. #registerDoSEQ() # Stop multi-processor mode end_time &lt;- Sys.time() # End timer end_time - start_time # Display elapsed time ## Time difference of 28.80269 secs 3.3.16 Plot Variable Importance, TRANSFORMED data plot(boruta_output, cex.axis = 1, las = 2, ylim = c(-5, 50), main = &quot;Variable Importance, TRANSFORMED data(Bigger=Better)&quot;) 3.3.17 Variable Importance Scores, TRANSFORMED data roughFixMod &lt;- TentativeRoughFix(boruta_output) imps &lt;- attStats(roughFixMod) imps2 &lt;- imps[imps$decision != &quot;Rejected&quot;, c(&quot;meanImp&quot;, &quot;decision&quot;)] meanImps &lt;- imps2[order(-imps2$meanImp), ] # descending sort kable(meanImps) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;)) meanImp decision R 43.17613 Confirmed H 34.30370 Confirmed P 28.70674 Confirmed C 27.72357 Confirmed K 27.60838 Confirmed E 26.18872 Confirmed Y 22.84975 Confirmed T 21.66359 Confirmed S 21.44119 Confirmed A 20.54316 Confirmed N 20.10100 Confirmed V 18.77068 Confirmed I 18.69155 Confirmed F 18.18632 Confirmed D 17.64435 Confirmed G 16.15207 Confirmed W 15.77085 Confirmed L 15.27614 Confirmed M 14.83421 Confirmed Q 14.12976 Confirmed # knitr::kable(meanImps, # full_width = F, # position = &quot;left&quot;, # caption = &quot;Mean Importance Scores &amp; Decision, TRANSFORMED data&quot;) The Boruta Random Rorest test shows that all features are essential therefore none should be dropped from TRANSFORMED data. 3.4 EDA Conclusion 3.4.1 Feature Selection &amp; Extraction It was determined early on that three amino acids (C, F, I) from the data amino acid percent compositions (c_m_RAW_AAC.csv) had Skewness greater than two. It was found that tranforming the features using the square root function lowered the skewness to {-0.102739 \\(\\leq\\) skew after transformation \\(\\leq\\) 0.3478132}. Table 7.1, Skewness Before And After Square-Root Transform Amino Acid Initial Skewness Skew After Square-Root Transform C, Cysteine 2.538162 0.347813248 F, Phenolalanine 2.128118 -0.102739748 I, Isoleucine 2.192145 0.293474879 The transformations of the three amino acids (C, F, I) did not appriciably change the Correlation coefficient, R. Therefore no R values were above 0.75 before or after testing. The highest coeffiecient of correlation being Threonine and Argnine with an R of 0.7098. This indicates that no features are collinear. Therefore the transformed data is used throughout this experiment. 3.4.2 Information Block** How to: Dimension Reduction using High Correlation How to reduce features given high correlation (|R| &gt;= 0.75) {-} Calculate the correlation matrix of the predictors. If the correlation plot produced of any two variables is greater than or equal to (|R| &gt;= 0.75), then we could consider feature elimination. This interesting heuristic approach would be used for determining which feature to eliminate.11 Determine if the two predictors associated with the most significant absolute pairwise correlation (R &gt; |0.75|), call them predictors A and B. Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a more significant average correlation, remove it; otherwise, remove predictor B. Repeat Steps 2–4 until no absolute correlations are above the threshold. An alternative test for variable importance carried out is called Boruta. Boruta builds Random Forests then “finds relevant features by comparing original attributes’ importance with importance achievable at random.” 12 Boruta is used for dimensionality reduction of the c_m_Transformed data. Bortua showed that all dependent features are essential for the generation of a Random Forest Decision Tree. It would wise to keep all features for that model test and throughout the generation of other models. All features have decisive mean importance, which is generated by a Gini calculation. https://en.wikipedia.org/wiki/Exploratory_data_analysis↩ https://www.itl.nist.gov/div898/handbook/↩ Roger Peng, Exploratory Data Analysis with R, https://leanpub.com/exdata, 2016↩ Ronald Pearson, Exploratory Data Analysis Using R, CRC Press, ISBN:9781138480605, 2018↩ Miron Kursa, Witold Rudnicki, Feature Selection with the Boruta Package, DOI:10.18637/jss.v036.i11, 2010↩ Ronald Pearson, Exploratory Data Analysis Using R, CRC Press, ISBN:9781138480605, 2018↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, P.43↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, P.47 (http://appliedpredictivemodeling.com/)↩ https://notabug.org/mbq/Boruta/↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, (http://appliedpredictivemodeling.com/)↩ https://notabug.org/mbq/Boruta/↩ "]
]
