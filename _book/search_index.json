[
["index.html", "Comparison of Binary Classification Using Six Machine Learning Methods 1 Preface", " Comparison of Binary Classification Using Six Machine Learning Methods Matthew C. Curcio February 20, 2020 1 Preface My work for this project started off as a hands-on Linux command line guide for new biology students. As I kept reading, I realized that I wanted to teach myself Machine Learning instead. This booklet is only part of what I have learned… Who This Book Is For I consider this booklet very informal as is my conversational style. It is a record of what I believed would help me in the future and what intereted me. I hope/plan on using this booklet as a template for other data science projects I will do in the near future. Acknowledgments I would like to thank Thor, Zeus, Jupiter, Buddha and Mars for their patience and generosity. Dedication To The Socratic Paradox, I know that I know nothing About the Author Matthew C. Curcio is a Chemist, Biochemist, Teacher and Student who loves science slightly more than teaching children. "],
["introduction.html", "2 Introduction 2.1 What is Machine Learning? 2.2 What is Predictive Modeling? 2.3 The Epicycle of Analysis 2.4 Predictive Modeling 2.5 Four Challenges In Predictive Modeling 2.6 SECTION TITLE (??)", " 2 Introduction At the intersection between Applied Mathematics, Computer Science, and Biological Sciences is a subset of knowledge known as Bioinformatics. Some find Bioinformatics and its relative Data Science difficult to define. But the most ubiquitous pictograph of Data Science indeed says a thousand words if we cannot. See Figure 1. More generally Data Science is a mixture using biological data analysis, computers, software and importantly statistical or predictive modeling to describe a narrative of some systematic research. For some Bioinformaticians the question is ’How can we use available data? describe it? model it by using applied mathematics? DNA and Proteins are coded strands of information which can be categorized and enumerated in a myriad ways. Other currently popular fields one may come across are the study of chemistry using applied mathematics and computers which beget the field of Chemoinformatics.1 2 While the career path of Health or Healthcare begets the field of Healthcare-Informatics.3 Venn Diagrams of Bioinformatics Vs Data Science 4 2.1 What is Machine Learning? “Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions” — Ian Goodfellow, et al5 2.2 What is Predictive Modeling? The term ‘Predictive Modeling’ should bring to mind work in the computer science field, also called Machine Learning (ML), Artificial Intelligence (AI), Data Mining, Knowledge discovery in databases (KDD), and possibly even encompassing Big Data as well. “Indeed, these associations are appropriate, and the methods implied by these terms are an integral piece of the predictive modeling process. But predictive modeling encompasses much more than the tools and techniques for uncovering patterns within data. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model’s prediction accuracy on future, yet-to-be-seen data.” — Max Kuhn 6 As an aside, I use Predictive Modeling and Machine Learning interchangeably in this document. In the booklet entitled “The Elements of Data Analytic Style,” 7 there is an useful checklist for the uninitiated into the realm of science report writing and, indeed, scientific thinking. A shorter, more succinct listing of the steps, which I prefer, and is described by Roger Peng in his book, The Art Of Data Science. The book lists what he describes as the “Epicycle of Analysis.” 8 2.3 The Epicycle of Analysis Stating and refining the question Exploring the data Building formal statistical models Interpreting the results Communicating the results 2.4 Predictive Modeling In general, there are three types of Predictive Modeling or Machine Learning approaches; Supervised, Unsupervised, Reinforcement. For the sake of this brevity, only Supervised &amp; Unsupervised learning are discussed in this document. 2.4.1 Supervised Learning In supervised learning, data consists of observations \\(X_i\\) (where \\(X\\) may be a matrix of values) that also contains a corresponding label, \\(y_i\\). The label \\(y\\) maybe anyone of \\(C\\) classes. In our case of a binary classifier, we have {‘Is myoglobin’, ‘Is control’}. Data set: \\((X_1, y_1), (X_2 , y_2), ~. . ., ~(X_N , y_N); ~~~y \\in \\{1, ..., ~C\\}\\), where \\(C\\) is the number of classes A machine learning algorithm determines a pattern from the input information and groups this with its necessary title or classification. One example might be that we require a machine that separates red widgets from blue widgets. One predictive algorithm is called a K-Nearest Neighbor (K-NN) algorithm. K-NN looks at an unknown object and then proceeds to calculate the distance (most commonly, the euclidean distance) to the \\(K\\) nearest neighbors. If we consider the figure below and choose \\(K\\) = 3, we would find a circumstance as shown. In the dark solid black on the K-Nearest-Neighbor figure, we find that the green widget is nearest to two red widgets and one blue widget. In the voting process, the K-NN algorithm (2 reds vs. 1 blue) means that the consignment of our unknown green object is red. For the K-NN algorithm to function, the data optimally most be complete with a set of features and a label of each item. Without the corresponding label, a data scientist would need different criteria to track the widgets. Five of the six algorithms that this report investigates are supervised. Logit, support vector machines, and the neural network that I have chosen require labels for the classification process. K-Nearest-Neighbor 9 What is a shallow learner? Let us investigate the K-NN algorithm and figure a little further. If we change our value of \\(K\\) to 5, then we see a different result. By using \\(K = 5\\), we consider the out dashed-black line. This more considerable \\(K\\) value contains three blue widgets and two red widgets. If we ask to vote on our choice, we find that 3 blue beats the 2 red, and we assign the unknown a BLUE widget. This assignment is the opposite of the inner circle. If a researcher were to use K-NN, then the algorithm would have to test many possible \\(K\\) values and compare the results, then choose the \\(K\\) with the highest accuracy. However, this is where K-NN falters. The K-NN algorithm needs to keep all data points used for its initial training (accuracy testing). Any new unknowns could be conceivably tested against any or all the previous data points. The K-NN does use a generalized rule that would make future assignments quick on the contrary. It must memorize all the points for the algorithm to work. K-NN cannot delete the points until it is complete. It is true that the algorithm is simple but not efficient. Matter and fact, as the number of feature dimensions increases, this causes the complexity (also known as Big O) to rise. The complexity of K-NN is \\(O(K-NN) ~\\propto ~nkd\\). Where \\(n\\) is the number of observations, \\(k\\) is the number of nearest neighbors it must check, and d is the number of dimensions.10 Given that K-NN tends to ‘memorize’ its data to complete its task, it is considered a lazy and shallow learner. Lazy indicates that the decision is left to the moment a new point is learned of predicted. If we were to use a more generalized rule, such as {Blue for (\\(x &lt;= 5\\))} this would be a more dynamic and more in-depth approach by comparison. 2.4.2 Unsupervised Learning In contrast to the supervised learning system, unsupervised learning does not require a label for it to operate. Data set: \\((X_1), (X_2), ~. . ., ~(X_N)\\) where \\(X\\) may represent a matrix (\\(m\\) observations by \\(n\\) features) of values. Principal Component Analysis is an example of unsupervised learning, which we discuss in more detail in chapter 3. The data, despite or without its labels, are transformed to provide maximization of the variances in the dataset. Yet another objective of Unsupervised learning is to discover “interesting structures”11 in the data. There are several methods that show structure. These include clustering, knowledge discovery of latent variables, or discovering graph structure. In many instances and as a subheading to the aforementioned points, unsupervised learning can be used for dimension reduction or feature selection. Among the simplest unsupervised learning algorithms is K-means. K-means does not rely on the class labels of the dataset at all. K-means may be used to determine any number of classes despite any predetermined values. K-means can discover clusters later used in classification or hierarchical feature representation. K-means has several alternative methods but, in general, calculates the distance (or conversely the similarity) of observations to a mean value of the \\(K\\)th grouping. The mean value is called the center of mass, the Physics term that provides an excellent analogy since the center of mass is a weighted average. By choosing a different number of groupings (values of \\(K\\), much like the K-NN), then comparing the grouping by a measure of accuracy, one example being, mean square error. K-Means 12 It is easy to see through much or machine learning or predictive modeling if one understands bits of the inner workings of these algorithms, 2.5 Four Challenges In Predictive Modeling To many predictive modeling is a panacea for all sorts of issues. Although it does show promise, some hurdles need research. Martin Jaggi13 has summarized four points that elucidate current problems in the field that need research. Problem 1: The vast majority of information in the world is unlabeled, so it would be advantageous to have a good Unsupervised machine learning algorithms to use. Problem 2: Algorithms are very specialized, too specific. Problem 3: Transfer learning to new environments Problem 4: Scale, the scale of information is vast in reality, and we have computers that work in gigabytes, not the Exabytes that humans may have available to them. The scale of distributed Big Data The specific predictive models which are executed in this report are discussed in further detail in their own sections. ========================================== 2.6 SECTION TITLE (??) Therefore let us start by posing a question; Is there a correlation between the data points, which are outliers from principal component analysis (PCA), and 6 types of predictive modeling? This experiment is interested in determining if PCA would provide information on the false-positives and false-negatives that were an inevitable part of model building and optimization. The six predictive models that have chosen for this work are Logistic Regression, Support Vector Machines (SVM) (linear, polynomial, and radial basis function kernels), Random Forest, and a Neural Network which uses Auto-encoding. It is common for Data Scientists to test their data sets for feature importance and feature selection. One test that has interested this researcher is Principal component analysis. It can be a useful tool. PCA is an unsupervised machine learning technique which “reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs.” 14 However, the results that it provides may not be immediately intuitive to the layperson. How do the advantages and disadvantages of using PCA compare with other machine learning techniques? The advantages are numerable. They include dimensionality reduction and filtering out noise inherent in the data, and it may preserve the global structure of the data. Does the global and graphical structure of the data produced by the first two principal components provide any insights into how the predictive models of Logistic Regression, Neural Networks utilizing auto-encoders, Support Vector Machines, and Random Forest? In essence, is PCA sufficiently similar to any of the applied mathematics tools of more advanced approaches? Also, this work is to teach me machine learning or predictive modeling techniques. The data for this study is from the Uniprot database. From the Uniprot database was queried for two protein groups. The first group was Myoglobin, and the second was a control group comprised of human proteins not related to Hemoglobin or Myoglobin. See Figure There have been a group of papers that are striving to classify types of proteins by their amino acid structure alone. The most straightforward classification procedures involve using the percent amino acid composition (AAC). The AAC is calculated by using the count of an amino acid over the total number in that protein. Percent Amino Acid Composition: \\[\\begin{equation} \\%AAC_X ~=~ \\frac{N_{Amino~Acid~X}}{Total ~ N ~ of ~ AA} \\end{equation}\\] The Exploratory Data Analysis determines if features were skewed and needed must be transformed. In a random system where amino acids were chosen at random, one would expect the percent amino acid composition to be close to 5%. However, this is far from the case for the Myoglobin proteins or the control protein samples. Mean % Amino Acid Compositions for Control &amp; Myoglobin 2.6.1 Experimental Procedure The experimental procedure is broken into 3 significant steps. 2.6.1.1 Exploratory Data Analysis (EDA) During EDA, the data is checked for irregularities, such as missing data, outliers among features, skewness, and visually for normality using QQ-plots. The only irregularity that posed a significant issue was the skewness of the amino acid features. Many of 20 amino acid features had a significant number of outliers, as seen by Boxplot analysis. However, only three features had skew, which might have presented a problem. Dealing with the skew of the AA was necessary since Principal Component Analysis was a significant aspect of this experiment. Testing determined earlier that three amino acids (C, F, I) from the single amino acid percent composition needs transformation by using the square root function. The choice of transformations was Natural log, log base 10, squaring (\\(x^2\\)), and using the reciprocal (\\(1 / x\\)) of the values. The square root transformation lowered the skewness to values of less than 1.0 from high points of greater than 2 in all three cases to {-0.102739 \\(\\leq\\) skew after transformation \\(\\leq\\) 0.3478132}. Amino Acid Initial skewness Skew after square root transform C, Cysteine 2.538162 0.347813248 F, Phenolalanine 2.128118 -0.102739748 I, Isoleucine 2.192145 0.293474879 Three transformations take place for this dataset. ~/00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv and used throughout the rest of the analysis. All work uses R15, RStudio16 and a machine learning library/framework caret17. 2.6.1.2 Caret library for R The R/caret library is attractive to use for many reasons. It currently allows 238 machine learning models that use different options and data structures. 18 The utility of caret is that it organizes the input and output into a standard format making the need for learning only one grammar and syntax. Caret also harmonizes the use of hyper-parameters. Work becomes reproducible. 2.6.1.3 Training the Predictive model Setting up the training section for caret, for this experiment, can be broken into three parts. 2.6.1.3.1 Tuning Hyper-parameters The tune.grid command set allows a researcher to experiment by varying the hyper-parameters of the given model to investigate optimum values. Currently, there are no algorithms that allow for the quick and robust tuning of parameters. Instead of searching, a sizable experimental space test searches an n-dimensional grid in a full factorial design if desired. Although some models have many parameters, the most common one is to search along a cost hyper-parameter. The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. 19 The cost function (a term derived from business modeling, i.e., optimizing the cost) is an estimate as to how well models predicted value fits from the actual value. A typical cost function is the squared error function. Example Cost Function: 20 \\[\\begin{equation} Cost ~=~ \\left ( y_i - \\hat f(x_i) \\right )^2 \\end{equation}\\] It may be important to search the literature to determine if other researchers have used a specific range of optimum value, which may speed a search. For example, C.W. Hsu et al. suggest using a broad range of 20 orders of magnitude of powers of 2, e.g. cost = {\\(2^{-5}, 2^{-3}, ..., 2^{15}\\)} for an initial gross search then switch to 4 or 5 orders of magnitude with 1/4 log steps. e.g. cost = {\\(2^{1}, 2^{1.25}, ..., 2^{5}\\)} for a fine search for unknown SVM using a radial basis function. 21 2.6.1.3.2 k-Fold Cross validation of results Another valuable option that caret has is the ability to cross-validate results. Cross-validation is a statistical method used to estimate the skill of machine learning models.22 “The samples are randomly partitioned into k sets of roughly equal size. A model is fit using all samples except the first subset (called the first fold). The held-out samples are used for prediction by the recent model. The performance estimate measures the accuracy of the”out of bag&quot; or “held out” samples. The first subset is returned to the training set, and the procedure repeats with the second subset held out, and so on. The k resampled estimates of performance are summarized (usually with the mean and standard error) and used to understand the relationship between the tuning parameter(s) and model utility.&quot; 23 Cross-validation has the advantage of using the entire dataset for training and testing, increasing the opportunity that more training samples produce a better model. Example R/caret code: ## 10 fold Cross Validation repeated 5 times fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, # Type of Cross-Validation number = 10, # Number of splits repeats = 5, # Number of 10 times validations savePredictions = &quot;final&quot;) # Save all predictions found during C.V. testing 2.6.1.3.3 Train command The training command produces an object of the model. The first line should point out the “formula,” which is modeled. The dependent variable is first. The ~ (Tilda sign) indicates a model is called. Then the desired features can be listed or abbreviated with the all (.) sign. Example Train command: model_object &lt;- train(Class ~ ., # READ: Class is modeled by all features. data = training_set, # data used trControl = fitControl, # Train control allows Cross Validation setup method = &quot;svmLinear&quot;, # Use any method from 238 caret utilizes tune.Grid = grid) # Hyperparameter scouting and exploration 2.6.1.4 Analysis of results In binary classification, a two by two contingency table describes predicted versus actual value classifications. This table is also known as a confusion matrix for machine learning students. 2 x 2 Confusion Matrix Actual = 0 Actual = 1 Predicted = 0 True-Negatives False-Negatives Predicted = 1 False-Positives True-Positives There are many ways to describe the results further using this confusion matrix. However, Accuracy is used for all comparisons. \\[\\begin{equation} Accuracy ~=~ \\frac{TP + TN}{N_{Total}} \\end{equation}\\] The second goal of this experiment is to produce the False Positives and False-Negatives and evaluating these by comparing them to the Principal Component Analysis Biplot of the first two Principal Components. https://www.acs.org/content/acs/en/careers/college-to-career/chemistry-careers/cheminformatics.html↩ https://jcheminf.biomedcentral.com/↩ https://www.usnews.com/education/best-graduate-schools/articles/2014/03/26/consider-pursuing-a-career-in-health-informatics↩ http://omgenomics.com/what-is-bioinformatics/↩ Ian Goodfellow, Yoshua Bengio, Aaron Courville, ‘Deep Learning’, MIT Press, 2016, http://www.deeplearningbook.org↩ Max Kuhn, Kjell Johnson, Applied Predictive Modeling, Springer, ISBN:978-1-4614-6848-6, 2013↩ Jeff Leek, The Elements of Data Analytic Style, A guide for people who want to analyze data., Leanpub Books, http://leanpub.com/datastyle, 2015↩ Roger D. Peng and Elizabeth Matsui, The Art of Data Science, A Guide for Anyone Who Works with Data, Leanpub Books, http://leanpub.com/artofdatascience, 2015↩ https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm↩ Olga Veksler, Machine Learning in Computer Vision, http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf↩ Kevin Murphy, Machine learning a probabilistic perspective, 2012, ISBN 978-0-262-01802-9↩ https://www.slideshare.net/teofili/machine-learning-with-apache-hama/20-KMeans_clustering_20↩ https://www.machinelearning.ai/machine-learning/4-big-challenges-in-machine-learning-ft-martin-jaggi-2/↩ Jake Lever, Martin Krzywinski, Naomi Altman, Principal component analysis, Nature Methods, Vol.14 No.7, July 2017, 641-2↩ https://cran.r-project.org/↩ https://rstudio.com/↩ http://topepo.github.io/caret/index.html↩ http://topepo.github.io/caret/available-models.html↩ Ian Goodfellow, Yoshua Bengio, Aaron Courville, Deep Learning, MIT Press, http://www.deeplearningbook.org, 2016↩ Roberto Battiti and Mauro Brunato, The LION way. Machine Learning-Intelligent Optimization, LIONlab, University of Trento, Italy“, 2017”, http://intelligent-optimization.org/LIONbook↩ Chih-Wei Hsu, et al., A Practical Guide to Support Vector Classification, 2016, http://www.csie.ntu.edu.tw/~cjlin↩ https://machinelearningmastery.com/k-fold-cross-validation/↩ Max Kuhn, Kjell Johnson, Applied Predictive Modeling, 2013, ISBN 978-1-4614-6848-6↩ "],
["exploratory-data-analysis.html", "3 Exploratory Data Analysis 3.1 Introduction 3.2 Analysis of RAW data 3.3 Analysis of TRANSFORMED data 3.4 EDA Conclusion", " 3 Exploratory Data Analysis “Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.” 24 3.1 Introduction The experiment described herein involves taking groups of proteins from the Uniprot.org database and comparing how well different machine learning techniques do at separating the positive from the negative control grouping. In this circumstance, proteins from the myoglobin family are analyzed against randomly chosen human proteins, which are not related to hemoglobin or myoglobin. This work is to characterize the outliers derived from PCA and compare them to the false-positives and false-negatives generated from each of 6 machine learning approaches produces; Logit, SVM-Linear, SVM-polynomial, SVM-RBF, Neural Network. 3.1.1 Four-Step Analysis At this stage, data is inspected in a careful and structured way. Hence, I have chosen a four-step process: Hypothesize Summarize Visualize Normalize 3.1.2 Useful Guides for Exploratory Data Analysis The summarization of the amino acid dataset is based on a hybrid set of guidelines; NIST Handbook of Statistics, 25 Roger Peng’s booklet on ‘Exploratory Data Analysis with R,’ 26 ‘Exploratory Data Analysis Using R,’ by Ronald K. Pearson. 27 3.1.3 Questions During EDA Although exploratory data analysis does not always have a formal hypothesis testing portion, I do, however, pose several questions concerning the structure, quality, and types of data. Do the independent variables of this study have large skewed distributions? 1.1 If skews are greater than 2.0, then can a transformation be used for normalization? 1.2 Determine what transformation to use? Can Feature Selection be used, and which procedures are appropriate? 2.1 Use the Random Forest technique known as Boruta 28 for feature importance or reduction? 2.2 Will coefficients of correlation (R) find collinearity and reduce the number of features? 2.3 Will principal component analysis (PCA) be useful in finding hidden structures of patterns? 2.4 Can PCA be used successfully for Feature Selection? What is the structure of the data? 3.1 Is the data representative of the entire experimental space? 3.2 Is missing data an issue? 3.3 Does the data have certain biases, either known or unknown? 3.4 What relationships do we expect from these variables? 29 3.2 Analysis of RAW data # Import libraries Libraries &lt;- c(&quot;knitr&quot;, &quot;readr&quot;, &quot;RColorBrewer&quot;, &quot;corrplot&quot;, &quot;doMC&quot;, &quot;Boruta&quot;) for (i in Libraries) { library(i, character.only = TRUE) } # Import RAW data c_m_RAW_AAC &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_RAW_AAC.csv&quot;) Class &lt;- as.factor(c_m_RAW_AAC$Class) 3.2.1 Visually inspect RAW data files Use the command-line interface followed by the command less. Check for binary instead of ASCII and bad Unicode. 3.2.2 Inspect RAW dataframe structure, str() ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2340 obs. of 23 variables: ## $ Class : num 0 0 0 0 0 0 0 0 0 0 ... ## $ TotalAA: num 226 221 624 1014 699 ... ## $ PID : chr &quot;C1&quot; &quot;C2&quot; &quot;C3&quot; &quot;C4&quot; ... ## $ A : num 0.2655 0.2081 0.0433 0.0661 0.0644 ... ## $ C : num 0 0 0.00962 0.01381 0.03577 ... ## $ D : num 0.00442 0.00452 0.04647 0.06114 0.02861 ... ## $ E : num 0.031 0.0271 0.0833 0.074 0.0472 ... ## $ F : num 0.00442 0.00452 0.02564 0.02959 0.06295 ... ## $ G : num 0.0708 0.0769 0.0817 0.07 0.0443 ... ## $ H : num 0 0 0.0176 0.0187 0.0157 ... ## $ I : num 0.00885 0.0181 0.03045 0.04734 0.0701 ... ## $ K : num 0.28761 0.27602 0.00962 0.12426 0.05579 ... ## $ L : num 0.0442 0.0452 0.0577 0.0888 0.1359 ... ## $ M : num 0.00442 0.00452 0.01442 0.02465 0.02289 ... ## $ N : num 0.0177 0.0136 0.0641 0.0355 0.0558 ... ## $ P : num 0.0841 0.0995 0.0449 0.0434 0.0472 ... ## $ Q : num 0.00442 0.00905 0.04327 0.03353 0.02861 ... ## $ R : num 0.0133 0.0181 0.1202 0.0325 0.0415 ... ## $ S : num 0.0575 0.0724 0.1875 0.0838 0.0787 ... ## $ T : num 0.0531 0.0633 0.0625 0.0414 0.0744 ... ## $ V : num 0.0442 0.0543 0.0385 0.0671 0.0458 ... ## $ W : num 0 0 0.00481 0.01282 0.00715 ... ## $ Y : num 0.00442 0.00452 0.01442 0.03156 0.0372 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Class = col_double(), ## .. TotalAA = col_double(), ## .. PID = col_character(), ## .. A = col_double(), ## .. C = col_double(), ## .. D = col_double(), ## .. E = col_double(), ## .. F = col_double(), ## .. G = col_double(), ## .. H = col_double(), ## .. I = col_double(), ## .. K = col_double(), ## .. L = col_double(), ## .. M = col_double(), ## .. N = col_double(), ## .. P = col_double(), ## .. Q = col_double(), ## .. R = col_double(), ## .. S = col_double(), ## .. T = col_double(), ## .. V = col_double(), ## .. W = col_double(), ## .. Y = col_double() ## .. ) 3.2.3 Check RAW data head &amp; tail head(c_m_RAW_AAC, n = 2) ## # A tibble: 2 x 23 ## Class TotalAA PID A C D E F G H I K L M N ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 226 C1 0.265 0 0.00442 0.0310 0.00442 0.0708 0 0.00885 0.288 0.0442 0.00442 0.0177 ## 2 0 221 C2 0.208 0 0.00452 0.0271 0.00452 0.0769 0 0.0181 0.276 0.0452 0.00452 0.0136 ## # … with 8 more variables: P &lt;dbl&gt;, Q &lt;dbl&gt;, R &lt;dbl&gt;, S &lt;dbl&gt;, T &lt;dbl&gt;, V &lt;dbl&gt;, W &lt;dbl&gt;, Y &lt;dbl&gt; tail(c_m_RAW_AAC, n = 2) ## # A tibble: 2 x 23 ## Class TotalAA PID A C D E F G H I K L M ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 335 M1123 0.0567 0.00299 0.0537 0.0716 0.0507 0.0507 0.0388 0.0776 0.0716 0.0687 0.0179 ## 2 1 43 M1124 0.0698 0 0.116 0.116 0.0930 0.0465 0 0.0233 0.0233 0.0698 0.0698 ## # … with 9 more variables: N &lt;dbl&gt;, P &lt;dbl&gt;, Q &lt;dbl&gt;, R &lt;dbl&gt;, S &lt;dbl&gt;, T &lt;dbl&gt;, V &lt;dbl&gt;, W &lt;dbl&gt;, ## # Y &lt;dbl&gt; 3.2.4 Check RAW data types is.data.frame(c_m_RAW_AAC) ## [1] TRUE class(c_m_RAW_AAC$Class) # Col 1 ## [1] &quot;numeric&quot; class(c_m_RAW_AAC$TotalAA) # Col 2 ## [1] &quot;numeric&quot; class(c_m_RAW_AAC$PID) # Col 3 ## [1] &quot;character&quot; class(c_m_RAW_AAC$A) # Col 4 ## [1] &quot;numeric&quot; 3.2.5 Check RAW dataframe dimensions dim(c_m_RAW_AAC) ## [1] 2340 23 3.2.6 Check RAW for missing values No missing values found. apply(is.na(c_m_RAW_AAC), 2, which) ## integer(0) # sapply(c_m_RAW_AAC, function(x) sum(is.na(x))) # Sum up NA by columns # c_m_RAW_AAC[rowSums(is.na(c_m_RAW_AAC)) != 0,] # Show rows where NA&#39;s is not zero 3.2.7 Number of polypeptides per Class: Class 0 = Control, Class 1 = Myoglobin ## ## 0 1 ## 1216 1124 3.2.8 Numerical summary of RAW features ## Class TotalAA PID A C ## Min. :0.0000 Min. : 2.0 Length:2340 Min. :0.00000 Min. :0.000000 ## 1st Qu.:0.0000 1st Qu.: 109.8 Class :character 1st Qu.:0.05108 1st Qu.:0.000000 ## Median :0.0000 Median : 154.0 Mode :character Median :0.07364 Median :0.007034 ## Mean :0.4803 Mean : 353.8 Mean :0.07835 Mean :0.011970 ## 3rd Qu.:1.0000 3rd Qu.: 407.0 3rd Qu.:0.10261 3rd Qu.:0.020408 ## Max. :1.0000 Max. :4660.0 Max. :0.28000 Max. :0.159420 ## D E F G H ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.03401 1st Qu.:0.05435 1st Qu.:0.03801 1st Qu.:0.04544 1st Qu.:0.01324 ## Median :0.05195 Median :0.07143 Median :0.04545 Median :0.06394 Median :0.02297 ## Mean :0.04900 Mean :0.07451 Mean :0.05135 Mean :0.06193 Mean :0.02890 ## 3rd Qu.:0.06567 3rd Qu.:0.09091 3rd Qu.:0.05501 3rd Qu.:0.08625 3rd Qu.:0.04095 ## Max. :0.17647 Max. :0.50000 Max. :0.37500 Max. :0.36364 Max. :0.13333 ## I K L M N ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.04348 1st Qu.:0.05797 1st Qu.:0.07480 1st Qu.:0.01087 1st Qu.:0.01948 ## Median :0.05992 Median :0.08182 Median :0.09136 Median :0.01948 Median :0.04145 ## Mean :0.06839 Mean :0.08386 Mean :0.09313 Mean :0.01949 Mean :0.04228 ## 3rd Qu.:0.08216 3rd Qu.:0.12081 3rd Qu.:0.11688 3rd Qu.:0.02721 3rd Qu.:0.05788 ## Max. :0.50000 Max. :0.28761 Max. :0.25000 Max. :0.11111 Max. :0.12563 ## P Q R S T ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.02464 1st Qu.:0.02212 1st Qu.:0.01476 1st Qu.:0.04348 1st Qu.:0.03247 ## Median :0.03401 Median :0.03598 Median :0.03896 Median :0.05564 Median :0.05194 ## Mean :0.03825 Mean :0.03342 Mean :0.03818 Mean :0.06191 Mean :0.04838 ## 3rd Qu.:0.04772 3rd Qu.:0.04545 3rd Qu.:0.05370 3rd Qu.:0.06964 3rd Qu.:0.06522 ## Max. :0.20635 Max. :0.18182 Max. :0.24324 Max. :0.22619 Max. :0.18750 ## V W Y ## Min. :0.00000 Min. :0.000000 Min. :0.00000 ## 1st Qu.:0.04575 1st Qu.:0.001899 1st Qu.:0.01463 ## Median :0.05844 Median :0.011492 Median :0.02865 ## Mean :0.06512 Mean :0.012327 Mean :0.03644 ## 3rd Qu.:0.07405 3rd Qu.:0.017889 3rd Qu.:0.04564 ## Max. :0.20000 Max. :0.133333 Max. :0.14286 3.2.9 Visualize RAW Data With Descriptive Statistics Formulas for mean: \\[\\begin{equation} E[X] = \\sum_{i=1}^n x_i p_i ~~; ~~~~~~ \\bar x = \\frac {1}{n} \\sum_{i=1}^n x_i \\end{equation}\\] 3.2.10 Scatter plot of means of Myoglobin-Control amino acid composition of c_m_RAW_AAC dataframe This Scatter-plot shows the means for each feature (column-means) in the dataset. The means represent the ungrouped or total of all proteins (where n = 2340) versus AA type. 3.2.11 Means of percent amino acid composition of control &amp; myoglobin categories, RAW data 3.2.12 Boxplots of grand-means of overall amino acid composition, RAW data 3.2.13 Boxplots of amino acid compositions for control (only), RAW data 3.2.14 Boxplots of amino acid compositions for myoglobin (only), RAW data 3.2.15 Boxplots Of Length Of Polypeptides For Combined RAW Data 3.2.16 Plot Coefficient Of Variance For RAW Data Standard deviations are sensitive to scale. Therefore I compare the normalized standard deviations. This normalized standard deviation is more commonly called the coefficient of variation (CV). \\[\\begin{equation} CV = \\frac {\\sigma (x)} {E [|x|]} ~~~ where ~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} CV ~~=~~ \\frac{1}{\\bar x} \\cdot \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2} \\end{equation}\\] AA_var_norm ## A C D E F G H I K L ## 0.6095112 1.2444944 0.5478540 0.4156102 0.5436243 0.5201625 0.7966296 0.6005962 0.4689544 0.3215591 ## M N P Q R S T V W Y ## 0.6529752 0.7352478 0.7383244 0.5752622 0.7680977 0.4948690 0.5830352 0.4420595 0.9461276 0.8461615 3.2.17 Skewness of distributions, RAW data \\[\\begin{equation} Skewness ~= E\\left[ \\left( \\frac{X - \\mu}{\\sigma(x)} \\right)^3 \\right] ~~~~ where ~~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} Skewness ~= \\frac { \\frac{1}{n} \\sum^n_{i=1} (x_i - \\bar x)^3 } { \\left( \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2 } \\right) ^ {3}} \\end{equation}\\] Skewness values for each A.A. are determined in totality. 3.2.18 QQ-Plots of 20 amino acids, RAW data 3.2.19 Determine coefficients of correlation, RAW data An easily interpretable test is a correlation 2D-plot for investigating multicollinearity or feature reduction. Fewer attributes “means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they measure the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions”.30 Pearson’s correlation coefficient: \\[\\begin{equation} \\rho_{x,y} = \\frac {E \\left[(X - \\mu_x)(X - \\mu_y) \\right]} {\\sigma_x \\sigma_y} \\end{equation}\\] \\[\\begin{equation} r_{xy} = \\frac {\\sum^n_{i=1} (x_i - \\bar x)(y_1 - \\bar y)} { {\\sqrt {\\sum^n_{i=1} (x_i - \\bar x)^2 }} {\\sqrt {\\sum^n_{i=1} (y_i - \\bar y)^2 }} } \\end{equation}\\] c_m_corr_mat &lt;- cor(c_m_RAW_AAC[, c(2, 4:23)], method = &quot;p&quot;) # &quot;p&quot;: Pearson test for continous variables corrplot(abs(c_m_corr_mat), title = &quot;Correlation Plot Of AAC Features&quot;, method = &quot;square&quot;, type = &quot;lower&quot;, tl.pos = &quot;d&quot;, cl.lim = c(0, 1), addgrid.col = &quot;lightgrey&quot;, cl.pos = &quot;b&quot;, # Color legend position bottom. order = &quot;FPC&quot;, # &quot;FPC&quot; = first principal component order. mar = c(1, 2, 1, 2), tl.col = &quot;black&quot;) NOTE: Amino acids shown in First Principal Component order, top to bottom. Maximum value of Correlation between T &amp; N. ## [1] 0.7098085 According to Max Kuhn 31, correlation coefficients need only be addressed if the |R| &gt;= 0.75. Therefore is no reason to consider multicollinearity. 3.2.20 How to: Dimension Reduction using High Correlation How to reduce features given high correlation (|R| &gt;= 0.75) {-} Calculate the correlation matrix of the predictors. If the correlation plot produced of any two variables is greater than or equal to (|R| &gt;= 0.75), then we could consider feature elimination. This interesting heuristic approach would be used for determining which feature to eliminate.32 Determine if the two predictors associated with the most significant absolute pairwise correlation (R &gt; |0.75|), call them predictors A and B. Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a more significant average correlation, remove it; otherwise, remove predictor B. Repeat Steps 2–4 until no absolute correlations are above the threshold. 3.2.21 Boruta - dimensionality reduction, RAW data c_m_class_20 &lt;- c_m_RAW_AAC[, -c(2, 3)] # Remove TotalAA &amp; PID Class &lt;- as.factor(c_m_class_20$Class) # Convert &#39;Class&#39; To Factor NOTE: mcAdj = TRUE, If True, multiple comparisons will be adjusted using the Bonferroni method to calculate p-values. Therefore, \\(p_i \\leq \\large \\frac {\\alpha} {m}\\) where \\(\\alpha\\) is the desired p-value and \\(m\\) is the total number of null hypotheses. set.seed(1000) registerDoMC(cores = 3) # Start multi-processor mode start_time &lt;- Sys.time() # Start timer boruta_output &lt;- Boruta(Class ~ ., data = c_m_class_20[, -1], mcAdj = TRUE, # See Note above. doTrace = 1) # doTrace = 1, represents non-verbose mode. ## After 11 iterations, +15 secs: ## confirmed 20 attributes: A, C, D, E, F and 15 more; ## no more attributes left. registerDoSEQ() # Stop multi-processor mode end_time &lt;- Sys.time() # End timer end_time - start_time # Display elapsed time ## Time difference of 14.98022 secs 3.2.22 Plot variable importance 3.2.23 Variable importance scores ## Warning in TentativeRoughFix(boruta_output): There are no Tentative attributes! Returning original ## object. Table 3.1: Mean Importance Scores &amp; Decision meanImp decision R 43.18824 Confirmed H 34.29757 Confirmed P 28.70225 Confirmed C 27.67710 Confirmed K 27.60808 Confirmed E 26.18884 Confirmed Y 22.85337 Confirmed T 21.67689 Confirmed S 21.43716 Confirmed A 20.53089 Confirmed N 20.09681 Confirmed V 18.77054 Confirmed I 18.76492 Confirmed F 18.31240 Confirmed D 17.64592 Confirmed G 16.15461 Confirmed W 15.74107 Confirmed L 15.27767 Confirmed M 14.82861 Confirmed Q 14.13939 Confirmed 3.2.24 Conclusion for Boruta random forest test All features are essential. None should be dropped. 3.2.25 Conclusions For EDA, RAW data Three amino acids (C, F, I) from the single amino acid percent composition are transformed by using the square root function. A quick investigation (data not shown) showed that a square root transformation would be sufficient. The square root transformation lowered the skewness from greater than 2 in all cases to {-0.102739 \\(\\leq\\) skew after transformation \\(\\leq\\) 0.3478132}. Protein Initial skewness Skew after square root transform C, Cysteine 2.538162 0.3478132 F, Phenolalanine 2.128118 -0.102739 I, Isoleucine 2.192145 0.2934749 3.3 Analysis of TRANSFORMED data This EDA section is a reevaluation square root transformed, c_m_RAW_ACC.csv data set, hence called c_m_TRANSFORMED.csv. The \\(\\sqrt x_i\\) Transformed data is derived from c_m_RAW_ACC.csv where the amino acids C, F, I were transformed using a square root function. This transformation was done to reduce the skewness of these samples and avoid modeling problems arising from high skewness, as seen below. Amino Acid Initial skewness Skew after square root transformation C, Cysteine 2.538162 0.3478132 F, Phenolalanine 2.128118 -0.102739 I, Isoleucine 2.192145 0.2934749 # Import Transformed data c_m_TRANSFORMED &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;) Class &lt;- as.factor(c_m_TRANSFORMED$Class) 3.3.1 Check Transformed dataframe dimensions dim(c_m_TRANSFORMED) ## [1] 2340 23 3.3.2 Check Transformed for missing values apply(is.na(c_m_TRANSFORMED), 2, which) ## integer(0) No missing values found. 3.3.3 Count Transformed data for the number of polypeptides per class Number of polypeptides per Class: Class 0 = Control, Class 1 = Myoglobin ## ## 0 1 ## 1216 1124 3.3.4 Visualization of Transformed Data Descriptive Statistics Formulas for mean: \\[\\begin{equation} E[X] = \\sum_{i=1}^n x_i p_i ~~; ~~~~~~ \\bar x = \\frac {1}{n} \\sum_{i=1}^n x_i \\end{equation}\\] 3.3.5 Scatter plot of means of Myoglobin-Control amino acid composition \\(\\sqrt x_i\\) Transformed (c_m_TRANSFORMED) dataframe This plot shows the means for each feature (column-means) in the dataset. The means represent the ungrouped or total of all proteins (where n=2340) versus AA type. 3.3.6 Grouped bar chart of means for percent amino acid composition of Transformed Data; control &amp; myoglobin categories 3.3.7 Boxplots of grand-means of the overall amino acid composition of square-root transformed data 3.3.8 Boxplots of amino acid compositions for control (only) of square-root transformed data 3.3.9 Boxplots of amino acid compositions for myoglobin of square-root transformed Data(only) of square-root transformed data 3.3.10 Boxplots Of Length Of Polypeptides Of Transformed Data; Myoglobin, Control &amp; Combined 3.3.11 Coefficient of variance (CV) Of Transformed data Standard deviations are sensitive to scale. Therefore I compare the normalized standard deviations. This normalized standard deviation is more commonly called the coefficient of variation (CV). \\[\\begin{equation} CV = \\frac {\\sigma (x)} {E [|x|]} ~~~ where ~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} CV ~~=~~ \\frac{1}{\\bar x} \\cdot \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2} \\end{equation}\\] 3.3.12 Plot of Coefficient Of Variance (CV) AA_var_norm ## A C D E F G H I K L ## 0.6095112 0.8729758 0.5478540 0.4156102 0.2815745 0.5201625 0.7966296 0.2999687 0.4689544 0.3215591 ## M N P Q R S T V W Y ## 0.6529752 0.7352478 0.7383244 0.5752622 0.7680977 0.4948690 0.5830352 0.4420595 0.9461276 0.8461615 3.3.13 Skewness of distributions Of Transformed Data \\[\\begin{equation} Skewness ~= E\\left[ \\left( \\frac{X - \\mu}{\\sigma(x)} \\right)^3 \\right] ~~~~ where ~~~~ \\sigma(x) \\equiv \\sqrt{ E[x - \\mu]^2 } \\end{equation}\\] \\[\\begin{equation} Skewness ~= \\frac { \\frac{1}{n} \\sum^n_{i=1} (x_i - \\bar x)^3 } { \\left( \\sqrt{ \\frac{1}{n-1} \\sum^n_{i=1} (x_i - \\bar x)^2 } \\right) ^ {3}} \\end{equation}\\] Skewness values for each A.A. by Class of square-root transformed data AA_skewness ## A C D E F G H I ## 0.670502595 0.347813248 -0.058540442 1.782876260 -0.102739748 0.091338300 1.135783661 0.293474879 ## K L M N P Q R S ## 0.223433207 -0.172566877 0.744002991 0.633532783 1.493903282 0.306716333 1.241930812 1.448521897 ## T V W Y ## -0.006075043 1.338971930 1.831047440 1.694362388 3.3.14 QQ Plots of 20 amino acids of Transformed data 3.3.15 Determine coefficients of correlation of Transformed Data An easily interpretable test is a correlation 2D-plot for investigating multicollinearity or feature reduction. Fewer attributes “means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they measure the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions”.33 Pearson’s correlation coefficient: \\[\\begin{equation} \\rho_{x,y} = \\frac {E \\left[(X - \\mu_x)(X - \\mu_y) \\right]} {\\sigma_x \\sigma_y} \\end{equation}\\] \\[\\begin{equation} r_{xy} = \\frac {\\sum^n_{i=1} (x_i - \\bar x)(y_1 - \\bar y)} { {\\sqrt {\\sum^n_{i=1} (x_i - \\bar x)^2 }} {\\sqrt {\\sum^n_{i=1} (y_i - \\bar y)^2 }} } \\end{equation}\\] c_m_corr_mat[&quot;T&quot;, &quot;N&quot;] ## [1] 0.7098085 No values in the correlation matrix meet the 0.75 cut off criteria for problems. 3.3.16 Boruta - dimensionality reduction of Transformed data Perform Boruta search NOTE: mcAdj = TRUE: If True, multiple comparisons will be adjusted using the Bonferroni method to calculate p-values. Therefore, \\(p_i \\leq \\frac {\\alpha} {m}\\) where \\(\\alpha\\) is the desired p-value and \\(m\\) is the total number of null hypotheses. set.seed(1000) registerDoMC(cores = 3) # Start multi-processor mode start_time &lt;- Sys.time() # Start timer boruta_output &lt;- Boruta(Class ~ ., data = c_m_class_20[, -1], mcAdj = TRUE, # See Note above. doTrace = 1) # doTrace = 1, represents non-verbose mode. registerDoSEQ() # Stop multi-processor mode end_time &lt;- Sys.time() # End timer end_time - start_time # Display elapsed time ## Time difference of 14.8071 secs 3.3.17 Plot variable importance plot(boruta_output, cex.axis = 1, las = 2, ylim = c(-5, 50), main = &quot;Variable Importance (Bigger=Better)&quot;) 3.3.18 Variable importance scores roughFixMod &lt;- TentativeRoughFix(boruta_output) imps &lt;- attStats(roughFixMod) imps2 &lt;- imps[imps$decision != &quot;Rejected&quot;, c(&quot;meanImp&quot;, &quot;decision&quot;)] meanImps &lt;- imps2[order(-imps2$meanImp), ] # descending sort knitr::kable(meanImps, full_width = F, position = &quot;left&quot;, caption = &quot;Mean Importance Scores &amp; Decision&quot;) Table 3.2: Mean Importance Scores &amp; Decision meanImp decision R 43.17613 Confirmed H 34.30370 Confirmed P 28.70674 Confirmed C 27.72357 Confirmed K 27.60838 Confirmed E 26.18872 Confirmed Y 22.84975 Confirmed T 21.66359 Confirmed S 21.44119 Confirmed A 20.54316 Confirmed N 20.10100 Confirmed V 18.77068 Confirmed I 18.69155 Confirmed F 18.18632 Confirmed D 17.64435 Confirmed G 16.15207 Confirmed W 15.77085 Confirmed L 15.27614 Confirmed M 14.83421 Confirmed Q 14.12976 Confirmed The ‘Boruta random forest test’ shows that all features are essential. None should be dropped. 3.4 EDA Conclusion It was determined earlier that three amino acids (C, F, I) from the single amino acid percent composition should be transformed by using the square root function. The square root transformation lowered the skewness from greater than 2 in all cases to {-0.102739 \\(\\leq\\) skew after transformation \\(\\leq\\) 0.3478132}. Amino Acid Initial skewness Skew after square root transform C, Cysteine 2.538162 0.347813248 F, Phenolalanine 2.128118 -0.102739748 I, Isoleucine 2.192145 0.293474879 The transformations of the three amino acids (C, F, I) did not appreciably change any critical measures, such as the feature importance derived from the Boruta random forest feature selection work. Nor did the transformations of C, F, I appreciably change the correlation coefficient matrix. Therefore the transformed data will be used throughout this experiment. Boruta, which is used for dimensionality reduction of Transformed data, showed that all dependent features are essential for the generation of a Decision Tree. I believe that this would imply that given that the Random Forest approach will be used, it would wise to keep all features for that model test and throughout the generation of other models. All features have decisive mean importance, which is generated by a Gini calculation. Regarding the coefficients of correlation of the Transformed dataset, there are no examples of coefficients that are greater than or equal to 0.75; therefore, this implies that no features are collinear. https://en.wikipedia.org/wiki/Exploratory_data_analysis↩ https://www.itl.nist.gov/div898/handbook/↩ Peng, Roger, Exploratory Data Analysis with R, https://leanpub.com/exdata, 2016↩ Ronald Pearson, ‘Exploratory Data Analysis Using R,’ P.11, CRC Press, ISBN:9781138480605, 2018↩ Miron Kursa, Witold Rudnicki, Feature Selection with the Boruta Package, DOI:10.18637/jss.v036.i11, 2010↩ Ronald Pearson, ‘Exploratory Data Analysis Using R,’ P.11, CRC Press, 2018↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, P.43↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, P.47 (http://appliedpredictivemodeling.com/)↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018, (http://appliedpredictivemodeling.com/)↩ Max Kuhn and Kjell Johnson, Applied Predictive Modeling, Springer Publishing, 2018↩ "],
["principle-component-analysis-of-myoglobincontrol-protein-sets.html", "4 Principle Component Analysis of Myoglobin/Control Protein Sets 4.1 Introduction 4.2 Data centering / scaling / normalization 4.3 Principle component analysis using norm_c_m_20aa 4.4 Screeplot and Cumulative Proportion of Variance plot 4.5 Biplots 4.6 Obtain Outliers From Biplot #2: PC1 Vs. PC2 4.7 PCA Conclusions", " 4 Principle Component Analysis of Myoglobin/Control Protein Sets 4.1 Introduction This chapter describes the use and functional understanding of Principle Component Analysis (PCA). PCA is very popular and commonly used during the early phases of model development to provide information on variance. In particular, PCA is a transformative process that orders and maximizes variances found within a dataset. The primary goal of principal components analysis is to reveal the hidden structure in a dataset. In so doing, we may be able to; 34 identify how different variables work together to create the dynamics of the system, reduce the dimensionality of the data, decrease redundancy in the data, filter some of the noise in the data, compress the data, prepare the data for further analysis using other techniques. Advantages Of Using PCA Include PCA preserves the global structure among the data points, It is efficiently applied to large data sets, PCA may provide information on the importance of features found in the original datasets. Disadvantages Of PCA Should Be Considered PCA can easily suffer from scale complications, Similarly to the point above, PCA is susceptible to significant outliers. If the number of samples is small or when values have many potential outliers, this can influence scaling and relative point placement, Intuitive understanding can be tricky. 4.2 Data centering / scaling / normalization It is common for the first step when carrying out a PCA is to center, scale, normalize the data. This is important due the fact that PCA is sensitive to the scale of the features. If the features are quite different frome each other, i.e. different by one or more orders of magnitude then scaling is crucial. While determining the variance of your dataset, it should be clear that the order of magnitude of your data features matters significantly. The reasons for this should be clear that if one axis is in 1,000’s while the second axis is between 1 and 10, the larger scale will have a higher variance distorting the results. What do the center and scale arguments do in the prcomp command? There are four common methods for scaling data: Scaling Method Formula Centering \\(f(x) ~=~ \\large x - \\bar x\\) Scaling between [0, 1] \\(f(x) ~=~ \\Large \\frac {x - min(x)} {max(x) - min(x)}\\) Scaling between [a, b] \\(f(x) ~=~ \\large (b - a)* \\Large \\frac {x - min(x)} {max(x) - min(x)} + a\\) Normalizing \\(f(x) ~=~ \\Large \\frac {x - mean(x)} {\\sigma_x}\\) 4.2.1 Histograms of Scaled Vs. Unscaled data Investigating the differences between the amino acid Phenylalanine (F) before and after 2 scaling methods. Investigating the plots above, the main idea to recognize is that the data has not been fundamentally changed, simply ‘shifted and stretched’ or more accurately transformed. It appears that any visible changes of the distributions can be accounted for by differing binnings. Although the differences are between all three histograms are minor, any transformation would be sufficient to use. However, I chose to use the Normalized dataset. 4.2.2 Finding the Covariance Matrix The first step for calculating PCA is to determine the Covariance matrix. Covariance provides a measure of how strongly variables change together.35 36 Covariance of two variables Remember, this simplified formula is to determine covariance for a two-dimensional system. \\[\\begin{equation} cov(x, y) ~=~ \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar x) (y_i - \\bar y) \\end{equation}\\] Where \\(N\\) is the number of observations, \\(\\bar x\\) is the mean of the independent variable, \\(\\bar y\\) is the mean of the dependent variable. Covariance of matrices When dealing with a many feature variables one needs to determine the covariance of matrices, \\(\\large M\\) using linear algebra. 37 Find the column means of the matrix, \\(M_{means}\\). Find the difference matrix, \\(D ~=~ M - M_{means}\\). Finally calculate the covariance matrix: \\[\\begin{equation} cov ~ (M) ~=~ \\left( \\frac{1}{N-1} \\right) ~ D^T \\cdot D, ~~~ where ~~~~~ D = M - M_{means} \\end{equation}\\] Where \\(D^T\\) is the transpose of the difference matrix, \\(N\\) is the number of observations or rows in this case. 4.2.3 Finding PCA via singular value decomposition The procedure below is an outline, not the full computation of PCA. This procedure for PCA relies on the fact that it is similar to the singular value decomposition (SVD) used when determining eigenvectors and eigenvalues. 38 Singular value decomposition says that every n x p matrix can be written as the product of three matrices: \\(A = U \\Sigma V^T\\) where: \\(U\\) is an orthogonal n x n matrix. \\(\\Sigma\\) is a diagonal n x p matrix. In practice, the diagonal elements are ordered so that \\(\\Sigma_{ii} ~\\geqq~ \\Sigma_{jj}\\) for all i &lt; j. \\(V\\) is an orthogonal p x p matrix, and \\(V^T\\) represents a matrix transpose. The SVD represents the essential geometry of a linear transformation. It tells us that every linear transformation is a composition of three fundamental actions. Reading the equation from right to left: The matrix \\(V\\) represents a rotation or reflection of vectors in the p-dimensional domain. The matrix \\(\\Sigma\\) represents a linear dilation or contraction along each of the p coordinate directions. If n \\(\\neq\\) p, this step also canonically embeds (or projects) the p-dimensional domain into (or onto) the n-dimensional range. The matrix \\(U\\) represents a rotation or reflection of vectors in the n-dimensional range. The intuition for understanding PCA is reasonably straightforward. Consider the 2-dimensional data cloud of points or observations in a hypothetical experiment, as seen in the figure on the left. Variances along both the x and y dimensions are calculated. However, given the data shown, there is a rotation of that x-y plane, which will present the data showing its most significant variance. This variance will reside on an axis analogous to points on an Ordinary Least Squares (OLS) line. This axis is called the first principle component followed by the second principal component and so on. Unlike an OLS calculation, PCA will determine not only the first and most significant variance of your data set, but it will, through the rotation and transform your dataset via linear algebra, calculating N variances within your dataset, where N is equal to the number of features in the dataset. The second principal component will be calculated only along a coordinate axis, which is perpendicular (orthogonal or orthonormal) to the first. Each subsequent principal component will then be calculated along axes which are orthogonal to each other. A further benefit of using PCA is that the variances it reports will be ranked in order from highest to lowest. 39 4.2.4 Example of two-dimensional PCA using random data Graphic Range (Green lines) Differences Raw Data (Left) 1 &lt;= x &lt;= 4 3 units Transformed Data (Right) -2.45 &lt;= x &lt;= 2.77 5.22 units If we investigate the figures above we find that the range of the samples is (1 &lt;= x &lt;= 4), while the range for the transformed data is (-2.45 &lt;= x &lt;= 2.76). The differences between the two ranges are 3 and 5.21 units, respectively. The rotation should be no surprise since the PCA is essentially a maximization of variance. Many R-packages will carry out the steps for PCA all behind the ‘scenes’ but giving no greater understanding for beginners. For example, stats::prcomp 40, stats::princomp 41 are most commonly used. However, there are dozens of similar packages. A keyword search for PCA at R-cran 42 provides 78 matches, as of November 2019. 4.3 Principle component analysis using norm_c_m_20aa start_time &lt;- Sys.time() # Start timer c_m_20_PCA &lt;- prcomp(norm_c_m_20aa) end_time &lt;- Sys.time() # End timer end_time - start_time # Display time ## Time difference of 0.008734465 secs 4.4 Screeplot and Cumulative Proportion of Variance plot Two plots are commonly used to determine the number of principal components that a researcher would generally accept as useful. The eigenvalues derived from PCA are proportional to the variances which they represent, and depending on the strategy used to calculate them, the eigenvalues are equal to the variances of the components. The first of the two plots which I which is the scree plot. 43 The scree plot is a ranked list of the eigenvalues plotted against its principal components. An eigenvalue score of one is thought to provide a comparable amount of information as a single variable un-transformed by PCA. The second plot describes the cumulative proportion of variance versus the principal component. This graphic shows how much each principal component represents the entire cumulative variances or total squared error. \\[\\begin{equation} Cumlative ~ Proportion ~of ~Variance ~=~ \\frac{\\sigma_i^2}{\\sum_{i=1}^N \\sigma_i^2} \\end{equation}\\] Here again, there are several criteria regarding how best to use the information from the is plot. The first of which is Cattell’s heuristic. Cattell advises using the principal component that is above the elbow of the curve. The second heuristic is keeping the total number of factors that best explains 80%-95% of the variance. There is no hard-fast rule at this time; a set of researchers only uses the first three factors or none at all.44 A second suggestion is to use the Kaiser rule, which states it is sufficient to use Principal Components, which have an eigenvalue greater than or equal to one. 45 If we investigate the ‘cumulative proportion of variance’ plot, we see an arbitrary line on the Y-axis, which denotes the 90% mark. At this point, the plot suggests that a researcher could use the most significant 12 of the variances from the PCA. 4.5 Biplots 4.5.1 Biplot 1: PC1 Vs. PC2 with ‘Class’ by color labels Black indicates control protein set, Class = 0 Blue indicates myoglobin protein set, Class = 1 The first two principal components describe 46.95% of the variance. 4.5.2 Biplot 2: Determination Of 4 Rule Set For Outliers 4.6 Obtain Outliers From Biplot #2: PC1 Vs. PC2 I have chosen to analyze the PCA biplot of the first and second principal components. The first and second components were used because they describe nearly 50% of the variance (46.95%). 4.6.1 Outliers from Principal Component-1 Rule Set Given PC1: Outlier_1: c_m_20_PCA$x[, 1] &gt; 3 std dev Outlier_2: c_m_20_PCA$x[, 1] &lt; -3 std dev outliers_PC1 &lt;- which((c_m_20_PCA$x[, 1] &gt; 3) | (c_m_20_PCA$x[, 1] &lt; -3)) length(outliers_PC1) ## [1] 285 4.6.2 Outliers from Principal Component-2 Rule Set Given PC2: Outlier_3: c_m_20_PCA$x[, 2] &gt; 3 std dev Outlier_4: c_m_20_PCA$x[, 2] &lt; -3 std dev outliers_PC2 &lt;- which((c_m_20_PCA$x[, 2] &gt; 3) | (c_m_20_PCA$x[, 2] &lt; -3)) length(outliers_PC2) ## [1] 177 4.6.3 List of all outliers (union and sorted) found using the ruleset 1 through 4 The list of total outliers is derived by taking the union of outliers_PC1 and outliers_PC2 and then using sort. total_pca_1_2_outliers &lt;- union(outliers_PC1, outliers_PC2) total_pca_1_2_outliers &lt;- sort(total_pca_1_2_outliers) length(total_pca_1_2_outliers) ## [1] 461 # Write out to Outliers folder write.table(total_pca_1_2_outliers, file = &quot;./00-data/03-ml_results/pca_outliers.csv&quot;, row.names = FALSE, na = &quot;&quot;, col.names = &quot;rowNum&quot;, sep = &quot;,&quot;) It is important to remember and understand that this list of “total_pca_1_2_outliers” includes BOTH negative and positive controls. The groupings are as follows: Group Range of Groups Controls 1, …, 1217 Positive (Myoglobin) 1218, …, 2341 4.7 PCA Conclusions Principal Component Analysis is very popular and an excellent choice to include during Exploratory Data. Analysis. One objective for using PCA is to filter noise from the dataset used and, in turn, increase any signal or to sufficiently delineate observations from each other. In fact, in the figure below, there are five colored groups outside the main body of observations that are marked at ‘outliers.’ The number of outliers obtained from PCA is 461 proteins. The premise of this experiment is to determine if PCA is an excellent representative measure for proteins that are categorized is false-positive, and false-negatives in the five subsequent machine learning model approach. It will be interesting to see if anyone of these groups will be present in the group of false-positives and false-negatives in any of the machine learning models. 4.7.1 Outliers derived from PC1 Vs PC2 The table and the figure below show a subset of outliers produced when the first and second principal component is graphed. My interest lies in finding if any one of the lettered groups (A-E) are part of the false-positives and false-negatives from each of the machine learning models. Each of the five groups is rich is a small number of amino acids. We hope that this information will shine a light on how the different machine models work. It is also expected that this will give help in constructing a model that is more interpretable for the more difficult opaque machine learning models, such as Random Forest, Neural Networks, and possibly Support Vector Machine using the Radial Basis Function. Group Increased concentration of amino acid Example observations A H, L, K 1478 B E, K 1934, 1870, 2100 C V, I, F, Y 182, 1752, 2156 D C, S 1360, 2240 E G, D, Q 664, 2304 Note: For more information on Eigenvalues, Eigenvectors and Eigen decomposition I suggest: CodeEmporium Victor Lavrenko Emily Mankin, Principal Components Analysis: A How-To Manual for R, http://people.tamu.edu/~alawing/materials/ESSM689/pca.pdf↩ http://mathworld.wolfram.com/Covariance.html↩ Trevor Hastie, Robert Tibshirani, Jerome Friedman, ‘The Elements of Statistical Learning; Data Mining, Inference, and Prediction,’ Second Edition, Springer, DOI:10.1007/978-0-387-84858-7, 2009↩ http://mathworld.wolfram.com/Covariance.html↩ https://blogs.sas.com/content/iml/2017/08/28/singular-value-decomposition-svd-sas.html↩ Brian Everitt, Torsten Hothorn, An Introduction to Applied Multivariate Analysis with R, Springer, DOI:10.1007/978-1-4419-9650-3, 2011↩ https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html↩ https://stat.ethz.ch/R-manual/R-devel/library/stats/html/princomp.html↩ https://cran.r-project.org/web/packages/available_packages_by_name.html↩ Raymond Cattell, “The scree test for the number of factors.” Multivariate Behavioral Research. 1 (2): 245–76. DOI:10.1207/s15327906mbr0102_10, 1966↩ Nicole Radzill, Ph.D., personal communication.↩ https://stats.stackexchange.com/questions/253535/the-advantages-and-disadvantages-of-using-kaiser-rule-to-select-the-number-of-pr↩ "],
["logistic-regression-for-binary-classification.html", "5 Logistic Regression For Binary Classification 5.1 Introduction 5.2 Logit Training #1 Using 20 Features 5.3 Logit Results #1 5.4 Logit Training #2 Using 9 Features 5.5 Logit Summary #2 5.6 Logit Confusion Matrix #2 5.7 Obtain List of False Positives &amp; False Negatives 5.8 Logit Conclusion", " 5 Logistic Regression For Binary Classification 5.1 Introduction For individuals who have studied cell biology or biochemistry, logistic regression may be familiar as dose-response curves, enzyme kinetic curves, sigmoidal curves, median lethal dose curve (LD-50) or even an exponential growth curve given limited resources. However, in the context of predictive modeling, Logistic Regression is used as a binary classifier that toggle between the logical values of zero or one. Logistic regression (Logit) derives its name from its similarity to linear regression, as we shall see below. The input/independent variable for Logit is the set of real numbers, (\\(X ~ \\in ~ \\Re\\)). While, the output of a Logistic Regression is not represented by {0, 1}, (\\(Y ~ \\notin ~ \\Re\\)), \\[\\begin{equation} f(x) = ~~ \\left \\{ \\begin{matrix} 0 ~~for~~ x &lt; 0 \\\\ 1 ~~for~~ x \\geq 0 \\end{matrix} \\right. \\end{equation}\\] Using Logistic Regression, we may calculate the presence or absence of a product or quality that we wish to model given a difficult situation where the transition is not clear. In the figure below, the function’s domain, \\(X \\in \\{-\\infty\\) to \\(\\infty\\}\\), whereby its range is {0, 1}. In the figure, the decision boundary is \\(x ~=\\) 0, denoted by the red dotted line. At the inflection point the curves range changes from zero, absence, to one, the presence of quality or item. The logistic growth curve is commonly denoted by: \\[\\begin{equation} f(x) ~=~ \\frac{M}{1 + Ae^{-r(x - x_0)}} \\end{equation}\\] where \\(M\\) is the curve’s maximum value, \\(r\\) is the maximum growth rate (also called the Malthusian parameter46), \\(x_0\\) is the midpoint of the curve, \\(A\\) is the number of times that the initial population must double to reach \\(M\\).47 In the specific case of Logistic Regression for Binary Classification where we have a probability between 0 and 1, \\(M\\), and \\(A\\) take on the value one. \\[\\begin{equation} f(x) ~=~ \\frac{1}{1 + e^{-(WX+b)}} \\end{equation}\\] Since the logistic equation is exponential, it is easier to work with the formula in terms of its odds or log-odds. Odds are the probabilities of success over failure denoted as \\(\\Large \\frac{p}{1-p}\\) or more precisely log-odds as \\(\\Large ln \\left (\\frac{p}{1-p} \\right )\\). Simply by using log-odds, logistic regression may be more easily expressed as a set of linear equations in x.48 Hence we can now go from linear regression to logistic regression. \\[\\begin{equation} ln ~ \\left ( \\frac{Pr(y_i ~=~ 1|x_i)}{Pr(y_i ~=~ 0|x_i)} \\right ) =~ \\beta_0 + \\beta_1 x_1 +~ ... ~+ \\beta_{n} x_{n} \\end{equation}\\] Substitute (\\(p\\) for \\(Pr(y_i ~=~ 1|x_i)\\)) and (\\(1-p\\) for \\(Pr(y_i ~=~ 0|x_i)\\)) and change notation to summation on the right hand side: \\[\\begin{equation} ln \\left( \\frac {p}{1-p} \\right) =~ \\sum_i^{k} \\beta_i x_i \\end{equation}\\] Eliminate the natural log by taking the exponent on both sides: \\[\\begin{equation} \\frac {p}{1-p} =~ exp \\left ( \\sum_i^{k} \\beta_i x_i \\right ) \\end{equation}\\] Substitute \\(u = \\sum_i^{k} \\beta_i x_i\\): \\[\\begin{equation} \\frac {p}{1-p} =~ e^u \\end{equation}\\] Rearrange to solve for \\(\\large p\\): \\[\\begin{equation} p(u) ~=~ \\frac{e^u}{1 + e^u} \\end{equation}\\] Take the derivative of both sides using quotient rule: \\[\\begin{equation} p&#39;(u) ~=~ \\frac {(e^u)(1 + e^u) - (e^u)(e^u)}{(1 + e^u)^2} \\end{equation}\\] Simplify: \\[\\begin{equation} p&#39;(u) ~=~ \\frac {e^u}{(1 + e^u)^2} \\end{equation}\\] Separate out to produce two fractions: \\[\\begin{equation} p&#39;(u) ~=~ \\left ( \\frac {e^u}{1 + e^u} \\right ) \\cdot \\left ( \\frac{1}{1 + e^u} \\right ) \\end{equation}\\] Substitute our previous success and failure variables back into place: \\[\\begin{equation} p&#39;(u) ~=~ p(u) \\cdot ( 1 - p(u)) \\end{equation}\\] Now we can calculate the probabilities as well as the values for any given x value. # Load Libraries Libraries &lt;- c(&quot;doMC&quot;, &quot;knitr&quot;, &quot;readr&quot;, &quot;tidyverse&quot;, &quot;caret&quot;) for (p in Libraries) { library(p, character.only = TRUE) } # Import relevant data c_m_TRANSFORMED &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;, col_types = cols(Class = col_factor(levels = c(&quot;0&quot;, &quot;1&quot;)), PID = col_skip(), TotalAA = col_skip())) # Partition data into training and testing sets set.seed(1000) index &lt;- createDataPartition(c_m_TRANSFORMED$Class, p = 0.8, list = FALSE) training_set.1 &lt;- c_m_TRANSFORMED[index, ] The test.set.1 and Class.test data sets are not produced since the Logit run with 20 features was not deemed useful. The reason for its dismissal was that is contained extraneous features. 5.2 Logit Training #1 Using 20 Features The first training run is to determine if all 20 features (amino acids) are necessary for our logistic regression model. set.seed(1000) registerDoMC(cores = 3) # Start multi-processor mode start_time &lt;- Sys.time() # Start timer # Create model, 10X fold CV repeated 5X tcontrol &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5) model_obj.1 &lt;- train(Class ~ ., data = training_set.1, trControl = tcontrol, method = &quot;glm&quot;, family = &quot;binomial&quot;) end_time &lt;- Sys.time() # End timer end_time - start_time # Display time ## Time difference of 1.7534 secs registerDoSEQ() # Stop multi-processor mode 5.3 Logit Results #1 summary(model_obj.1) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.9372 -0.2835 -0.0194 0.0516 3.6884 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.0525 9.2156 0.874 0.382234 ## A 5.0438 9.6899 0.521 0.602699 ## C -14.2228 2.6949 -5.278 1.31e-07 *** ## D -36.2676 8.0845 -4.486 7.25e-06 *** ## E 27.6016 11.1292 2.480 0.013135 * ## F 5.6174 5.2654 1.067 0.286034 ## G -22.1970 10.3043 -2.154 0.031229 * ## H 90.1101 12.1105 7.441 1.00e-13 *** ## I -5.9795 4.3945 -1.361 0.173610 ## K -2.8961 9.8468 -0.294 0.768669 ## L -3.7417 9.2217 -0.406 0.684926 ## M -0.1427 12.0747 -0.012 0.990570 ## N 3.3478 9.6749 0.346 0.729319 ## P -39.7466 11.1010 -3.580 0.000343 *** ## Q -5.6804 11.2516 -0.505 0.613664 ## R -83.6045 11.8104 -7.079 1.45e-12 *** ## S -9.9745 10.0872 -0.989 0.322750 ## T -36.5980 9.2791 -3.944 8.01e-05 *** ## V 16.3411 9.7859 1.670 0.094946 . ## W 9.0169 13.8870 0.649 0.516141 ## Y -31.9282 11.1167 -2.872 0.004078 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2593.68 on 1872 degrees of freedom ## Residual deviance: 657.72 on 1852 degrees of freedom ## AIC: 699.72 ## ## Number of Fisher Scoring iterations: 8 The Akaike information criterion (AIC)49 for model #1 is 699.72. This will be used later to compare the models generated to rate their ability to utilize the features best. - The list of probabilities for the estimates leaves us with only 9 important features to try re-modeling, R, H, P, C, E, Y, T, D, G. 5.4 Logit Training #2 Using 9 Features This test uses ONLY 9 features: (R, H, P, C, E, Y, T, D, G) # Data import &amp; handling c_m_9aa &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;, col_types = cols(Class = col_factor(levels = c(&quot;0&quot;, &quot;1&quot;)), A = col_skip(), F = col_skip(), I = col_skip(), K = col_skip(), L = col_skip(), M = col_skip(), N = col_skip(), PID = col_skip(), Q = col_skip(), V = col_skip(), S = col_skip(), TotalAA = col_skip(), W = col_skip())) # Partition data into training and testing sets set.seed(1000) index &lt;- createDataPartition(c_m_9aa$Class, p = 0.8, list = FALSE) training_set.2 &lt;- c_m_9aa[ index, ] test_set.2 &lt;- c_m_9aa[-index, ] Class_test.2 &lt;- as.factor(test_set.2$Class) 5.4.1 Logit Training #2 Using 9 Features set.seed(1000) registerDoMC(cores = 3) # Start multi-core start_time &lt;- Sys.time() # Start timer # Create model, 10X fold CV repeated 5X fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, savePredictions = &quot;final&quot;) # IMPORTANT: Saves predictions model_obj.2 &lt;- train(Class ~ ., data = training_set.2, trControl = fitControl, method = &quot;glm&quot;, family = &quot;binomial&quot;) end_time &lt;- Sys.time() # End timer end_time - start_time # Display time ## Time difference of 1.524966 secs registerDoSEQ() # Stop multi-core 5.5 Logit Summary #2 summary(model_obj.2) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.2083 -0.2984 -0.0204 0.0601 3.5666 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.306 1.007 8.245 &lt; 2e-16 *** ## C -14.755 1.908 -7.733 1.05e-14 *** ## D -31.411 4.949 -6.347 2.20e-10 *** ## E 21.932 5.092 4.307 1.66e-05 *** ## G -23.259 5.071 -4.587 4.49e-06 *** ## H 94.580 8.431 11.218 &lt; 2e-16 *** ## P -29.394 6.264 -4.692 2.70e-06 *** ## R -82.809 6.363 -13.015 &lt; 2e-16 *** ## T -40.915 5.624 -7.275 3.45e-13 *** ## Y -37.860 6.291 -6.018 1.77e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2593.68 on 1872 degrees of freedom ## Residual deviance: 688.96 on 1863 degrees of freedom ## AIC: 708.96 ## ## Number of Fisher Scoring iterations: 8 5.6 Logit Confusion Matrix #2 Predicted_test_vals &lt;- predict(model_obj.2, test_set.2[, -1]) confusionMatrix(Predicted_test_vals, Class_test.2, positive = &quot;1&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 235 18 ## 1 8 206 ## ## Accuracy : 0.9443 ## 95% CI : (0.9195, 0.9633) ## No Information Rate : 0.5203 ## P-Value [Acc &gt; NIR] : &lt; 2e-16 ## ## Kappa : 0.8883 ## ## Mcnemar&#39;s Test P-Value : 0.07756 ## ## Sensitivity : 0.9196 ## Specificity : 0.9671 ## Pos Pred Value : 0.9626 ## Neg Pred Value : 0.9289 ## Prevalence : 0.4797 ## Detection Rate : 0.4411 ## Detection Prevalence : 0.4582 ## Balanced Accuracy : 0.9434 ## ## &#39;Positive&#39; Class : 1 ## The Akaike information criterion (AIC) for model #2 is 708.96. This will be used later to compare the models generated to rate their ability to utilize the features best. The number of unique false-positives and false-negatives is 26. 5.7 Obtain List of False Positives &amp; False Negatives fp_fn_logit &lt;- model_obj.2 %&gt;% pluck(&quot;pred&quot;) %&gt;% dplyr::filter(obs != pred) # Write CSV in R write.table(fp_fn_logit, file = &quot;./00-data/03-ml_results/fp_fn_logit.csv&quot;, row.names = FALSE, na = &quot;&quot;, col.names = TRUE, sep = &quot;,&quot;) nrow(fp_fn_logit) ## NOTE: NOT UNIQUE NOR SORTED ## [1] 536 The logistic regression second test produced 536 protein samples, which are either false-positives or false-negatives. The list of 536 proteins may have duplicates. Therefore they are NOT UNIQUE NOR SORTED. 5.8 Logit Conclusion Logit is easy to implement and understand and can be used for feature selection. Considering the table Logit Models, below, it is clear that model #2 with nine features best describes the better of the two models. Akaike Information Criterion 50 \\[\\begin{equation} AIC ~=~ 2 K ~-~ ln (\\widehat{L}) \\end{equation}\\] Where \\(ln (\\widehat{L})\\) is the log-likelihood, \\(K\\) is the number of parameters. Two Logit Models Model # Features AIC 1 20 699.72 2 9 708.96 Logit is a common machine learning method. It is easy to understand and explain. This supervised binary classification method is very useful for determining the importance of the features which can be applied. As we saw in Model#1, there were 11 features that had probabilities of the estimates used above the 5% threshold cut-off. In Model#2, only nine features were used to describe the model, and the AIC increased by 9.24. The nine features which best described the logistic regression model were R, H, P, C, E, Y, T, D, G. If we compare this to the Boruta test carried out in the EDA, we find the overlap interesting. Comparison of Boruta Vs Logit: Order of Importance Test Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Boruta R H P K C E Y T S A V U I F D G N L M Q Logit R H P . C E Y T . . . . . . D G . . . . The first 7 out of 8 amino acid features are seen in the proper order, as described by the Boruta Random Forest model. This is confirmation that Logit can pick up the importance of features similar to Boruta. Logit produced 536 proteins, which are false-negatives or false-positives. It should be noted that the 536 are NOT UNIQUE NOR SORTED. The number of unique FN/FP from the confusion matrix is 26. These proteins will be investigated further in the Outliers chapter, which compares these FN/FP proteins to the PCA outliers. https://en.wikipedia.org/wiki/Malthusian_growth_model↩ https://en.wikipedia.org/wiki/Logistic_function↩ http://juangabrielgomila.com/en/logistic-regression-derivation/↩ https://en.wikipedia.org/wiki/Akaike_information_criterion↩ https://en.wikipedia.org/wiki/Akaike_information_criterion↩ "],
["neural-networks-for-binary-classification.html", "6 Neural Networks For Binary Classification 6.1 Introduction 6.2 The One Neuron System 6.3 The Two Neuron System 6.4 Neural Network Experiment For Binary Classification 6.5 Neural Network Conclusion", " 6 Neural Networks For Binary Classification “Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions” – Ian Goodfellow, et al51 6.1 Introduction If we discuss Neural Networks (NN), we should first consider the system we hope to emulate. Let us start with a simple count of neuronal cells in various organisms along the earth’s phylogenetic tree. We might get a better idea of the type of “computing power” these living creatures possess. See table 5.1. Table 5.1: Organisms Vs Number of Neurons In Each (Wikipedia) Organism Common Name Approximate Number of Neurons C. elegans roundworm 302 Chrysaora fuscescens jellyfish 5,600 Apis linnaeus honey bee 960,000 Mus musculus mouse 71,000,000 Felis silvestris cat 760,000,000 Canis lupus familiaris dog 2,300,000,000 Homo sapien sapien humans 100,000,000,000 This table portrays a high-level overview of the computing power of neuronal clusters and brains produced throughout evolution. However, there is one missing number worth noting. The table above does not describe the connectivity between neurons. The connectivity of neurons varies greatly from lower to higher organisms. For example, some simple animals have only “four to eight separate branches,” 52 per nerve cell. While human neurons may have approximately \\(10^4\\) inter-connected synaptic junctions per neuron, thus resulting in a total of approximately 600 x \\(10^{12}\\) synapses per human brain. 53 Although neurons have differing morphologies, neurons in the human brain are extremely diverse. Indeed, size and shape may not be the definitive way of classifying neurons but instead by what neurotransmitters the cells secrete. “Neurotransmitters can be classified as either excitatory or inhibitory.”54 Currently the NeuroPep database “holds 5949 non-redundant neuropeptide entries originating from 493 organisms belonging to 65 neuropeptide families.”55 Figure 6.1: Basic Neuron Types and S.E.M. Image 56 Figure 6.2: Two Neuron System (Image From The Public Domain) Given an order of operation via: Dendrite(s) \\(\\Longrightarrow\\) Cell body \\(\\Longrightarrow\\) Fibrous Axon \\(\\Longrightarrow\\) Synaptic Junction or Synaptic Gap \\(\\Longrightarrow\\) Dendrite(s) … Ad infinitum. However, nature is more subtle and intricate than to have neurons in a series, only blinking on and off, firing or not. NN are often programmed to classify dangerous road objects, as is the case of Tesla cars. The goal of a Tesla auto-piloted car is to use all available sensors to correctly classify all the conceivable circumstances on the road. On the road, a Tesla automobile uses dozens of senors which the computer needs to evaluate and weigh the values of all these sensors to formulate a ‘decision.’ The altitude of the auto, derived from the GPS, may weigh less heavily than the speed of the vehicle or Lidar estimates on how close objects are. However, our goal of safe driving can be thwarted when an artificial intelligence system decides a truck is a sign and does not apply the brakes.57 Figure 6.3: Goal of a Tesla Neural Networks is to generate the correct repsonses for its environment. 6.2 The One Neuron System If we investigate a one neuron system, our neuron could be diagrammed in four sections.58 Figure 6.4: One Neuron Schema If we investigate one neuron for a moment, we find two separate mathematical functions are being carried out by a single nerve cell. 6.2.1 Summation Function The first segment is a summation function. It receives the real number values from, \\(x_1\\) to \\(x_n\\), all the branches of the dendritic trees, and multiplies them by a set of weights. These \\(X\\) inputs are multiplied by a set of corresponding unique weights from \\(w_1\\) to \\(w_n\\). An analogy I prefer is of small or large rivers joining giving a total current. The current moves through the branches giving a total signal or current of sodium ions. Interestingly the summation in each neuron, while dealing with the vectors of inputs and weights, is carrying out the dot product of these vectors, such that; Initially, the NN used the Heaviside-Threshold Function, as shown in figure 4, the ‘One Neuron System.’ The benefits of step functions were their simplicity and high signal to noise ratio. While the detriments were, it is a discontinuous function, therefore not able to be differentiated and a mathematical problem. Let us take into account the product, \\(x_0 \\cdot w_0\\). If we assign \\(x_0 = T\\) and \\(w_0 = -1\\) this simply becomes a bias. This bias allows us the ability to shift our Activation Function and its inflection point in the positive or negative x-direction. \\[\\begin{equation} \\large \\hat Y ~=~ X^T \\cdot W - Bias ~~\\equiv~~ \\sum_{i=0}^n x_i w_i - T \\end{equation}\\] 6.2.2 Activation Functions The second function is called an Activation Function. Once the Summation Function yields a value, its result is sent to the Activation Function or Threshold Function. \\[\\begin{equation} \\large {Z}^{(1)} = f \\left( \\sum_{i=0}^n x_i w_i - T\\right) = \\{0, 1\\} \\end{equation}\\] The function displayed in figure #4, One Neuron Schema, is a step function. However this step function has a problem mathematically, namely it is a discontinuous and therefore not differentiable. This fact is important. Therefore several functions may be used in place of the step function. One is the hyperbolic tangent (tanh) function, the sigmoidal function, a Hard Tanh, a reLU, and Softmax Functions. These have certain advantages, namely they simplify the hyperbolic tangent function. Not only does the Hard Tanh and reLU simplify calculations it is useful for increasing the gain near the asymptotic limits of the sigmoidal and tanh functions. The derivatives of the sigmoidal and tanh functions are very small, near 0 and 1, while the reLU and Hard Tanh slopes are one or zero. \\[\\begin{equation} \\large Z^{(2)} ~=~ tanh(x) = \\frac{1 - e^{-{\\alpha}}}{1 + e^{-{\\alpha}}} ~~~:~~~ \\large where ~~~ \\large \\alpha = \\sum_{i=1}^n x_i w_i - T \\end{equation}\\] \\[\\begin{equation} \\large Z^{(3)} ~=~ sigmoid(x) ~=~ \\frac{1}{1 + e^{-{\\alpha}}} \\end{equation}\\] \\[\\begin{equation} \\large Z^{(4)} ~=~ Hard ~ Tanh (x) ~=~ \\large \\left\\{ \\begin{array}{rcl} 1 &amp; x &gt; 1 \\\\ x &amp; -1 \\leq x \\leq 1 \\\\ -1 &amp; x &lt; -1 \\end{array}\\right. \\end{equation}\\] Several alternative functions are useful for various reasons. The most common of which are Softmax and reLU functions. Rectified Linear Activation Unit, (ReLU): \\[\\begin{equation} \\large Z^{(5)} ~=~ \\large ReLU ~= \\begin{cases} x \\geqq 0 ~~~~y = x\\\\ x &lt; 0 ~~~~y = 0 \\end{cases} \\end{equation}\\] 6.2.3 Binary Output Or Probability In the case of real neurons, the output is off or on, zero or one. However, in the case of our electronic model, it is advantageous to calculate a probability for greater interpretability. The Softmax function may appear like the Sigmoid function from above but it differs in major ways.59 The softmax activation function returns the probability distribution over mutually exclusive output classes. The calculated probabilities will be in the range of 0 to 1. The sum of all the probabilities is equals to 1. Typically the Softmax Function is used in binary or multiple classification logistic regression models and in building the final output layer of NN. \\[\\begin{equation} \\large Z^{(6)} ~=~ Softmax(x) = \\frac {e^{\\alpha_i}}{\\sum_{i=1}^n e^{\\alpha_i}} \\end{equation}\\] The benefit of these activation functions is that they are now differentiable. This fact becomes important for Back-Propagation, which is discussed later. 6.3 The Two Neuron System Building up in complexity, let us could consider our first Neural Network by using only two neurons. In two neuron systems, let us first generalize a bit more by adding that \\(X\\) is an array of all the inputs as is \\(W_1\\) and \\(W_2\\) is also an array of weights for each neuron. See figure #5. Figure 6.5: A Two Neuron System 6.3.1 Feed-Forward In A Two Neuron Network In our two neuron network, we can now write out the mathematics for each step as it progresses in a “forward” (left to right) direction. Step #1: To move from \\(X\\) to \\(P_1\\) \\[\\begin{equation} f^1( \\overrightarrow{x}, \\overrightarrow{w}) \\equiv~~ P_1 = \\left( X^T \\cdot W_1 - T \\right) \\end{equation}\\] Step #2: \\(P_1\\) feeds forward to \\(Y\\) \\[\\begin{equation} f^2(P_1) ~~\\equiv~ \\hat Y = \\left( \\frac{1}{1 + e^{- \\alpha}} \\right) ~~:~~ where ~~~ \\alpha = P_1 \\end{equation}\\] Step #3: \\(Y\\) feeds forward to \\(P_2\\) \\[\\begin{equation} f^3(\\overrightarrow{y}, \\overrightarrow{w}) ~~\\equiv~ P_2 = \\left( Y^T \\cdot W_2 - T \\right) \\end{equation}\\] Step #4: \\(P_2\\) feeds forward to \\(Z\\) \\[\\begin{equation} f^4(P_2) ~~\\equiv~ \\hat Z = \\left( \\frac{1}{1 + e^{- \\large \\alpha}} \\right) ~~~:~~~ where ~~ \\alpha = P_2 \\end{equation}\\] Step #5: Our complicated function is simply a matter of chaining one result so that it may be used in the next step. \\[\\begin{equation} \\hat Z ~=~ f^4 \\left( f^3 \\left( f^2 \\left( f^1 \\left( X, W \\right) \\right) \\right) \\right) \\end{equation}\\] In our Feed-Forward Propagation, we can now take the values from any numerical system and produce zeros, ones, or probabilities. Remember, in this set of experiments, we are using the concentrations of the 20 amino acids to provide a categorical or binary output, belongs to Myoglobin protein family, or does not. 6.3.2 Error Back-propagation Now that we have learned to calculate the output of our neurons using the Feed-Forward process, what if our final answer is incorrect? Can we build a feed back system to determine the weights needed to obtain our desired value of \\(\\hat z\\)? The answer is yes. The process for determining the weights is known as Back-Propagation. Back-Propagation, also known as error back-propagation, is crucial to understanding and tuning a neural network. Simply stated Back-Propagation is an optimization routine which iteratively calculates the errors that occur at each stage of a neural network. Back-Propagation uses the partial derivatives of the feed forward functions, specifically. The chain rule and gradient descent are also used to determine the weights (\\(W_1 ~~and~~ W_2\\)) which are propagated through the network to find weights used in the summation step of a neuron.60 This thumbnail sketch gives the building blocks to calculate \\(W\\) which can be run until we reach a value that we desire. However the first time the back-propagation is carried out all the weights are chosen randomly. If the weights were set to the same number there would be no change throughout the system. In the two neuron system, our first step is to generate an error or performance (Perf) function to minimize. If we call \\(d\\) our desired value, we can minimize the square error, a common choice.61 Step #1: Performance (Perf) \\[\\begin{equation} \\mathbf{Perf} ~~=~~ c \\cdot (d - \\hat z)^2 \\end{equation}\\] Step #2: \\[\\begin{equation} \\frac{d Z}{d x} ~~=~~ \\frac{d \\left \\{ f^4 \\left( f^3 \\left( f^2 \\left( f^1 \\left( X, W \\right) \\right) \\right) \\right) \\right \\}}{dx} \\end{equation}\\] Figure 6.6: A Two Neuron System Using the chain-rule, working backward and the ‘Two Neuron System’ figure as a guide through the error back-propagation, we find: Step #3: Neuron 2 \\(\\Rightarrow\\) 1 \\[\\begin{equation} \\frac{\\delta Perf}{\\delta w_1} ~=~ \\frac{\\delta Perf}{\\delta z} \\cdot \\frac{\\delta z}{\\delta P_2} \\cdot \\frac{\\delta P_2}{\\delta y} \\cdot \\frac{\\delta y}{\\delta P_1} \\cdot \\frac{\\delta P_2}{\\delta w_1} \\end{equation}\\] Step #4: Performance \\[\\begin{equation} \\frac{\\delta Perf}{\\delta z} ~~=~~ \\frac{\\delta \\left\\{ \\frac{1}{2} \\| \\overrightarrow{d} - \\overrightarrow{z} \\|^2 \\right\\}} {\\delta z} ~~=~~ \\mathbf{\\overrightarrow{d} - \\overrightarrow{z}} \\end{equation}\\] Step #5: Substitute \\(P_2=\\alpha\\) \\[\\begin{equation} \\frac{\\delta z}{\\delta P_2} ~~=~~ \\frac{\\delta~ ((1 + e^{-\\alpha})^{-1})}{\\delta \\alpha} ~~=~~ e^{-\\alpha} \\cdot (1 + e^{-\\alpha})^{-2} \\end{equation}\\] Step #6: Rearrange the expression \\[\\begin{equation} \\frac{ e^{-\\alpha} }{ (1 + e^{-\\alpha})^{-2} } ~~=~~ \\frac{e^{-\\alpha}}{1 + e^{-\\alpha}} \\cdot \\frac{1}{1 + e^{-\\alpha}} \\end{equation}\\] Step #7: Add 1 and subtract 1 \\[\\begin{equation} = ~~ \\frac{ (1+ e^{-\\alpha}) -1 }{1 + e^{-\\alpha}} \\cdot \\frac{1}{1 + e^{-\\alpha}} \\end{equation}\\] Step #8: Rearrange to find \\[\\begin{equation} = ~~ \\left( \\frac{ 1+ e^{-\\alpha} }{1 + e^{-\\alpha}} ~-~ \\frac{ 1 }{1 + e^{-\\alpha}} \\right) \\left( \\frac{1}{1 + e^{-\\alpha}} \\right) ~~=~~ \\left(1- \\frac{1}{1 + e^{-\\alpha}} \\right) \\left( \\frac{1}{1 + e^{-\\alpha}} \\right) \\end{equation}\\] Step #9: Therefore we find \\[\\begin{equation} \\frac{\\delta z}{\\delta \\alpha} ~~=~~ \\frac{\\delta~ ((1 + e^{-\\alpha})^{-1})}{\\delta \\alpha} ~~=~~ \\left(1- \\frac{1}{1 + e^{-\\alpha}} \\right) \\left( \\frac{1}{1 + e^{-\\alpha}} \\right) \\end{equation}\\] Nevertheless, we need one more part to ascertain the weights. As the error back-propagation is computed this process does not reveal how much the weights need to be adjusted/changed to compute the next round of weights given their current errors. For this we require one last equation or concept. Once we compute the weights from our chain rule set of equations we must change the values in the direction proportional to the change in error. This is performed by using gradient descent. Step #10: Learning Rate \\[\\begin{equation} \\Delta W ~:~ W_{i+1} ~=~ W_i ~-~ \\eta \\cdot \\frac{\\delta Perf}{\\delta W} \\end{equation}\\] where \\(\\eta\\) is the learning rate for the system. The key to the learning rate is that it must be sought and its range mapped for optimum efficiency. However smaller rates have the advantage of not overshooting the desired minimum/maximum. If the learning rate is too large the values of \\(W\\) may jump wildly and not settle into a max/min. There is a fine balance that must be considered such that the weights are not trapped in a local minimum and wildly oscillate unable to converge. The last step of error back-propagation is simply setting up the derivatives mechanically and is not shown for brevity. 6.4 Neural Network Experiment For Binary Classification # Load Libraries Libraries &lt;- c(&quot;dplyr&quot;, &quot;knitr&quot;, &quot;readr&quot;, &quot;caret&quot;, &quot;MASS&quot;, &quot;nnet&quot;, &quot;purrr&quot;, &quot;doMC&quot;) for (p in Libraries) { library(p, character.only = TRUE) } education &lt;- read_csv(&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;, col_types = cols(Class = col_factor(levels = c(&quot;0&quot;,&quot;1&quot;)), PID = col_skip(), TotalAA = col_skip())) 6.4.0.1 Create Training Data set.seed(1000) # Stratified sampling TrainingDataIndex &lt;- createDataPartition(education$Class, p = 0.8, list = FALSE) # Create Training Data trainingData &lt;- education[ TrainingDataIndex, ] testData &lt;- education[-TrainingDataIndex, ] TrainingParameters &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5, savePredictions = &quot;final&quot;) # IMPORTANT: Saves predictions 6.4.1 Train model with neural networks end_time - start_time # Display time ## Time difference of 21.52432 secs 6.4.1.1 Confusion Matrix and Statistics NNPredictions &lt;- predict(NNModel, testData) # Create confusion matrix cmNN &lt;-confusionMatrix(NNPredictions, testData$Class) print(cmNN) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 237 12 ## 1 6 212 ## ## Accuracy : 0.9615 ## 95% CI : (0.9398, 0.977) ## No Information Rate : 0.5203 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.9227 ## ## Mcnemar&#39;s Test P-Value : 0.2386 ## ## Sensitivity : 0.9753 ## Specificity : 0.9464 ## Pos Pred Value : 0.9518 ## Neg Pred Value : 0.9725 ## Prevalence : 0.5203 ## Detection Rate : 0.5075 ## Detection Prevalence : 0.5332 ## Balanced Accuracy : 0.9609 ## ## &#39;Positive&#39; Class : 0 ## NNModel ## Neural Network ## ## 1873 samples ## 20 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## Pre-processing: scaled (20), centered (20) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 1498, 1498, 1499, 1499, 1498, 1499, ... ## Resampling results across tuning parameters: ## ## size decay Accuracy Kappa ## 1 0e+00 0.9339109 0.8671305 ## 1 1e-04 0.9346544 0.8686793 ## 1 1e-01 0.9481084 0.8958797 ## 3 0e+00 0.9565473 0.9128846 ## 3 1e-04 0.9564355 0.9126691 ## 3 1e-01 0.9607087 0.9212480 ## 5 0e+00 0.9571825 0.9142100 ## 5 1e-04 0.9570772 0.9139950 ## 5 1e-01 0.9625266 0.9249177 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were size = 5 and decay = 0.1. 6.4.1.2 Obtain List of False Positives &amp; False Negatives fp_fn_NNModel &lt;- NNModel %&gt;% pluck(&quot;pred&quot;) %&gt;% dplyr::filter(obs != pred) # Write/save .csv write.table(fp_fn_NNModel, file = &quot;./00-data/03-ml_results/fp_fn_NN.csv&quot;, row.names = FALSE, na = &quot;&quot;, col.names = TRUE, sep = &quot;,&quot;) nrow(fp_fn_NNModel) ## NOTE: NOT UNIQUE NOR SORTED ## [1] 351 6.4.1.3 False Positive &amp; False Negative Neural Network set keep &lt;- &quot;rowIndex&quot; fp_fn_NN &lt;- read_csv(&quot;./00-data/03-ml_results/fp_fn_NN.csv&quot;) NN_fp_fn_nums &lt;- sort(unique(unlist(fp_fn_NN[, keep], use.names = FALSE))) length(NN_fp_fn_nums) ## [1] 140 NN_fp_fn_nums ## [1] 1 2 4 6 10 15 46 57 58 88 100 114 115 116 130 136 141 146 150 170 ## [21] 172 182 183 185 191 201 231 239 249 254 260 349 407 421 427 430 436 439 449 452 ## [41] 453 503 516 518 526 530 531 532 534 536 537 542 546 547 551 554 562 566 570 573 ## [61] 575 576 577 580 583 584 589 592 655 910 913 980 1032 1033 1034 1035 1041 1067 1069 1092 ## [81] 1093 1094 1096 1100 1101 1106 1115 1121 1130 1144 1190 1219 1222 1223 1226 1233 1234 1245 1264 1279 ## [101] 1282 1340 1471 1482 1484 1487 1510 1522 1569 1571 1575 1576 1577 1579 1582 1585 1587 1588 1589 1594 ## [121] 1600 1608 1618 1619 1621 1622 1623 1693 1723 1734 1771 1780 1789 1828 1829 1830 1831 1832 1833 1873 write_csv(x = as.data.frame(NN_fp_fn_nums), path = &quot;./00-data/04-sort_unique_outliers/NN_nums.csv&quot;) The Neural Network set included a total of 140 unique observations containing both FP and FN. 6.5 Neural Network Conclusion Lorem Ian Goodfellow, Yoshua Bengio, Aaron Courville, ‘Deep Learning’, MIT Press, 2016, http://www.deeplearningbook.org↩ https://www.wormatlas.org/hermaphrodite/nervous/Neuroframeset.html↩ Shepherd, G. M. (2004), The synaptic organization of the brain (5th ed.), Oxford University Press, New York.↩ https://www.kenhub.com/en/library/anatomy/neurotransmitters↩ http://isyslab.info/NeuroPep/home.jsp↩ https://www.howstuffworks.com/↩ https://arstechnica.com/cars/2019/05/feds-autopilot-was-active-during-deadly-march-tesla-crash/↩ Tom Mitchell, Machine Learning, McGraw-Hill, 1997, ISBN: 0070428077↩ Josh Patterson, Adam Gibson, Deep Learning; A Practitioner’s Approach, 2017, O’Rreilly↩ David Rumelhart, Geoffrey Hinton, &amp; Ronald Williams, Learning represetnations by back-propagating Errors, Nature, 323, 533-536, Oct. 9, 1986↩ Ivan N. da Silva, Danilo H. Spatti, Rogerio A. Flauzino, Luisa H. B. Liboni, Silas F. dos Reis Alves, Artificial Neural Networks: A Practical Course, DOI 10.1007/978-3-319-43162-8, 2017↩ "],
["appendices.html", "7 Appendices 7.1 Install R &amp; RStudio 7.2 Load Libraries Used In This Project 7.3 Calculate the amino acid compositions (AAC) and Di-peptide compositions (DPC) 7.4 Calculate AAC and DPC values function 7.5 Run Myoglobin 7.6 Run Control / Human-NOT-myoglobin 7.7 Run Controls 7.8 KEEP AAC ONLY FOR RAW DATA 7.9 Transform {C, F, I} from c_m_RAW_AAC 7.10 Where To Find Help 7.11 Machine Setting &amp; Session Info", " 7 Appendices Word To The Wise LEARN GIT, get a Github account! Put Every computer program you write and Eevery stick of knowledge related to your work in a clean format on Git. You will benefit from it in the long run… To Biologists and Biochemists (which I also consider myself), it is your task to become more familiar with computer languages, miscellaneous computer concepts and statistics. As of the writing of this booklet, Data Science and Bioinformatics are now centered around the computer languages R and Python. To many researchers in data science and bioinformatics the field now includes such languages as, in no particular order, R Python Bash shell scripting SQL Julia Perl C Javascript RMarkdown, (this one is fun and easy) Just start with one language Stick with it for a time and learn it. Learn the ins-and-outs of one language first. Is R or Python better? R &amp; Python are both FREE, Both have great integrated development environments (IDEs), RStudio is great &amp; FREE, Spyder is also great &amp; FREE, PyCharm Sublime Visual Studio Code Vim Jupyter Notebook Both languages have been around for &gt; 20 years, therefore both have tons of FREE information &amp; tutorials on YouTube, 7.1 Install R &amp; RStudio NOTE: I use Ubuntu. Go to: https://cran.r-project.org/ Choose R for your operating system. If you are using Linux, I recommend that you download/install 4 R files. r-base-core_#.#.#.*.deb (approx. 30 MB) r-base-dev_#.#.#.*.deb (approx. 45 KB) r-base_#.#.#*.deb (approx. 90 KB) r-base-html-#.#.#*.deb (approx. 90 KB) If you do have Linux you may try this video: How to install R. Go to: https://www.rstudio.com/ From RStudio’s homepage click, Products then click RStudio from the dropdown menu. Click the Download of the FREE version of RStudio Desktop Click RStudio #.# to download a version for your machine Have Linux? Try this - How to install RStudio. If you are looking for instructions for Mac &amp; Windows machines try: FreeCodeCamp 7.2 Load Libraries Used In This Project If you are using Ubuntu/Linux you may need these Linux libraries first. sudo apt-get install libcurl4-openssl-dev sudo apt-get install libssl-dev sudo apt-get install libxml2-dev To install car &amp; rgl sudo apt-get install xorg sudo apt-get install libx11-dev sudo apt-get install libglu1-mesa-dev sudo apt-get install libfreetype6-dev install.packages(&quot;rlang&quot;) library(rlang) load_or_install &lt;- function(package_names) { for(package_name in package_names) { if(!is_installed(package_name)) { install.packages(package_name, repos = &quot;http://lib.stat.cmu.edu/R/CRAN&quot;, dependencies = TRUE) } library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE) print(&quot;OK&quot;) } } load_or_install(c(&quot;readr&quot;, &quot;doMC&quot;, &quot;corrplot&quot;, &quot;knitr&quot;, &quot;caret&quot;, &quot;tidyverse&quot;)) load_or_install(c(&quot;ggplot2&quot;, &quot;rmarkdown&quot;, &quot;bookdown&quot;, &quot;blogdown&quot;, &quot;kernlab&quot;)) load_or_install(c(&quot;e1071&quot;, &quot;plyr&quot;, &quot;RColorBrewer&quot;, &quot;neuralnet&quot;)) load_or_install(c(&quot;rpart&quot;, &quot;MASS&quot;, &quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;seqinr&quot;)) load_or_install(c(&quot;LogicReg&quot;, &quot;randomForest&quot;, &quot;foreach&quot;)) load_or_install(c(&quot;import&quot;, &quot;dplyr&quot;, &quot;stringr&quot;, &quot;stringi&quot;)) 7.3 Calculate the amino acid compositions (AAC) and Di-peptide compositions (DPC) from .fasta formats, {Myoglobin, Non-Myoglobin} Calculating the Amino Acid and Di-peptide composition of a protein string is a simple calculation requiring the total amino acid length of the peptide or poly-peptide of interest and a count of substrings. Initially, the command seqinr::read.fasta reads .fasta file formats and returns a list of proteins stripping away all other information. Secondly, the command stringr::str_count() produces an integer value of the number of substrings in a larger string, i.e. peptide. For example, aa_nums[j] = str_count(peptide, col_titles[j]) / total_aa, Where; aa_nums[j] is an array to saving values for later writing to file, peptide is the string to check, i.e. protein of interest, col_titles[j] is the substring which is either a single amino acid or di-peptide. Input: .fasta Output: .csv Libraries Libraries = c(&quot;stringr&quot;, &quot;knitr&quot;, &quot;seqinr&quot;) for (p in Libraries) { # Install Libraries library(p, character.only = TRUE) } opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, align = &quot;center&quot;) Import uniprot-myoglobin.fasta - Read peptide lines read_fasta &lt;- function(file) { listo_proteins &lt;- read.fasta(file = file, seqtype = &quot;AA&quot;, as.string = TRUE, seqonly = FALSE, strip.desc = TRUE) return(listo_proteins) } file = &quot;./00-data/ORIGINAL_DATA/uniprot-myoglobin.fasta&quot; myoglobins &lt;- read_fasta(file) Column_titles column_titles = function() { peptides = c(&quot;A&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;, &quot;N&quot;, &quot;P&quot;, &quot;Q&quot;, &quot;R&quot;, &quot;S&quot;, &quot;T&quot;, &quot;V&quot;, &quot;W&quot;, &quot;Y&quot;) # Add DIPEPTIDES column titles di_titles = vector(mode = &quot;character&quot;, length = 400) k = 1 for (i in 1:20) { for (j in 1:20) { di_titles[k] &lt;- paste(peptides[i], peptides[j], sep = &quot;&quot;) k = k + 1 } } aa_di_titles &lt;- c(&quot;Class&quot;,&quot;TotalAA&quot;,&quot;PID&quot;, peptides, di_titles) return(aa_di_titles) } col_titles &lt;- column_titles() col_titles Write empty .csv write_empty_csv &lt;- function(protein_class = &quot;C&quot;) { col_titles &lt;- column_titles() file_name &lt;- paste(protein_class, &quot;_aac_dpc.csv&quot;, sep = &quot;&quot;) write.table(t(col_titles), file_name, sep = &quot;,&quot;, col.names = FALSE, row.names = FALSE, eol = &quot;\\n&quot;) return(file_name) } file_name &lt;- write_empty_csv() 7.4 Calculate AAC and DPC values function calc_aac_dpc &lt;- function(peptide, protein_class = &quot;C&quot;, i, file_name) { aa_nums = matrix(0, ncol = 423) ############################### # First column is class aa_nums[1] = ifelse(protein_class == &quot;C&quot;, 0, 1) # Second column is total number of amino acids total_aa = nchar(peptide) aa_nums[2] = total_aa # Third line is &#39;Protein ID&#39;, PID aa_nums[3] = paste(protein_class, i, sep = &quot;&quot;) # Column 4:423 - Calculate AAC/DPC for (j in 4:423) { aa_nums[j] = str_count(peptide, col_titles[j]) / total_aa } write(t(aa_nums), file = file_name, append = TRUE, ncolumns = 423, sep = &quot;,&quot;) } 7.5 Run Myoglobin # RUN Myoglobin for (i in 1:1124) { peptide &lt;- myoglobins[[i]][1] calc_aac_dpc(peptide, protein_class = &quot;M&quot;, i, file_name) } 7.6 Run Control / Human-NOT-myoglobin Import data - Read peptide lines read_fasta &lt;- function(file) { listo_proteins &lt;- read.fasta(file = file, seqtype = &quot;AA&quot;, as.string = TRUE, seqonly = FALSE, strip.desc = TRUE) return(listo_proteins) } file = &quot;./00-data/ORIGINAL_DATA/uniprot-human+NOT+hemoglobin+NOT+myoglobin+random.fasta&quot; controls &lt;- read_fasta(file) 7.7 Run Controls for (i in 1:1216) { peptide &lt;- controls[[i]][1] calc_aac_dpc(peptide, protein_class = &quot;C&quot;, i, file_name) } 7.8 KEEP AAC ONLY FOR RAW DATA file = &quot;./00-data/aac_dpc_values/C+M_aac_dpc.csv&quot; C+M_aac_dpc &lt;- read.csv(file, stringsAsFactors=FALSE) # View(`C+M_aac_dpc`) # Select 1st thru 23rd variables c_m_RAW_AAC &lt;- C+M_aac_dpc[c(1:23)] To A Comma Delimited Text File setwd(&quot;../00-data/02-aac_dpc_values/&quot;) write.table(c_m_RAW_AAC, file = &quot;./00-data/02-aac_dpc_values/c_m_RAW_AAC.csv&quot;, sep = &quot;,&quot;, row.names = F) 7.9 Transform {C, F, I} from c_m_RAW_AAC library(readr) file = &quot;../00-data/02-aac_dpc_values/c_m_RAW_AAC.csv&quot; c_m_RAW_AAC &lt;- read_csv(file, col_types = cols(Class = col_factor(levels = c(&quot;0&quot;,&quot;1&quot;)))) c_m_TRANSFORMED_AAC &lt;- c_m_RAW_AAC Transfrom C,F,I using sqrt(x) Columns: C=5, F=8, I=11 c_m_TRANSFORMED_AAC[, 5] &lt;- sqrt(c_m_TRANSFORMED_AAC[, 5]) # C c_m_TRANSFORMED_AAC[, 8] &lt;- sqrt(c_m_TRANSFORMED_AAC[, 8]) # F c_m_TRANSFORMED_AAC[,11] &lt;- sqrt(c_m_TRANSFORMED_AAC[,11]) # I file = &quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot; write_csv(c_m_TRANSFORMED_AAC, file = file, col_names = T) 7.10 Where To Find Help Cheat Sheets https://community.rstudio.com https://www.reddit.com/r/RStudio/ https://R-bloggers.com/ https://resources.rstudio.com/ Rpubs.com Rpubs.com contains R/RStudio notebooks and Markdown pages, VERY HELPFUL work from other peoples online R documents. It is a way to learn from others and share your work. - Sign up, it is FREE! then press: Get Started NOTE: If you are interested in seeing what others have published search Google, Rpubs.com does not have its own search function. In Google, Search: site:rpubs.com eda Other sites: Coursera Stack Overflow Quora Roger Peng’s EDA Bookdown - terrible not necessary resource The Lean Publishing (https://leanpub.com) company contains a library in the form of FREE down-loadable books/pdfs. I recommend; How to be a modern scientist62 by Jeffrey Leek63 R Programming for Data Science64 by Roger Peng65 Exploratory Data Analysis with R66 by Roger Peng Data Analysis for the Life Sciences67 by Rafael Irizarry &amp; Michael Love 7.11 Machine Setting &amp; Session Info Sys.info()[c(1:3,5)] ## sysname release ## &quot;Linux&quot; &quot;4.15.0-88-generic&quot; ## version machine ## &quot;#88-Ubuntu SMP Tue Feb 11 20:11:34 UTC 2020&quot; &quot;x86_64&quot; sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Linux Mint 19.2 ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] nnet_7.3-12 MASS_7.3-51.5 png_0.1-7 jpeg_0.1-8.1 caret_6.0-85 ## [6] lattice_0.20-40 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [11] tidyr_1.0.2 tibble_2.1.3 tidyverse_1.3.0 ggfortify_0.4.8 ggplot2_3.2.1 ## [16] Boruta_6.0.0 ranger_0.12.1 doMC_1.3.6 iterators_1.0.12 foreach_1.4.8 ## [21] corrplot_0.84 RColorBrewer_1.1-2 readr_1.3.1 knitr_1.28 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-144 fs_1.3.1 lubridate_1.7.4 httr_1.4.1 ## [5] tools_3.6.2 backports_1.1.5 utf8_1.1.4 R6_2.4.1 ## [9] rpart_4.1-15 DBI_1.1.0 lazyeval_0.2.2 colorspace_1.4-1 ## [13] withr_2.1.2 tidyselect_1.0.0 gridExtra_2.3 compiler_3.6.2 ## [17] cli_2.0.1 rvest_0.3.5 xml2_1.2.2 labeling_0.3 ## [21] bookdown_0.17 scales_1.1.0 digest_0.6.24 rmarkdown_2.1 ## [25] pkgconfig_2.0.3 htmltools_0.4.0 dbplyr_1.4.2 highr_0.8 ## [29] rlang_0.4.4 readxl_1.3.1 rstudioapi_0.11 farver_2.0.3 ## [33] generics_0.0.2 jsonlite_1.6.1 ModelMetrics_1.2.2.1 magrittr_1.5 ## [37] Matrix_1.2-18 Rcpp_1.0.3 munsell_0.5.0 fansi_0.4.1 ## [41] lifecycle_0.1.0 pROC_1.16.1 stringi_1.4.6 yaml_2.2.1 ## [45] plyr_1.8.5 recipes_0.1.9 crayon_1.3.4 haven_2.2.0 ## [49] splines_3.6.2 hms_0.5.3 pillar_1.4.3 stats4_3.6.2 ## [53] reshape2_1.4.3 codetools_0.2-16 reprex_0.3.0 glue_1.3.1 ## [57] packrat_0.5.0 evaluate_0.14 data.table_1.12.8 modelr_0.1.5 ## [61] vctrs_0.2.3 cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 ## [65] xfun_0.12 gower_0.2.1 prodlim_2019.11.13 broom_0.5.4 ## [69] e1071_1.7-3 class_7.3-15 survival_3.1-8 timeDate_3043.102 ## [73] lava_1.6.6 ipred_0.9-9 https://leanpub.com/modernscientist↩ http://jtleek.com↩ https://leanpub.com/rprogramming↩ https://simplystatistics.org↩ https://leanpub.com/exdata↩ https://leanpub.com/dataanalysisforthelifesciences↩ "]
]
