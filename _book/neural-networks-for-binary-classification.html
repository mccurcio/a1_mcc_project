<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Neural Networks For Binary Classification | Comparison of Binary Classification Using Six Machine Learning Methods</title>
  <meta name="description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Neural Networks For Binary Classification | Comparison of Binary Classification Using Six Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  <meta name="github-repo" content="github.com/mccurcio/a1_mcc_project" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Neural Networks For Binary Classification | Comparison of Binary Classification Using Six Machine Learning Methods" />
  
  <meta name="twitter:description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  

<meta name="author" content="Matthew C. Curcio" />


<meta name="date" content="2020-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression-for-binary-classification.html"/>
<link rel="next" href="appendices.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#what-is-predictive-modeling"><i class="fa fa-check"></i><b>2.2</b> What is Predictive Modeling?</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#the-epicycle-of-analysis"><i class="fa fa-check"></i><b>2.3</b> The Epicycle of Analysis</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#predictive-modeling"><i class="fa fa-check"></i><b>2.4</b> Predictive Modeling</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>2.4.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.4.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#four-challenges-in-predictive-modeling"><i class="fa fa-check"></i><b>2.5</b> Four Challenges In Predictive Modeling</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#section-title"><i class="fa fa-check"></i><b>2.6</b> SECTION TITLE (??)</a><ul>
<li class="chapter" data-level="2.6.1" data-path="introduction.html"><a href="introduction.html#experimental-procedure"><i class="fa fa-check"></i><b>2.6.1</b> Experimental Procedure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#four-step-analysis"><i class="fa fa-check"></i><b>3.1.1</b> Four-Step Analysis</a></li>
<li class="chapter" data-level="3.1.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#useful-guides-for-exploratory-data-analysis"><i class="fa fa-check"></i><b>3.1.2</b> Useful Guides for Exploratory Data Analysis</a></li>
<li class="chapter" data-level="3.1.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#questions-during-eda"><i class="fa fa-check"></i><b>3.1.3</b> Questions During EDA</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#analysis-of-raw-data"><i class="fa fa-check"></i><b>3.2</b> Analysis of RAW data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visually-inspect-raw-data-files"><i class="fa fa-check"></i><b>3.2.1</b> Visually inspect RAW data files</a></li>
<li class="chapter" data-level="3.2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#inspect-raw-dataframe-structure-str"><i class="fa fa-check"></i><b>3.2.2</b> Inspect RAW dataframe structure, <code>str()</code></a></li>
<li class="chapter" data-level="3.2.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-data-head-tail"><i class="fa fa-check"></i><b>3.2.3</b> Check RAW data <code>head</code> &amp; <code>tail</code></a></li>
<li class="chapter" data-level="3.2.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-data-types"><i class="fa fa-check"></i><b>3.2.4</b> Check RAW data types</a></li>
<li class="chapter" data-level="3.2.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-dataframe-dimensions"><i class="fa fa-check"></i><b>3.2.5</b> Check RAW dataframe dimensions</a></li>
<li class="chapter" data-level="3.2.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-for-missing-values"><i class="fa fa-check"></i><b>3.2.6</b> Check RAW for missing values</a></li>
<li class="chapter" data-level="3.2.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#number-of-polypeptides-per-class"><i class="fa fa-check"></i><b>3.2.7</b> Number of polypeptides per Class:</a></li>
<li class="chapter" data-level="3.2.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#numerical-summary-of-raw-features"><i class="fa fa-check"></i><b>3.2.8</b> Numerical summary of RAW features</a></li>
<li class="chapter" data-level="3.2.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visualize-raw-data-with-descriptive-statistics"><i class="fa fa-check"></i><b>3.2.9</b> Visualize RAW Data With Descriptive Statistics</a></li>
<li class="chapter" data-level="3.2.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatter-plot-of-means-of-myoglobin-control-amino-acid-composition-of-c_m_raw_aac-dataframe"><i class="fa fa-check"></i><b>3.2.10</b> Scatter plot of means of <em>Myoglobin-Control</em> amino acid composition of <code>c_m_RAW_AAC</code> dataframe</a></li>
<li class="chapter" data-level="3.2.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#means-of-percent-amino-acid-composition-of-control-myoglobin-categories-raw-data"><i class="fa fa-check"></i><b>3.2.11</b> Means of percent amino acid composition of control &amp; myoglobin categories, RAW data</a></li>
<li class="chapter" data-level="3.2.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-grand-means-of-overall-amino-acid-composition-raw-data"><i class="fa fa-check"></i><b>3.2.12</b> Boxplots of grand-means of overall amino acid composition, RAW data</a></li>
<li class="chapter" data-level="3.2.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-control-only-raw-data"><i class="fa fa-check"></i><b>3.2.13</b> Boxplots of amino acid compositions for control (only), RAW data</a></li>
<li class="chapter" data-level="3.2.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-myoglobin-only-raw-data"><i class="fa fa-check"></i><b>3.2.14</b> Boxplots of amino acid compositions for myoglobin (only), RAW data</a></li>
<li class="chapter" data-level="3.2.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-length-of-polypeptides-for-combined-raw-data"><i class="fa fa-check"></i><b>3.2.15</b> Boxplots Of Length Of Polypeptides For Combined RAW Data</a></li>
<li class="chapter" data-level="3.2.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-coefficient-of-variance-for-raw-data"><i class="fa fa-check"></i><b>3.2.16</b> Plot Coefficient Of Variance For RAW Data</a></li>
<li class="chapter" data-level="3.2.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness-of-distributions-raw-data"><i class="fa fa-check"></i><b>3.2.17</b> Skewness of distributions, RAW data</a></li>
<li class="chapter" data-level="3.2.18" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#qq-plots-of-20-amino-acids-raw-data"><i class="fa fa-check"></i><b>3.2.18</b> QQ-Plots of 20 amino acids, RAW data</a></li>
<li class="chapter" data-level="3.2.19" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#determine-coefficients-of-correlation-raw-data"><i class="fa fa-check"></i><b>3.2.19</b> Determine coefficients of correlation, RAW data</a></li>
<li class="chapter" data-level="3.2.20" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#how-to-dimension-reduction-using-high-correlation"><i class="fa fa-check"></i><b>3.2.20</b> How to: Dimension Reduction using High Correlation</a></li>
<li class="chapter" data-level="3.2.21" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boruta---dimensionality-reduction-raw-data"><i class="fa fa-check"></i><b>3.2.21</b> Boruta - dimensionality reduction, RAW data</a></li>
<li class="chapter" data-level="3.2.22" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-variable-importance"><i class="fa fa-check"></i><b>3.2.22</b> Plot variable importance</a></li>
<li class="chapter" data-level="3.2.23" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#variable-importance-scores"><i class="fa fa-check"></i><b>3.2.23</b> Variable importance scores</a></li>
<li class="chapter" data-level="3.2.24" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#conclusion-for-boruta-random-forest-test"><i class="fa fa-check"></i><b>3.2.24</b> Conclusion for Boruta random forest test</a></li>
<li class="chapter" data-level="3.2.25" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#conclusions-for-eda-raw-data"><i class="fa fa-check"></i><b>3.2.25</b> Conclusions For EDA, RAW data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#analysis-of-transformed-data"><i class="fa fa-check"></i><b>3.3</b> Analysis of TRANSFORMED data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-transformed-dataframe-dimensions"><i class="fa fa-check"></i><b>3.3.1</b> Check Transformed dataframe dimensions</a></li>
<li class="chapter" data-level="3.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-transformed-for-missing-values"><i class="fa fa-check"></i><b>3.3.2</b> Check Transformed for missing values</a></li>
<li class="chapter" data-level="3.3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#count-transformed-data-for-the-number-of-polypeptides-per-class"><i class="fa fa-check"></i><b>3.3.3</b> Count Transformed data for the number of polypeptides per class</a></li>
<li class="chapter" data-level="3.3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visualization-of-transformed-data-descriptive-statistics"><i class="fa fa-check"></i><b>3.3.4</b> Visualization of Transformed Data Descriptive Statistics</a></li>
<li class="chapter" data-level="3.3.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatter-plot-of-means-of-myoglobin-control-amino-acid-composition-sqrt-x_i-transformed-c_m_transformed-dataframe"><i class="fa fa-check"></i><b>3.3.5</b> Scatter plot of means of <em>Myoglobin-Control</em> amino acid composition <span class="math inline">\(\sqrt x_i\)</span> Transformed (c_m_TRANSFORMED) dataframe</a></li>
<li class="chapter" data-level="3.3.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#grouped-bar-chart-of-means-for-percent-amino-acid-composition-of-transformed-data-control-myoglobin-categories"><i class="fa fa-check"></i><b>3.3.6</b> Grouped bar chart of means for percent amino acid composition of Transformed Data; control &amp; myoglobin categories</a></li>
<li class="chapter" data-level="3.3.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-grand-means-of-the-overall-amino-acid-composition-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.7</b> Boxplots of grand-means of the overall amino acid composition of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-control-only-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.8</b> Boxplots of amino acid compositions for control (only) of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-myoglobin-of-square-root-transformed-dataonly-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.9</b> Boxplots of amino acid compositions for myoglobin of square-root transformed Data(only) of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-length-of-polypeptides-of-transformed-data-myoglobin-control-combined"><i class="fa fa-check"></i><b>3.3.10</b> Boxplots Of Length Of Polypeptides Of Transformed Data; Myoglobin, Control &amp; Combined</a></li>
<li class="chapter" data-level="3.3.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#coefficient-of-variance-cv-of-transformed-data"><i class="fa fa-check"></i><b>3.3.11</b> Coefficient of variance (CV) Of Transformed data</a></li>
<li class="chapter" data-level="3.3.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-of-coefficient-of-variance-cv"><i class="fa fa-check"></i><b>3.3.12</b> Plot of Coefficient Of Variance (CV)</a></li>
<li class="chapter" data-level="3.3.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness-of-distributions-of-transformed-data"><i class="fa fa-check"></i><b>3.3.13</b> Skewness of distributions Of Transformed Data</a></li>
<li class="chapter" data-level="3.3.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#qq-plots-of-20-amino-acids-of-transformed-data"><i class="fa fa-check"></i><b>3.3.14</b> QQ Plots of 20 amino acids of Transformed data</a></li>
<li class="chapter" data-level="3.3.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#determine-coefficients-of-correlation-of-transformed-data"><i class="fa fa-check"></i><b>3.3.15</b> Determine coefficients of correlation of Transformed Data</a></li>
<li class="chapter" data-level="3.3.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boruta---dimensionality-reduction-of-transformed-data"><i class="fa fa-check"></i><b>3.3.16</b> Boruta - dimensionality reduction of Transformed data</a></li>
<li class="chapter" data-level="3.3.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-variable-importance-1"><i class="fa fa-check"></i><b>3.3.17</b> Plot variable importance</a></li>
<li class="chapter" data-level="3.3.18" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#variable-importance-scores-1"><i class="fa fa-check"></i><b>3.3.18</b> Variable importance scores</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#eda-conclusion"><i class="fa fa-check"></i><b>3.4</b> EDA Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><i class="fa fa-check"></i><b>4</b> Principle Component Analysis of Myoglobin/Control Protein Sets</a><ul>
<li class="chapter" data-level="4.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#data-centering-scaling-normalization"><i class="fa fa-check"></i><b>4.2</b> Data centering / scaling / normalization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#histograms-of-scaled-vs.-unscaled-data"><i class="fa fa-check"></i><b>4.2.1</b> Histograms of Scaled Vs. Unscaled data</a></li>
<li class="chapter" data-level="4.2.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#finding-the-covariance-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Finding the Covariance Matrix</a></li>
<li class="chapter" data-level="" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#covariance-of-two-variables"><i class="fa fa-check"></i>Covariance of two variables</a></li>
<li class="chapter" data-level="" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#covariance-of-matrices"><i class="fa fa-check"></i>Covariance of matrices</a></li>
<li class="chapter" data-level="4.2.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#finding-pca-via-singular-value-decomposition"><i class="fa fa-check"></i><b>4.2.3</b> Finding PCA via singular value decomposition</a></li>
<li class="chapter" data-level="4.2.4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#example-of-two-dimensional-pca-using-random-data"><i class="fa fa-check"></i><b>4.2.4</b> Example of two-dimensional PCA using random data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#principle-component-analysis-using-norm_c_m_20aa"><i class="fa fa-check"></i><b>4.3</b> Principle component analysis using <code>norm_c_m_20aa</code></a></li>
<li class="chapter" data-level="4.4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#screeplot-and-cumulative-proportion-of-variance-plot"><i class="fa fa-check"></i><b>4.4</b> Screeplot and Cumulative Proportion of Variance plot</a></li>
<li class="chapter" data-level="4.5" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplots"><i class="fa fa-check"></i><b>4.5</b> Biplots</a><ul>
<li class="chapter" data-level="4.5.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplot-1-pc1-vs.-pc2-with-class-by-color-labels"><i class="fa fa-check"></i><b>4.5.1</b> Biplot 1: PC1 Vs. PC2 with ‘Class’ by color labels</a></li>
<li class="chapter" data-level="4.5.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplot-2-determination-of-4-rule-set-for-outliers"><i class="fa fa-check"></i><b>4.5.2</b> Biplot 2: Determination Of 4 Rule Set For Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#obtain-outliers-from-biplot-2-pc1-vs.-pc2"><i class="fa fa-check"></i><b>4.6</b> Obtain Outliers From Biplot #2: PC1 Vs. PC2</a><ul>
<li class="chapter" data-level="4.6.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-from-principal-component-1"><i class="fa fa-check"></i><b>4.6.1</b> Outliers from Principal Component-1</a></li>
<li class="chapter" data-level="4.6.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-from-principal-component-2"><i class="fa fa-check"></i><b>4.6.2</b> Outliers from Principal Component-2</a></li>
<li class="chapter" data-level="4.6.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#list-of-all-outliers-union-and-sorted-found-using-the-ruleset-1-through-4"><i class="fa fa-check"></i><b>4.6.3</b> List of all outliers (union and sorted) found using the ruleset 1 through 4</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#pca-conclusions"><i class="fa fa-check"></i><b>4.7</b> PCA Conclusions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-derived-from-pc1-vs-pc2"><i class="fa fa-check"></i><b>4.7.1</b> Outliers derived from PC1 Vs PC2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression For Binary Classification</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-1-using-20-features"><i class="fa fa-check"></i><b>5.2</b> Logit Training #1 Using 20 Features</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-results-1"><i class="fa fa-check"></i><b>5.3</b> Logit Results #1</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-2-using-9-features"><i class="fa fa-check"></i><b>5.4</b> Logit Training #2 Using 9 Features</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-2-using-9-features-1"><i class="fa fa-check"></i><b>5.4.1</b> Logit Training #2 Using 9 Features</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-summary-2"><i class="fa fa-check"></i><b>5.5</b> Logit Summary #2</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-confusion-matrix-2"><i class="fa fa-check"></i><b>5.6</b> Logit Confusion Matrix #2</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#obtain-list-of-false-positives-false-negatives"><i class="fa fa-check"></i><b>5.7</b> Obtain List of False Positives &amp; False Negatives</a></li>
<li class="chapter" data-level="5.8" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-conclusion"><i class="fa fa-check"></i><b>5.8</b> Logit Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html"><i class="fa fa-check"></i><b>6</b> Neural Networks For Binary Classification</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#the-one-neuron-system"><i class="fa fa-check"></i><b>6.2</b> The One Neuron System</a><ul>
<li class="chapter" data-level="6.2.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#summation-function"><i class="fa fa-check"></i><b>6.2.1</b> Summation Function</a></li>
<li class="chapter" data-level="6.2.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#activation-functions"><i class="fa fa-check"></i><b>6.2.2</b> Activation Functions</a></li>
<li class="chapter" data-level="6.2.3" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#binary-output-or-probability"><i class="fa fa-check"></i><b>6.2.3</b> Binary Output Or Probability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#the-two-neuron-system"><i class="fa fa-check"></i><b>6.3</b> The Two Neuron System</a><ul>
<li class="chapter" data-level="6.3.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#feed-forward-in-a-two-neuron-network"><i class="fa fa-check"></i><b>6.3.1</b> Feed-Forward In A Two Neuron Network</a></li>
<li class="chapter" data-level="6.3.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#error-back-propagation"><i class="fa fa-check"></i><b>6.3.2</b> Error Back-propagation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#neural-network-experiment-for-binary-classification"><i class="fa fa-check"></i><b>6.4</b> Neural Network Experiment For Binary Classification</a><ul>
<li class="chapter" data-level="6.4.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#train-model-with-neural-networks"><i class="fa fa-check"></i><b>6.4.1</b> Train model with neural networks</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#neural-network-conclusion"><i class="fa fa-check"></i><b>6.5</b> Neural Network Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>7</b> Appendices</a><ul>
<li class="chapter" data-level="7.1" data-path="appendices.html"><a href="appendices.html#install-r-rstudio"><i class="fa fa-check"></i><b>7.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="7.2" data-path="appendices.html"><a href="appendices.html#load-libraries-used-in-this-project"><i class="fa fa-check"></i><b>7.2</b> Load Libraries Used In This Project</a></li>
<li class="chapter" data-level="7.3" data-path="appendices.html"><a href="appendices.html#calculate-the-amino-acid-compositions-aac-and-di-peptide-compositions-dpc"><i class="fa fa-check"></i><b>7.3</b> Calculate the amino acid compositions (AAC) and Di-peptide compositions (DPC)</a></li>
<li class="chapter" data-level="7.4" data-path="appendices.html"><a href="appendices.html#calculate-aac-and-dpc-values-function"><i class="fa fa-check"></i><b>7.4</b> Calculate AAC and DPC values function</a></li>
<li class="chapter" data-level="7.5" data-path="appendices.html"><a href="appendices.html#run-myoglobin"><i class="fa fa-check"></i><b>7.5</b> Run Myoglobin</a></li>
<li class="chapter" data-level="7.6" data-path="appendices.html"><a href="appendices.html#run-control-human-not-myoglobin"><i class="fa fa-check"></i><b>7.6</b> Run Control / Human-NOT-myoglobin</a></li>
<li class="chapter" data-level="7.7" data-path="appendices.html"><a href="appendices.html#run-controls"><i class="fa fa-check"></i><b>7.7</b> Run Controls</a></li>
<li class="chapter" data-level="7.8" data-path="appendices.html"><a href="appendices.html#keep-aac-only-for-raw-data"><i class="fa fa-check"></i><b>7.8</b> KEEP AAC ONLY FOR RAW DATA</a></li>
<li class="chapter" data-level="7.9" data-path="appendices.html"><a href="appendices.html#transform-c-f-i-from-c_m_raw_aac"><i class="fa fa-check"></i><b>7.9</b> Transform {C, F, I} from c_m_RAW_AAC</a></li>
<li class="chapter" data-level="7.10" data-path="appendices.html"><a href="appendices.html#where-to-find-help"><i class="fa fa-check"></i><b>7.10</b> Where To Find Help</a></li>
<li class="chapter" data-level="7.11" data-path="appendices.html"><a href="appendices.html#machine-setting-session-info"><i class="fa fa-check"></i><b>7.11</b> Machine Setting &amp; Session Info</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Comparison of Binary Classification Using Six Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-networks-for-binary-classification" class="section level1">
<h1><span class="header-section-number">6</span> Neural Networks For Binary Classification</h1>
<blockquote>
<p>“Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions”</p>
<p>– Ian Goodfellow, et al<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a></p>
</blockquote>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction</h2>
<p>If we discuss Neural Networks (NN), we should first consider the system we hope to emulate. Let us start with a simple count of neuronal cells in various organisms along the earth’s phylogenetic tree. We might get a better idea of the type of “computing power” these living creatures possess. See table 5.1.</p>
<div id="table-5.1-organisms-vs-number-of-neurons-in-each-wikipedia" class="section level4 unnumbered">
<h4>Table 5.1: Organisms Vs Number of Neurons In Each (<a href="https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons">Wikipedia</a>)</h4>
<table>
<thead>
<tr class="header">
<th align="left">Organism</th>
<th align="right">Common Name</th>
<th align="right">Approximate Number of Neurons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">C. elegans</td>
<td align="right">roundworm</td>
<td align="right">302</td>
</tr>
<tr class="even">
<td align="left">Chrysaora fuscescens</td>
<td align="right">jellyfish</td>
<td align="right">5,600</td>
</tr>
<tr class="odd">
<td align="left">Apis linnaeus</td>
<td align="right">honey bee</td>
<td align="right">960,000</td>
</tr>
<tr class="even">
<td align="left">Mus musculus</td>
<td align="right">mouse</td>
<td align="right">71,000,000</td>
</tr>
<tr class="odd">
<td align="left">Felis silvestris</td>
<td align="right">cat</td>
<td align="right">760,000,000</td>
</tr>
<tr class="even">
<td align="left">Canis lupus familiaris</td>
<td align="right">dog</td>
<td align="right">2,300,000,000</td>
</tr>
<tr class="odd">
<td align="left">Homo sapien sapien</td>
<td align="right">humans</td>
<td align="right">100,000,000,000</td>
</tr>
</tbody>
</table>
<p>This table portrays a high-level overview of the computing power of neuronal clusters and brains produced throughout evolution. However, there is one missing number worth noting. The table above does not describe the connectivity between neurons. The connectivity of neurons varies greatly from lower to higher organisms. For example, some simple animals have only “four to eight separate branches,” <a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> per nerve cell. While human neurons may have approximately <span class="math inline">\(10^4\)</span> inter-connected synaptic junctions per neuron, thus resulting in a total of approximately 600 x <span class="math inline">\(10^{12}\)</span> synapses per human brain. <a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a></p>
<p>Although neurons have differing morphologies, neurons in the human brain are extremely diverse. Indeed, size and shape may not be the definitive way of classifying neurons but instead by what neurotransmitters the cells secrete. “Neurotransmitters can be classified as either excitatory or inhibitory.”<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a> Currently the <a href="http://isyslab.info/NeuroPep/home.jsp">NeuroPep</a> database “holds 5949 non-redundant neuropeptide entries originating from 493 organisms belonging to 65 neuropeptide families.”<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a></p>
<div style="page-break-after: always;"></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-30"></span>
<img src="_main_files/figure-html/unnamed-chunk-30-1.png" alt="Basic Neuron Types and S.E.M. Image" width="384" />
<p class="caption">
Figure 6.1: Basic Neuron Types and S.E.M. Image
</p>
</div>
<p><a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<img src="_main_files/figure-html/unnamed-chunk-31-1.png" alt="Two Neuron System (Image From The Public Domain)" width="384" />
<p class="caption">
Figure 6.2: Two Neuron System (Image From The Public Domain)
</p>
</div>
<p>Given an order of operation via:</p>
<p>Dendrite(s) <span class="math inline">\(\Longrightarrow\)</span> Cell body <span class="math inline">\(\Longrightarrow\)</span> Fibrous Axon <span class="math inline">\(\Longrightarrow\)</span> Synaptic Junction or Synaptic Gap <span class="math inline">\(\Longrightarrow\)</span> Dendrite(s) … Ad infinitum.</p>
<p>However, nature is more subtle and intricate than to have neurons in a series, only blinking on and off, firing or not. NN are often programmed to classify dangerous road objects, as is the case of Tesla cars. The goal of a Tesla auto-piloted car is to use all available sensors to correctly classify all the conceivable circumstances on the road. On the road, a Tesla automobile uses dozens of senors which the computer needs to evaluate and weigh the values of all these sensors to formulate a ‘decision.’ The altitude of the auto, derived from the GPS, may weigh less heavily than the speed of the vehicle or Lidar estimates on how close objects are. However, our goal of safe driving can be thwarted when an artificial intelligence system decides a truck is a sign and does not apply the brakes.<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="_main_files/figure-html/unnamed-chunk-32-1.png" alt="Goal of a Tesla Neural Networks is to generate the correct repsonses for its environment." width="288" />
<p class="caption">
Figure 6.3: Goal of a Tesla Neural Networks is to generate the correct repsonses for its environment.
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="the-one-neuron-system" class="section level2">
<h2><span class="header-section-number">6.2</span> The One Neuron System</h2>
If we investigate a one neuron system, <em>our</em> neuron could be diagrammed in four sections.<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-33"></span>
<img src="_main_files/figure-html/unnamed-chunk-33-1.png" alt="One Neuron Schema" width="384" />
<p class="caption">
Figure 6.4: One Neuron Schema
</p>
</div>
<p>If we investigate one neuron for a moment, we find two separate mathematical functions are being carried out by a single nerve cell.</p>
<div id="summation-function" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Summation Function</h3>
<p>The first segment is a summation function. It receives the real number values from, <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_n\)</span>, all the branches of the dendritic trees, and multiplies them by a set of weights. These <span class="math inline">\(X\)</span> inputs are multiplied by a set of corresponding unique weights from <span class="math inline">\(w_1\)</span> to <span class="math inline">\(w_n\)</span>. An analogy I prefer is of small or large rivers joining giving a total current. The current moves through the branches giving a total signal or current of sodium ions. Interestingly the summation in each neuron, while dealing with the vectors of inputs and weights, is carrying out the <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length">dot product</a> of these vectors, such that;</p>
<p>Initially, the NN used the Heaviside-Threshold Function, as shown in figure 4, the ‘One Neuron System.’ The benefits of step functions were their simplicity and high signal to noise ratio. While the detriments were, it is a discontinuous function, therefore not able to be differentiated and a mathematical problem.</p>
<p>Let us take into account the product, <span class="math inline">\(x_0 \cdot w_0\)</span>. If we assign <span class="math inline">\(x_0 = T\)</span> and <span class="math inline">\(w_0 = -1\)</span> this simply becomes a bias. This bias allows us the ability to shift our Activation Function and its inflection point in the positive or negative x-direction.</p>
<p><span class="math display">\[\begin{equation} 
\large \hat Y ~=~ X^T \cdot W - Bias ~~\equiv~~ \sum_{i=0}^n x_i w_i - T
\end{equation}\]</span></p>
</div>
<div id="activation-functions" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Activation Functions</h3>
<p>The second function is called an Activation Function. Once the Summation Function yields a value, its result is sent to the <em>Activation Function</em> or <em>Threshold Function</em>.</p>
<p><span class="math display">\[\begin{equation} 
  \large {Z}^{(1)} = f \left( \sum_{i=0}^n x_i w_i - T\right) = \{0, 1\}
\end{equation}\]</span></p>
<p>The function displayed in figure #4, One Neuron Schema, is a step function. However this step function has a problem mathematically, namely it is a discontinuous and therefore not differentiable. This fact is important.</p>
<p>Therefore several functions may be used in place of the step function. One is the hyperbolic tangent (<em>tanh</em>) function, the <em>sigmoidal</em> function, a <em>Hard Tanh</em>, a <em>reLU</em>, and <em>Softmax</em> Functions. These have certain advantages, namely they simplify the hyperbolic tangent function. Not only does the Hard Tanh and reLU simplify calculations it is useful for increasing the gain near the asymptotic limits of the sigmoidal and tanh functions. The derivatives of the sigmoidal and tanh functions are very small, near 0 and 1, while the reLU and Hard Tanh slopes are one or zero.</p>
<p><span class="math display">\[\begin{equation} 
  \large Z^{(2)} ~=~ tanh(x) = \frac{1 - e^{-{\alpha}}}{1 + e^{-{\alpha}}} ~~~:~~~ \large where ~~~ \large \alpha = \sum_{i=1}^n x_i w_i - T
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} 
  \large Z^{(3)} ~=~ sigmoid(x) ~=~ \frac{1}{1 + e^{-{\alpha}}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} 
  \large Z^{(4)} ~=~ Hard ~ Tanh (x) ~=~ \large \left\{ \begin{array}{rcl} 1 &amp;  x &gt; 1 \\ x &amp; -1 \leq x \leq 1 \\ -1 &amp; x &lt; -1 \end{array}\right.
\end{equation}\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Several alternative functions are useful for various reasons. The most common of which are Softmax and reLU functions.</p>
<p>Rectified Linear Activation Unit, (ReLU):</p>
<p><span class="math display">\[\begin{equation} 
  \large Z^{(5)} ~=~ \large ReLU ~= \begin{cases} x \geqq 0 ~~~~y = x\\ x &lt; 0 ~~~~y = 0 \end{cases}
\end{equation}\]</span></p>
</div>
<div id="binary-output-or-probability" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Binary Output Or Probability</h3>
<p>In the case of real neurons, the output is off or on, zero or one. However, in the case of our electronic model, it is advantageous to calculate a probability for greater interpretability.</p>
<blockquote>
<p>The Softmax function may appear like the Sigmoid function from above but it differs in major ways.<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a></p>
<ul>
<li>The softmax activation function returns the probability distribution over mutually exclusive output classes.</li>
<li>The calculated probabilities will be in the range of 0 to 1.</li>
<li>The sum of all the probabilities is equals to 1.</li>
</ul>
</blockquote>
<p>Typically the Softmax Function is used in binary or multiple classification logistic regression models and in building the final output layer of NN.</p>
<p><span class="math display">\[\begin{equation} 
  \large Z^{(6)} ~=~ Softmax(x) = \frac {e^{\alpha_i}}{\sum_{i=1}^n e^{\alpha_i}}
\end{equation}\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The benefit of these activation functions is that they are now differentiable. This fact becomes important for <em>Back-Propagation</em>, which is discussed later.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="the-two-neuron-system" class="section level2">
<h2><span class="header-section-number">6.3</span> The Two Neuron System</h2>
<p>Building up in complexity, let us could consider our first Neural Network by using <em>only</em> two neurons. In two neuron systems, let us first generalize a bit more by adding that <span class="math inline">\(X\)</span> is an array of all the inputs as is <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> is also an array of weights for each neuron. See figure #5.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-36"></span>
<img src="_main_files/figure-html/unnamed-chunk-36-1.png" alt="A Two Neuron System" width="480" />
<p class="caption">
Figure 6.5: A Two Neuron System
</p>
</div>
<div id="feed-forward-in-a-two-neuron-network" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Feed-Forward In A Two Neuron Network</h3>
<p>In our two neuron network, we can now write out the mathematics for each step as it progresses in a “forward” (left to right) direction.</p>
<p>Step #1: To move from <span class="math inline">\(X\)</span> to <span class="math inline">\(P_1\)</span>
<span class="math display">\[\begin{equation} 
  f^1( \overrightarrow{x}, \overrightarrow{w}) \equiv~~ P_1 = \left( X^T \cdot W_1 - T \right)
\end{equation}\]</span></p>
<p>Step #2: <span class="math inline">\(P_1\)</span> feeds forward to <span class="math inline">\(Y\)</span>
<span class="math display">\[\begin{equation} 
  f^2(P_1)  ~~\equiv~ \hat Y = \left( \frac{1}{1 + e^{- \alpha}} \right) ~~:~~ where ~~~ \alpha = P_1
\end{equation}\]</span></p>
<p>Step #3: <span class="math inline">\(Y\)</span> feeds forward to <span class="math inline">\(P_2\)</span>
<span class="math display">\[\begin{equation}
  f^3(\overrightarrow{y}, \overrightarrow{w}) ~~\equiv~ P_2 = \left( Y^T \cdot W_2 - T \right)
\end{equation}\]</span></p>
<p>Step #4: <span class="math inline">\(P_2\)</span> feeds forward to <span class="math inline">\(Z\)</span>
<span class="math display">\[\begin{equation}
  f^4(P_2) ~~\equiv~ \hat Z = \left( \frac{1}{1 + e^{- \large \alpha}} \right) ~~~:~~~ where ~~ \alpha = P_2
\end{equation}\]</span></p>
<p>Step #5: Our complicated function is simply a matter of chaining one result so that it may be used in the next step.</p>
<p><span class="math display">\[\begin{equation}
   \hat Z ~=~ f^4 \left( f^3 \left( f^2 \left( f^1 \left( X, W \right) \right) \right) \right)
\end{equation}\]</span></p>
<p>In our <strong>Feed-Forward Propagation</strong>, we can now take the values from any numerical system and produce zeros, ones, or probabilities. Remember, in this set of experiments, we are using the concentrations of the 20 amino acids to provide a categorical or binary output, belongs to Myoglobin protein family, or does not.</p>
</div>
<div id="error-back-propagation" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Error Back-propagation</h3>
<p>Now that we have learned to calculate the output of our neurons using the Feed-Forward process, what if our final answer is incorrect? Can we build a feed back system to determine the weights needed to obtain our desired value of <span class="math inline">\(\hat z\)</span>? The answer is yes. The process for determining the weights is known as Back-Propagation. Back-Propagation, also known as error back-propagation, is crucial to understanding and tuning a neural network.</p>
<p>Simply stated Back-Propagation is an optimization routine which iteratively calculates the errors that occur at each stage of a neural network. Back-Propagation uses the partial derivatives of the feed forward functions, specifically. The chain rule and gradient descent are also used to determine the weights (<span class="math inline">\(W_1 ~~and~~ W_2\)</span>) which are propagated through the network to find weights used in the summation step of a neuron.<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a></p>
<p>This thumbnail sketch gives the building blocks to calculate <span class="math inline">\(W\)</span> which can be run until we reach a value that we desire. However the first time the back-propagation is carried out all the weights are chosen randomly. If the weights were set to the same number there would be no change throughout the system.</p>
<p>In the two neuron system, our first step is to generate an error or performance (Perf) function to minimize. If we call <span class="math inline">\(d\)</span> our desired value, we can minimize the square error, a common choice.<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a></p>
<p>Step #1: Performance (Perf)
<span class="math display">\[\begin{equation}
  \mathbf{Perf} ~~=~~ c \cdot (d - \hat z)^2
\end{equation}\]</span></p>
<p>Step #2:
<span class="math display">\[\begin{equation}
\frac{d Z}{d x} ~~=~~ \frac{d \left \{ f^4 \left( f^3 \left( f^2 \left( f^1 \left( X, W \right) \right) \right) \right) \right \}}{dx}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-37"></span>
<img src="_main_files/figure-html/unnamed-chunk-37-1.png" alt="A Two Neuron System" width="672" />
<p class="caption">
Figure 6.6: A Two Neuron System
</p>
</div>
<p>Using the chain-rule, working backward and the ‘Two Neuron System’ figure as a guide through the error back-propagation, we find:</p>
<p>Step #3: Neuron 2 <span class="math inline">\(\Rightarrow\)</span> 1
<span class="math display">\[\begin{equation}
  \frac{\delta Perf}{\delta w_1} ~=~ \frac{\delta Perf}{\delta z} \cdot \frac{\delta z}{\delta P_2} \cdot \frac{\delta P_2}{\delta y} \cdot \frac{\delta y}{\delta P_1} \cdot \frac{\delta P_2}{\delta w_1}
\end{equation}\]</span></p>
<p>Step #4: Performance
<span class="math display">\[\begin{equation}
  \frac{\delta Perf}{\delta z} ~~=~~ \frac{\delta \left\{ \frac{1}{2} \| \overrightarrow{d} - \overrightarrow{z} \|^2 \right\}} {\delta z} ~~=~~ \mathbf{\overrightarrow{d} - \overrightarrow{z}}
\end{equation}\]</span></p>
<p>Step #5: Substitute <span class="math inline">\(P_2=\alpha\)</span>
<span class="math display">\[\begin{equation}
\frac{\delta z}{\delta P_2} ~~=~~ \frac{\delta~ ((1 + e^{-\alpha})^{-1})}{\delta \alpha} ~~=~~ e^{-\alpha} \cdot (1 + e^{-\alpha})^{-2}
\end{equation}\]</span></p>
<p>Step #6: Rearrange the expression
<span class="math display">\[\begin{equation}
  \frac{ e^{-\alpha} }{ (1 + e^{-\alpha})^{-2} } ~~=~~ \frac{e^{-\alpha}}{1 + e^{-\alpha}} \cdot \frac{1}{1 + e^{-\alpha}}
\end{equation}\]</span></p>
<p>Step #7: Add 1 <em>and</em> subtract 1
<span class="math display">\[\begin{equation}
  = ~~ \frac{ (1+ e^{-\alpha}) -1 }{1 + e^{-\alpha}} \cdot \frac{1}{1 + e^{-\alpha}}
\end{equation}\]</span></p>
<p>Step #8: Rearrange to find
<span class="math display">\[\begin{equation}
 = ~~ \left( \frac{ 1+ e^{-\alpha} }{1 + e^{-\alpha}} ~-~ \frac{ 1 }{1 + e^{-\alpha}} \right)  \left( \frac{1}{1 + e^{-\alpha}} \right) ~~=~~ \left(1- \frac{1}{1 + e^{-\alpha}} \right) \left( \frac{1}{1 + e^{-\alpha}} \right)
\end{equation}\]</span></p>
<p>Step #9: Therefore we find
<span class="math display">\[\begin{equation}
\frac{\delta z}{\delta \alpha} ~~=~~ \frac{\delta~ ((1 + e^{-\alpha})^{-1})}{\delta \alpha} ~~=~~ \left(1- \frac{1}{1 + e^{-\alpha}} \right) \left( \frac{1}{1 + e^{-\alpha}} \right)
\end{equation}\]</span></p>
<p>Nevertheless, we need one more part to ascertain the weights. As the error back-propagation is computed this process does not reveal how much the weights need to be adjusted/changed to compute the next round of weights given their current errors. For this we require one last equation or concept.</p>
<p>Once we compute the weights from our chain rule set of equations we must change the values in the direction proportional to the change in error. This is performed by using gradient descent.</p>
<p>Step #10: Learning Rate
<span class="math display">\[\begin{equation}
\Delta W ~:~ W_{i+1} ~=~ W_i ~-~ \eta \cdot \frac{\delta Perf}{\delta W}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate for the system. The key to the learning rate is that it must be sought and its range mapped for optimum efficiency. However smaller rates have the advantage of not overshooting the desired minimum/maximum. If the learning rate is too large the values of <span class="math inline">\(W\)</span> may jump wildly and not settle into a max/min. There is a fine balance that must be considered such that the weights are not trapped in a local minimum and wildly oscillate unable to converge.</p>
<p>The last step of <em>error back-propagation</em> is simply setting up the derivatives mechanically and is not shown for brevity.</p>
</div>
</div>
<div id="neural-network-experiment-for-binary-classification" class="section level2">
<h2><span class="header-section-number">6.4</span> Neural Network Experiment For Binary Classification</h2>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="co"># Load Libraries</span></a>
<a class="sourceLine" id="cb83-2" data-line-number="2">Libraries &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;dplyr&quot;</span>, <span class="st">&quot;knitr&quot;</span>, <span class="st">&quot;readr&quot;</span>, <span class="st">&quot;caret&quot;</span>, <span class="st">&quot;MASS&quot;</span>, <span class="st">&quot;nnet&quot;</span>, <span class="st">&quot;purrr&quot;</span>, <span class="st">&quot;doMC&quot;</span>)</a>
<a class="sourceLine" id="cb83-3" data-line-number="3"><span class="cf">for</span> (p <span class="cf">in</span> Libraries) {  </a>
<a class="sourceLine" id="cb83-4" data-line-number="4">    <span class="kw">library</span>(p, <span class="dt">character.only =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb83-5" data-line-number="5">}</a></code></pre></div>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1">education &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv&quot;</span>,</a>
<a class="sourceLine" id="cb84-2" data-line-number="2">                      <span class="dt">col_types =</span> <span class="kw">cols</span>(<span class="dt">Class =</span> <span class="kw">col_factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>,<span class="st">&quot;1&quot;</span>)),</a>
<a class="sourceLine" id="cb84-3" data-line-number="3">                                       <span class="dt">PID =</span> <span class="kw">col_skip</span>(),</a>
<a class="sourceLine" id="cb84-4" data-line-number="4">                                       <span class="dt">TotalAA =</span> <span class="kw">col_skip</span>()))</a></code></pre></div>
<div id="create-training-data" class="section level4">
<h4><span class="header-section-number">6.4.0.1</span> Create Training Data</h4>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb85-2" data-line-number="2"><span class="co"># Stratified sampling</span></a>
<a class="sourceLine" id="cb85-3" data-line-number="3">TrainingDataIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(education<span class="op">$</span>Class, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb85-4" data-line-number="4"></a>
<a class="sourceLine" id="cb85-5" data-line-number="5"><span class="co"># Create Training Data </span></a>
<a class="sourceLine" id="cb85-6" data-line-number="6">trainingData &lt;-<span class="st"> </span>education[ TrainingDataIndex, ]</a>
<a class="sourceLine" id="cb85-7" data-line-number="7">testData     &lt;-<span class="st"> </span>education[<span class="op">-</span>TrainingDataIndex, ]</a>
<a class="sourceLine" id="cb85-8" data-line-number="8"></a>
<a class="sourceLine" id="cb85-9" data-line-number="9">TrainingParameters &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb85-10" data-line-number="10">                                   <span class="dt">number =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb85-11" data-line-number="11">                                   <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb85-12" data-line-number="12">                                   <span class="dt">savePredictions =</span> <span class="st">&quot;final&quot;</span>) <span class="co"># IMPORTANT: Saves predictions</span></a></code></pre></div>
</div>
<div id="train-model-with-neural-networks" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Train model with neural networks</h3>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">end_time <span class="op">-</span><span class="st"> </span>start_time            <span class="co"># Display time</span></a></code></pre></div>
<pre><code>## Time difference of 21.52432 secs</code></pre>
<div id="confusion-matrix-and-statistics" class="section level4">
<h4><span class="header-section-number">6.4.1.1</span> Confusion Matrix and Statistics</h4>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">NNPredictions &lt;-<span class="st"> </span><span class="kw">predict</span>(NNModel, testData)</a>
<a class="sourceLine" id="cb88-2" data-line-number="2"></a>
<a class="sourceLine" id="cb88-3" data-line-number="3"><span class="co"># Create confusion matrix</span></a>
<a class="sourceLine" id="cb88-4" data-line-number="4">cmNN &lt;-<span class="kw">confusionMatrix</span>(NNPredictions, testData<span class="op">$</span>Class)</a>
<a class="sourceLine" id="cb88-5" data-line-number="5"><span class="kw">print</span>(cmNN)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 237  12
##          1   6 212
##                                          
##                Accuracy : 0.9615         
##                  95% CI : (0.9398, 0.977)
##     No Information Rate : 0.5203         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9227         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.2386         
##                                          
##             Sensitivity : 0.9753         
##             Specificity : 0.9464         
##          Pos Pred Value : 0.9518         
##          Neg Pred Value : 0.9725         
##              Prevalence : 0.5203         
##          Detection Rate : 0.5075         
##    Detection Prevalence : 0.5332         
##       Balanced Accuracy : 0.9609         
##                                          
##        &#39;Positive&#39; Class : 0              
## </code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">NNModel</a></code></pre></div>
<pre><code>## Neural Network 
## 
## 1873 samples
##   20 predictor
##    2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## Pre-processing: scaled (20), centered (20) 
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 1498, 1498, 1499, 1499, 1498, 1499, ... 
## Resampling results across tuning parameters:
## 
##   size  decay  Accuracy   Kappa    
##   1     0e+00  0.9339109  0.8671305
##   1     1e-04  0.9346544  0.8686793
##   1     1e-01  0.9481084  0.8958797
##   3     0e+00  0.9565473  0.9128846
##   3     1e-04  0.9564355  0.9126691
##   3     1e-01  0.9607087  0.9212480
##   5     0e+00  0.9571825  0.9142100
##   5     1e-04  0.9570772  0.9139950
##   5     1e-01  0.9625266  0.9249177
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were size = 5 and decay = 0.1.</code></pre>
</div>
<div id="obtain-list-of-false-positives-false-negatives-1" class="section level4">
<h4><span class="header-section-number">6.4.1.2</span> Obtain List of False Positives &amp; False Negatives</h4>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">fp_fn_NNModel &lt;-<span class="st"> </span>NNModel <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pluck</span>(<span class="st">&quot;pred&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(obs <span class="op">!=</span><span class="st"> </span>pred)</a>
<a class="sourceLine" id="cb92-2" data-line-number="2"></a>
<a class="sourceLine" id="cb92-3" data-line-number="3"><span class="co"># Write/save .csv</span></a>
<a class="sourceLine" id="cb92-4" data-line-number="4"><span class="kw">write.table</span>(fp_fn_NNModel,</a>
<a class="sourceLine" id="cb92-5" data-line-number="5">            <span class="dt">file =</span> <span class="st">&quot;./00-data/03-ml_results/fp_fn_NN.csv&quot;</span>,</a>
<a class="sourceLine" id="cb92-6" data-line-number="6">            <span class="dt">row.names =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb92-7" data-line-number="7">            <span class="dt">na =</span> <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb92-8" data-line-number="8">            <span class="dt">col.names =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb92-9" data-line-number="9">            <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb92-10" data-line-number="10"></a>
<a class="sourceLine" id="cb92-11" data-line-number="11"><span class="kw">nrow</span>(fp_fn_NNModel) <span class="co">## </span><span class="al">NOTE</span><span class="co">: NOT UNIQUE NOR SORTED</span></a></code></pre></div>
<pre><code>## [1] 351</code></pre>
</div>
<div id="false-positive-false-negative-neural-network-set" class="section level4">
<h4><span class="header-section-number">6.4.1.3</span> False Positive &amp; False Negative Neural Network set</h4>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">keep &lt;-<span class="st"> &quot;rowIndex&quot;</span></a>
<a class="sourceLine" id="cb94-2" data-line-number="2"></a>
<a class="sourceLine" id="cb94-3" data-line-number="3">fp_fn_NN &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./00-data/03-ml_results/fp_fn_NN.csv&quot;</span>)</a>
<a class="sourceLine" id="cb94-4" data-line-number="4"></a>
<a class="sourceLine" id="cb94-5" data-line-number="5">NN_fp_fn_nums &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">unique</span>(<span class="kw">unlist</span>(fp_fn_NN[, keep], <span class="dt">use.names =</span> <span class="ot">FALSE</span>)))</a>
<a class="sourceLine" id="cb94-6" data-line-number="6"></a>
<a class="sourceLine" id="cb94-7" data-line-number="7"><span class="kw">length</span>(NN_fp_fn_nums)</a></code></pre></div>
<pre><code>## [1] 140</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1">NN_fp_fn_nums</a></code></pre></div>
<pre><code>##   [1]    1    2    4    6   10   15   46   57   58   88  100  114  115  116  130  136  141  146  150  170
##  [21]  172  182  183  185  191  201  231  239  249  254  260  349  407  421  427  430  436  439  449  452
##  [41]  453  503  516  518  526  530  531  532  534  536  537  542  546  547  551  554  562  566  570  573
##  [61]  575  576  577  580  583  584  589  592  655  910  913  980 1032 1033 1034 1035 1041 1067 1069 1092
##  [81] 1093 1094 1096 1100 1101 1106 1115 1121 1130 1144 1190 1219 1222 1223 1226 1233 1234 1245 1264 1279
## [101] 1282 1340 1471 1482 1484 1487 1510 1522 1569 1571 1575 1576 1577 1579 1582 1585 1587 1588 1589 1594
## [121] 1600 1608 1618 1619 1621 1622 1623 1693 1723 1734 1771 1780 1789 1828 1829 1830 1831 1832 1833 1873</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="kw">write_csv</span>(<span class="dt">x =</span> <span class="kw">as.data.frame</span>(NN_fp_fn_nums), </a>
<a class="sourceLine" id="cb98-2" data-line-number="2">          <span class="dt">path =</span> <span class="st">&quot;./00-data/04-sort_unique_outliers/NN_nums.csv&quot;</span>)</a></code></pre></div>
<ul>
<li>The Neural Network set included a total of 140 unique observations containing both FP and FN.</li>
</ul>
</div>
</div>
</div>
<div id="neural-network-conclusion" class="section level2">
<h2><span class="header-section-number">6.5</span> Neural Network Conclusion</h2>
<p>Lorem</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="51">
<li id="fn51"><p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, ‘Deep Learning’, MIT Press, 2016, <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a><a href="neural-networks-for-binary-classification.html#fnref51" class="footnote-back">↩</a></p></li>
<li id="fn52"><p><a href="https://www.wormatlas.org/hermaphrodite/nervous/Neuroframeset.html" class="uri">https://www.wormatlas.org/hermaphrodite/nervous/Neuroframeset.html</a><a href="neural-networks-for-binary-classification.html#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>Shepherd, G. M. (2004), The synaptic organization of the brain (5th ed.), Oxford University Press, New York.<a href="neural-networks-for-binary-classification.html#fnref53" class="footnote-back">↩</a></p></li>
<li id="fn54"><p><a href="https://www.kenhub.com/en/library/anatomy/neurotransmitters" class="uri">https://www.kenhub.com/en/library/anatomy/neurotransmitters</a><a href="neural-networks-for-binary-classification.html#fnref54" class="footnote-back">↩</a></p></li>
<li id="fn55"><p><a href="http://isyslab.info/NeuroPep/home.jsp" class="uri">http://isyslab.info/NeuroPep/home.jsp</a><a href="neural-networks-for-binary-classification.html#fnref55" class="footnote-back">↩</a></p></li>
<li id="fn56"><p><a href="https://www.howstuffworks.com/" class="uri">https://www.howstuffworks.com/</a><a href="neural-networks-for-binary-classification.html#fnref56" class="footnote-back">↩</a></p></li>
<li id="fn57"><p><a href="https://arstechnica.com/cars/2019/05/feds-autopilot-was-active-during-deadly-march-tesla-crash/" class="uri">https://arstechnica.com/cars/2019/05/feds-autopilot-was-active-during-deadly-march-tesla-crash/</a><a href="neural-networks-for-binary-classification.html#fnref57" class="footnote-back">↩</a></p></li>
<li id="fn58"><p>Tom Mitchell, Machine Learning, McGraw-Hill, 1997, ISBN: 0070428077<a href="neural-networks-for-binary-classification.html#fnref58" class="footnote-back">↩</a></p></li>
<li id="fn59"><p>Josh Patterson, Adam Gibson, Deep Learning; A Practitioner’s Approach, 2017, O’Rreilly<a href="neural-networks-for-binary-classification.html#fnref59" class="footnote-back">↩</a></p></li>
<li id="fn60"><p>David Rumelhart, Geoffrey Hinton, &amp; Ronald Williams, Learning represetnations by back-propagating Errors, Nature, 323, 533-536, Oct. 9, 1986<a href="neural-networks-for-binary-classification.html#fnref60" class="footnote-back">↩</a></p></li>
<li id="fn61"><p>Ivan N. da Silva, Danilo H. Spatti, Rogerio A. Flauzino, Luisa H. B. Liboni, Silas F. dos Reis Alves, Artificial Neural Networks: A Practical Course, DOI 10.1007/978-3-319-43162-8, 2017<a href="neural-networks-for-binary-classification.html#fnref61" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression-for-binary-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
