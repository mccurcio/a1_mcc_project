<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Introduction | Comparison of Binary Classification Using Six Machine Learning Methods</title>
  <meta name="description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Introduction | Comparison of Binary Classification Using Six Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  <meta name="github-repo" content="github.com/mccurcio/a1_mcc_project" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Introduction | Comparison of Binary Classification Using Six Machine Learning Methods" />
  
  <meta name="twitter:description" content="Matthew Curcio’s Masters thesis written for Worcester Polytechnic Institute" />
  

<meta name="author" content="Matthew C. Curcio" />


<meta name="date" content="2020-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="exploratory-data-analysis.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#what-is-predictive-modeling"><i class="fa fa-check"></i><b>2.2</b> What is Predictive Modeling?</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#the-epicycle-of-analysis"><i class="fa fa-check"></i><b>2.3</b> The Epicycle of Analysis</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#predictive-modeling"><i class="fa fa-check"></i><b>2.4</b> Predictive Modeling</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>2.4.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.4.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#four-challenges-in-predictive-modeling"><i class="fa fa-check"></i><b>2.5</b> Four Challenges In Predictive Modeling</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#section-title"><i class="fa fa-check"></i><b>2.6</b> SECTION TITLE (??)</a><ul>
<li class="chapter" data-level="2.6.1" data-path="introduction.html"><a href="introduction.html#experimental-procedure"><i class="fa fa-check"></i><b>2.6.1</b> Experimental Procedure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#four-step-analysis"><i class="fa fa-check"></i><b>3.1.1</b> Four-Step Analysis</a></li>
<li class="chapter" data-level="3.1.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#useful-guides-for-exploratory-data-analysis"><i class="fa fa-check"></i><b>3.1.2</b> Useful Guides for Exploratory Data Analysis</a></li>
<li class="chapter" data-level="3.1.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#questions-during-eda"><i class="fa fa-check"></i><b>3.1.3</b> Questions During EDA</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#analysis-of-raw-data"><i class="fa fa-check"></i><b>3.2</b> Analysis of RAW data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visually-inspect-raw-data-files"><i class="fa fa-check"></i><b>3.2.1</b> Visually inspect RAW data files</a></li>
<li class="chapter" data-level="3.2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#inspect-raw-dataframe-structure-str"><i class="fa fa-check"></i><b>3.2.2</b> Inspect RAW dataframe structure, <code>str()</code></a></li>
<li class="chapter" data-level="3.2.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-data-head-tail"><i class="fa fa-check"></i><b>3.2.3</b> Check RAW data <code>head</code> &amp; <code>tail</code></a></li>
<li class="chapter" data-level="3.2.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-data-types"><i class="fa fa-check"></i><b>3.2.4</b> Check RAW data types</a></li>
<li class="chapter" data-level="3.2.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-dataframe-dimensions"><i class="fa fa-check"></i><b>3.2.5</b> Check RAW dataframe dimensions</a></li>
<li class="chapter" data-level="3.2.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-raw-for-missing-values"><i class="fa fa-check"></i><b>3.2.6</b> Check RAW for missing values</a></li>
<li class="chapter" data-level="3.2.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#number-of-polypeptides-per-class"><i class="fa fa-check"></i><b>3.2.7</b> Number of polypeptides per Class:</a></li>
<li class="chapter" data-level="3.2.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#numerical-summary-of-raw-features"><i class="fa fa-check"></i><b>3.2.8</b> Numerical summary of RAW features</a></li>
<li class="chapter" data-level="3.2.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visualize-raw-data-with-descriptive-statistics"><i class="fa fa-check"></i><b>3.2.9</b> Visualize RAW Data With Descriptive Statistics</a></li>
<li class="chapter" data-level="3.2.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatter-plot-of-means-of-myoglobin-control-amino-acid-composition-of-c_m_raw_aac-dataframe"><i class="fa fa-check"></i><b>3.2.10</b> Scatter plot of means of <em>Myoglobin-Control</em> amino acid composition of <code>c_m_RAW_AAC</code> dataframe</a></li>
<li class="chapter" data-level="3.2.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#means-of-percent-amino-acid-composition-of-control-myoglobin-categories-raw-data"><i class="fa fa-check"></i><b>3.2.11</b> Means of percent amino acid composition of control &amp; myoglobin categories, RAW data</a></li>
<li class="chapter" data-level="3.2.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-grand-means-of-overall-amino-acid-composition-raw-data"><i class="fa fa-check"></i><b>3.2.12</b> Boxplots of grand-means of overall amino acid composition, RAW data</a></li>
<li class="chapter" data-level="3.2.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-control-only-raw-data"><i class="fa fa-check"></i><b>3.2.13</b> Boxplots of amino acid compositions for control (only), RAW data</a></li>
<li class="chapter" data-level="3.2.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-myoglobin-only-raw-data"><i class="fa fa-check"></i><b>3.2.14</b> Boxplots of amino acid compositions for myoglobin (only), RAW data</a></li>
<li class="chapter" data-level="3.2.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-length-of-polypeptides-for-combined-raw-data"><i class="fa fa-check"></i><b>3.2.15</b> Boxplots Of Length Of Polypeptides For Combined RAW Data</a></li>
<li class="chapter" data-level="3.2.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-coefficient-of-variance-for-raw-data"><i class="fa fa-check"></i><b>3.2.16</b> Plot Coefficient Of Variance For RAW Data</a></li>
<li class="chapter" data-level="3.2.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness-of-distributions-raw-data"><i class="fa fa-check"></i><b>3.2.17</b> Skewness of distributions, RAW data</a></li>
<li class="chapter" data-level="3.2.18" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#qq-plots-of-20-amino-acids-raw-data"><i class="fa fa-check"></i><b>3.2.18</b> QQ-Plots of 20 amino acids, RAW data</a></li>
<li class="chapter" data-level="3.2.19" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#determine-coefficients-of-correlation-raw-data"><i class="fa fa-check"></i><b>3.2.19</b> Determine coefficients of correlation, RAW data</a></li>
<li class="chapter" data-level="3.2.20" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#how-to-dimension-reduction-using-high-correlation"><i class="fa fa-check"></i><b>3.2.20</b> How to: Dimension Reduction using High Correlation</a></li>
<li class="chapter" data-level="3.2.21" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boruta---dimensionality-reduction-raw-data"><i class="fa fa-check"></i><b>3.2.21</b> Boruta - dimensionality reduction, RAW data</a></li>
<li class="chapter" data-level="3.2.22" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-variable-importance"><i class="fa fa-check"></i><b>3.2.22</b> Plot variable importance</a></li>
<li class="chapter" data-level="3.2.23" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#variable-importance-scores"><i class="fa fa-check"></i><b>3.2.23</b> Variable importance scores</a></li>
<li class="chapter" data-level="3.2.24" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#conclusion-for-boruta-random-forest-test"><i class="fa fa-check"></i><b>3.2.24</b> Conclusion for Boruta random forest test</a></li>
<li class="chapter" data-level="3.2.25" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#conclusions-for-eda-raw-data"><i class="fa fa-check"></i><b>3.2.25</b> Conclusions For EDA, RAW data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#analysis-of-transformed-data"><i class="fa fa-check"></i><b>3.3</b> Analysis of TRANSFORMED data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-transformed-dataframe-dimensions"><i class="fa fa-check"></i><b>3.3.1</b> Check Transformed dataframe dimensions</a></li>
<li class="chapter" data-level="3.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#check-transformed-for-missing-values"><i class="fa fa-check"></i><b>3.3.2</b> Check Transformed for missing values</a></li>
<li class="chapter" data-level="3.3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#count-transformed-data-for-the-number-of-polypeptides-per-class"><i class="fa fa-check"></i><b>3.3.3</b> Count Transformed data for the number of polypeptides per class</a></li>
<li class="chapter" data-level="3.3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visualization-of-transformed-data-descriptive-statistics"><i class="fa fa-check"></i><b>3.3.4</b> Visualization of Transformed Data Descriptive Statistics</a></li>
<li class="chapter" data-level="3.3.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatter-plot-of-means-of-myoglobin-control-amino-acid-composition-sqrt-x_i-transformed-c_m_transformed-dataframe"><i class="fa fa-check"></i><b>3.3.5</b> Scatter plot of means of <em>Myoglobin-Control</em> amino acid composition <span class="math inline">\(\sqrt x_i\)</span> Transformed (c_m_TRANSFORMED) dataframe</a></li>
<li class="chapter" data-level="3.3.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#grouped-bar-chart-of-means-for-percent-amino-acid-composition-of-transformed-data-control-myoglobin-categories"><i class="fa fa-check"></i><b>3.3.6</b> Grouped bar chart of means for percent amino acid composition of Transformed Data; control &amp; myoglobin categories</a></li>
<li class="chapter" data-level="3.3.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-grand-means-of-the-overall-amino-acid-composition-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.7</b> Boxplots of grand-means of the overall amino acid composition of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-control-only-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.8</b> Boxplots of amino acid compositions for control (only) of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-amino-acid-compositions-for-myoglobin-of-square-root-transformed-dataonly-of-square-root-transformed-data"><i class="fa fa-check"></i><b>3.3.9</b> Boxplots of amino acid compositions for myoglobin of square-root transformed Data(only) of square-root transformed data</a></li>
<li class="chapter" data-level="3.3.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots-of-length-of-polypeptides-of-transformed-data-myoglobin-control-combined"><i class="fa fa-check"></i><b>3.3.10</b> Boxplots Of Length Of Polypeptides Of Transformed Data; Myoglobin, Control &amp; Combined</a></li>
<li class="chapter" data-level="3.3.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#coefficient-of-variance-cv-of-transformed-data"><i class="fa fa-check"></i><b>3.3.11</b> Coefficient of variance (CV) Of Transformed data</a></li>
<li class="chapter" data-level="3.3.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-of-coefficient-of-variance-cv"><i class="fa fa-check"></i><b>3.3.12</b> Plot of Coefficient Of Variance (CV)</a></li>
<li class="chapter" data-level="3.3.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness-of-distributions-of-transformed-data"><i class="fa fa-check"></i><b>3.3.13</b> Skewness of distributions Of Transformed Data</a></li>
<li class="chapter" data-level="3.3.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#qq-plots-of-20-amino-acids-of-transformed-data"><i class="fa fa-check"></i><b>3.3.14</b> QQ Plots of 20 amino acids of Transformed data</a></li>
<li class="chapter" data-level="3.3.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#determine-coefficients-of-correlation-of-transformed-data"><i class="fa fa-check"></i><b>3.3.15</b> Determine coefficients of correlation of Transformed Data</a></li>
<li class="chapter" data-level="3.3.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boruta---dimensionality-reduction-of-transformed-data"><i class="fa fa-check"></i><b>3.3.16</b> Boruta - dimensionality reduction of Transformed data</a></li>
<li class="chapter" data-level="3.3.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plot-variable-importance-1"><i class="fa fa-check"></i><b>3.3.17</b> Plot variable importance</a></li>
<li class="chapter" data-level="3.3.18" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#variable-importance-scores-1"><i class="fa fa-check"></i><b>3.3.18</b> Variable importance scores</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#eda-conclusion"><i class="fa fa-check"></i><b>3.4</b> EDA Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><i class="fa fa-check"></i><b>4</b> Principle Component Analysis of Myoglobin/Control Protein Sets</a><ul>
<li class="chapter" data-level="4.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#data-centering-scaling-normalization"><i class="fa fa-check"></i><b>4.2</b> Data centering / scaling / normalization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#histograms-of-scaled-vs.-unscaled-data"><i class="fa fa-check"></i><b>4.2.1</b> Histograms of Scaled Vs. Unscaled data</a></li>
<li class="chapter" data-level="4.2.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#finding-the-covariance-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Finding the Covariance Matrix</a></li>
<li class="chapter" data-level="" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#covariance-of-two-variables"><i class="fa fa-check"></i>Covariance of two variables</a></li>
<li class="chapter" data-level="" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#covariance-of-matrices"><i class="fa fa-check"></i>Covariance of matrices</a></li>
<li class="chapter" data-level="4.2.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#finding-pca-via-singular-value-decomposition"><i class="fa fa-check"></i><b>4.2.3</b> Finding PCA via singular value decomposition</a></li>
<li class="chapter" data-level="4.2.4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#example-of-two-dimensional-pca-using-random-data"><i class="fa fa-check"></i><b>4.2.4</b> Example of two-dimensional PCA using random data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#principle-component-analysis-using-norm_c_m_20aa"><i class="fa fa-check"></i><b>4.3</b> Principle component analysis using <code>norm_c_m_20aa</code></a></li>
<li class="chapter" data-level="4.4" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#screeplot-and-cumulative-proportion-of-variance-plot"><i class="fa fa-check"></i><b>4.4</b> Screeplot and Cumulative Proportion of Variance plot</a></li>
<li class="chapter" data-level="4.5" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplots"><i class="fa fa-check"></i><b>4.5</b> Biplots</a><ul>
<li class="chapter" data-level="4.5.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplot-1-pc1-vs.-pc2-with-class-by-color-labels"><i class="fa fa-check"></i><b>4.5.1</b> Biplot 1: PC1 Vs. PC2 with ‘Class’ by color labels</a></li>
<li class="chapter" data-level="4.5.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#biplot-2-determination-of-4-rule-set-for-outliers"><i class="fa fa-check"></i><b>4.5.2</b> Biplot 2: Determination Of 4 Rule Set For Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#obtain-outliers-from-biplot-2-pc1-vs.-pc2"><i class="fa fa-check"></i><b>4.6</b> Obtain Outliers From Biplot #2: PC1 Vs. PC2</a><ul>
<li class="chapter" data-level="4.6.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-from-principal-component-1"><i class="fa fa-check"></i><b>4.6.1</b> Outliers from Principal Component-1</a></li>
<li class="chapter" data-level="4.6.2" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-from-principal-component-2"><i class="fa fa-check"></i><b>4.6.2</b> Outliers from Principal Component-2</a></li>
<li class="chapter" data-level="4.6.3" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#list-of-all-outliers-union-and-sorted-found-using-the-ruleset-1-through-4"><i class="fa fa-check"></i><b>4.6.3</b> List of all outliers (union and sorted) found using the ruleset 1 through 4</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#pca-conclusions"><i class="fa fa-check"></i><b>4.7</b> PCA Conclusions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="principle-component-analysis-of-myoglobincontrol-protein-sets.html"><a href="principle-component-analysis-of-myoglobincontrol-protein-sets.html#outliers-derived-from-pc1-vs-pc2"><i class="fa fa-check"></i><b>4.7.1</b> Outliers derived from PC1 Vs PC2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression For Binary Classification</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-1-using-20-features"><i class="fa fa-check"></i><b>5.2</b> Logit Training #1 Using 20 Features</a></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-results-1"><i class="fa fa-check"></i><b>5.3</b> Logit Results #1</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-2-using-9-features"><i class="fa fa-check"></i><b>5.4</b> Logit Training #2 Using 9 Features</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-training-2-using-9-features-1"><i class="fa fa-check"></i><b>5.4.1</b> Logit Training #2 Using 9 Features</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-summary-2"><i class="fa fa-check"></i><b>5.5</b> Logit Summary #2</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-confusion-matrix-2"><i class="fa fa-check"></i><b>5.6</b> Logit Confusion Matrix #2</a></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#obtain-list-of-false-positives-false-negatives"><i class="fa fa-check"></i><b>5.7</b> Obtain List of False Positives &amp; False Negatives</a></li>
<li class="chapter" data-level="5.8" data-path="logistic-regression-for-binary-classification.html"><a href="logistic-regression-for-binary-classification.html#logit-conclusion"><i class="fa fa-check"></i><b>5.8</b> Logit Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html"><i class="fa fa-check"></i><b>6</b> Neural Networks For Binary Classification</a><ul>
<li class="chapter" data-level="6.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#the-one-neuron-system"><i class="fa fa-check"></i><b>6.2</b> The One Neuron System</a><ul>
<li class="chapter" data-level="6.2.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#summation-function"><i class="fa fa-check"></i><b>6.2.1</b> Summation Function</a></li>
<li class="chapter" data-level="6.2.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#activation-functions"><i class="fa fa-check"></i><b>6.2.2</b> Activation Functions</a></li>
<li class="chapter" data-level="6.2.3" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#binary-output-or-probability"><i class="fa fa-check"></i><b>6.2.3</b> Binary Output Or Probability</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#the-two-neuron-system"><i class="fa fa-check"></i><b>6.3</b> The Two Neuron System</a><ul>
<li class="chapter" data-level="6.3.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#feed-forward-in-a-two-neuron-network"><i class="fa fa-check"></i><b>6.3.1</b> Feed-Forward In A Two Neuron Network</a></li>
<li class="chapter" data-level="6.3.2" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#error-back-propagation"><i class="fa fa-check"></i><b>6.3.2</b> Error Back-propagation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#neural-network-experiment-for-binary-classification"><i class="fa fa-check"></i><b>6.4</b> Neural Network Experiment For Binary Classification</a><ul>
<li class="chapter" data-level="6.4.1" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#train-model-with-neural-networks"><i class="fa fa-check"></i><b>6.4.1</b> Train model with neural networks</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="neural-networks-for-binary-classification.html"><a href="neural-networks-for-binary-classification.html#neural-network-conclusion"><i class="fa fa-check"></i><b>6.5</b> Neural Network Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>7</b> Appendices</a><ul>
<li class="chapter" data-level="7.1" data-path="appendices.html"><a href="appendices.html#install-r-rstudio"><i class="fa fa-check"></i><b>7.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="7.2" data-path="appendices.html"><a href="appendices.html#load-libraries-used-in-this-project"><i class="fa fa-check"></i><b>7.2</b> Load Libraries Used In This Project</a></li>
<li class="chapter" data-level="7.3" data-path="appendices.html"><a href="appendices.html#calculate-the-amino-acid-compositions-aac-and-di-peptide-compositions-dpc"><i class="fa fa-check"></i><b>7.3</b> Calculate the amino acid compositions (AAC) and Di-peptide compositions (DPC)</a></li>
<li class="chapter" data-level="7.4" data-path="appendices.html"><a href="appendices.html#calculate-aac-and-dpc-values-function"><i class="fa fa-check"></i><b>7.4</b> Calculate AAC and DPC values function</a></li>
<li class="chapter" data-level="7.5" data-path="appendices.html"><a href="appendices.html#run-myoglobin"><i class="fa fa-check"></i><b>7.5</b> Run Myoglobin</a></li>
<li class="chapter" data-level="7.6" data-path="appendices.html"><a href="appendices.html#run-control-human-not-myoglobin"><i class="fa fa-check"></i><b>7.6</b> Run Control / Human-NOT-myoglobin</a></li>
<li class="chapter" data-level="7.7" data-path="appendices.html"><a href="appendices.html#run-controls"><i class="fa fa-check"></i><b>7.7</b> Run Controls</a></li>
<li class="chapter" data-level="7.8" data-path="appendices.html"><a href="appendices.html#keep-aac-only-for-raw-data"><i class="fa fa-check"></i><b>7.8</b> KEEP AAC ONLY FOR RAW DATA</a></li>
<li class="chapter" data-level="7.9" data-path="appendices.html"><a href="appendices.html#transform-c-f-i-from-c_m_raw_aac"><i class="fa fa-check"></i><b>7.9</b> Transform {C, F, I} from c_m_RAW_AAC</a></li>
<li class="chapter" data-level="7.10" data-path="appendices.html"><a href="appendices.html#where-to-find-help"><i class="fa fa-check"></i><b>7.10</b> Where To Find Help</a></li>
<li class="chapter" data-level="7.11" data-path="appendices.html"><a href="appendices.html#machine-setting-session-info"><i class="fa fa-check"></i><b>7.11</b> Machine Setting &amp; Session Info</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Comparison of Binary Classification Using Six Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">2</span> Introduction</h1>
<p>At the intersection between Applied Mathematics, Computer Science, and Biological Sciences is a subset of knowledge known as Bioinformatics. Some find Bioinformatics and its relative Data Science difficult to define. But the most ubiquitous pictograph of Data Science indeed says a thousand words if we cannot. See Figure 1. More generally Data Science is a mixture using biological data analysis, computers, software and importantly statistical or predictive modeling to describe a narrative of some systematic research. For some Bioinformaticians the question is ’How can we use available data? describe it? model it by using applied mathematics? DNA and Proteins are coded strands of information which can be categorized and enumerated in a myriad ways. Other currently popular fields one may come across are the study of chemistry using applied mathematics and computers which beget the field of Chemoinformatics.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> While the career path of Health or Healthcare begets the field of Healthcare-Informatics.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<div class="figure">
<img src="00-data/10-images/Venn-diagram-original-768x432.png" alt="Venn Diagrams of Bioinformatics Vs Data Science" />
<p class="caption">Venn Diagrams of Bioinformatics Vs Data Science</p>
</div>
<p><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div id="what-is-machine-learning" class="section level2">
<h2><span class="header-section-number">2.1</span> What is Machine Learning?</h2>
<blockquote>
<p>“Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions”</p>
<p>— Ian Goodfellow, et al<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
</blockquote>
</div>
<div id="what-is-predictive-modeling" class="section level2">
<h2><span class="header-section-number">2.2</span> What is Predictive Modeling?</h2>
<p>The term ‘Predictive Modeling’ should bring to mind work in the computer science field, also called Machine Learning (ML), Artificial Intelligence (AI), Data Mining, Knowledge discovery in databases (KDD), and possibly even encompassing Big Data as well.</p>
<blockquote>
<p>“Indeed, these associations are appropriate, and the methods implied by these terms are an integral piece of the predictive modeling process. But predictive modeling encompasses much more than the tools and techniques for uncovering patterns within data. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model’s prediction accuracy on future, yet-to-be-seen data.”</p>
<p>— Max Kuhn <a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
</blockquote>
<p>As an aside, I use <code>Predictive Modeling</code> and <code>Machine Learning</code> interchangeably in this document.</p>
<p>In the booklet entitled “The Elements of Data Analytic Style,” <a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> there is an useful checklist for the uninitiated into the realm of science report writing and, indeed, scientific thinking. A shorter, more succinct listing of the steps, which I prefer, and is described by Roger Peng in his book, The Art Of Data Science. The book lists what he describes as the “Epicycle of Analysis.” <a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
</div>
<div id="the-epicycle-of-analysis" class="section level2">
<h2><span class="header-section-number">2.3</span> The Epicycle of Analysis</h2>
<ol style="list-style-type: decimal">
<li>Stating and refining the question</li>
<li>Exploring the data</li>
<li>Building formal statistical models</li>
<li>Interpreting the results</li>
<li>Communicating the results</li>
</ol>
</div>
<div id="predictive-modeling" class="section level2">
<h2><span class="header-section-number">2.4</span> Predictive Modeling</h2>
<p>In general, there are three types of Predictive Modeling or Machine Learning approaches;</p>
<ol style="list-style-type: decimal">
<li>Supervised,</li>
<li>Unsupervised,</li>
<li>Reinforcement.</li>
</ol>
<p>For the sake of this brevity, only Supervised &amp; Unsupervised learning are discussed in this document.</p>
<div id="supervised-learning" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Supervised Learning</h3>
<p>In supervised learning, data consists of observations <span class="math inline">\(X_i\)</span> (where <span class="math inline">\(X\)</span> may be a matrix of values) that also contains a corresponding label, <span class="math inline">\(y_i\)</span>. The label <span class="math inline">\(y\)</span> maybe anyone of <span class="math inline">\(C\)</span> classes. In our case of a binary classifier, we have {‘Is myoglobin’, ‘Is control’}.</p>
<p><strong>Data set</strong>: <span class="math inline">\((X_1, y_1), (X_2 , y_2), ~. . ., ~(X_N , y_N); ~~~y \in \{1, ..., ~C\}\)</span>, where <span class="math inline">\(C\)</span> is the number of classes</p>
<p>A machine learning algorithm determines a pattern from the input information and groups this with its necessary title or classification.</p>
<p>One example might be that we require a machine that separates red widgets from blue widgets. One predictive algorithm is called a K-Nearest Neighbor (K-NN) algorithm. K-NN looks at an unknown object and then proceeds to calculate the distance (most commonly, the euclidean distance) to the <span class="math inline">\(K\)</span> nearest neighbors. If we consider the figure below and choose <span class="math inline">\(K\)</span> = 3, we would find a circumstance as shown. In the dark solid black on the K-Nearest-Neighbor figure, we find that the green widget is nearest to two red widgets and one blue widget. In the voting process, the K-NN algorithm (2 reds vs. 1 blue) means that the consignment of our unknown green object is red.</p>
<p>For the K-NN algorithm to function, the data optimally most be complete with a set of features and a label of each item. Without the corresponding label, a data scientist would need different criteria to track the widgets.</p>
<p>Five of the six algorithms that this report investigates are supervised. Logit, support vector machines, and the neural network that I have chosen require labels for the classification process.</p>
<div class="figure">
<img src="00-data/10-images/K-Nearest-Neighbor.50.png" alt="K-Nearest-Neighbor " />
<p class="caption">K-Nearest-Neighbor </p>
</div>
<p><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<div id="what-is-a-shallow-learner" class="section level4 unnumbered">
<h4>What is a shallow learner?</h4>
<p>Let us investigate the K-NN algorithm and figure a little further. If we change our value of <span class="math inline">\(K\)</span> to 5, then we see a different result. By using <span class="math inline">\(K = 5\)</span>, we consider the out dashed-black line. This more considerable <span class="math inline">\(K\)</span> value contains three blue widgets and two red widgets. If we ask to vote on our choice, we find that 3 blue beats the 2 red, and we assign the unknown a BLUE widget. This assignment is the opposite of the inner circle.</p>
<p>If a researcher were to use K-NN, then the algorithm would have to test many possible <span class="math inline">\(K\)</span> values and compare the results, then choose the <span class="math inline">\(K\)</span> with the highest accuracy. However, this is where K-NN falters. The K-NN algorithm needs to keep all data points used for its initial training (accuracy testing). Any new unknowns could be conceivably tested against any or all the previous data points. The K-NN does use a generalized rule that would make future assignments quick on the contrary. It must memorize all the points for the algorithm to work. K-NN cannot delete the points until it is complete. It is true that the algorithm is simple but not efficient. Matter and fact, as the number of feature dimensions increases, this causes the complexity (also known as Big O) to rise. The complexity of K-NN is <span class="math inline">\(O(K-NN) ~\propto ~nkd\)</span>.</p>
<p>Where <span class="math inline">\(n\)</span> is the number of observations, <span class="math inline">\(k\)</span> is the number of nearest neighbors it must check, and d is the number of dimensions.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>Given that K-NN tends to ‘memorize’ its data to complete its task, it is considered a lazy and shallow learner. Lazy indicates that the decision is left to the moment a new point is learned of predicted. If we were to use a more generalized rule, such as {Blue for (<span class="math inline">\(x &lt;= 5\)</span>)} this would be a more dynamic and more in-depth approach by comparison.</p>
</div>
</div>
<div id="unsupervised-learning" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Unsupervised Learning</h3>
<p>In contrast to the supervised learning system, unsupervised learning does not require a label for it to operate.</p>
<p><strong>Data set</strong>: <span class="math inline">\((X_1), (X_2), ~. . ., ~(X_N)\)</span> where <span class="math inline">\(X\)</span> may represent a matrix (<span class="math inline">\(m\)</span> observations by <span class="math inline">\(n\)</span> features) of values.</p>
<p>Principal Component Analysis is an example of unsupervised learning, which we discuss in more detail in chapter 3. The data, despite or without its labels, are transformed to provide maximization of the variances in the dataset. Yet another objective of Unsupervised learning is to discover “interesting structures”<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> in the data. There are several methods that show structure. These include clustering, knowledge discovery of latent variables, or discovering graph structure. In many instances and as a subheading to the aforementioned points, unsupervised learning can be used for dimension reduction or feature selection.</p>
<p>Among the simplest unsupervised learning algorithms is K-means. K-means does not rely on the class labels of the dataset at all. K-means may be used to determine any number of classes despite any predetermined values. K-means can discover clusters later used in classification or hierarchical feature representation. K-means has several alternative methods but, in general, calculates the distance (or conversely the similarity) of observations to a mean value of the <span class="math inline">\(K\)</span>th grouping. The mean value is called the center of mass, the Physics term that provides an excellent analogy since the center of mass is a weighted average. By choosing a different number of groupings (values of <span class="math inline">\(K\)</span>, much like the K-NN), then comparing the grouping by a measure of accuracy, one example being, mean square error.</p>
<div class="figure">
<img src="00-data/10-images/k-means-2.50.png" alt="K-Means " />
<p class="caption">K-Means </p>
</div>
<p><a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>It is easy to see through much or machine learning or predictive modeling if one understands bits of the inner workings of these algorithms,</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="four-challenges-in-predictive-modeling" class="section level2">
<h2><span class="header-section-number">2.5</span> Four Challenges In Predictive Modeling</h2>
<p>To many predictive modeling is a panacea for all sorts of issues. Although it does show promise, some hurdles need research. Martin Jaggi<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> has summarized four points that elucidate current problems in the field that need research.</p>
<p>Problem 1: The vast majority of information in the world is unlabeled, so it would be advantageous to have a good Unsupervised machine learning algorithms to use.</p>
<p>Problem 2: Algorithms are very specialized, too specific.</p>
<p>Problem 3: Transfer learning to new environments</p>
<p>Problem 4: Scale, the scale of information is vast in reality, and we have computers that work in gigabytes, not the Exabytes that humans may have available to them. The scale of distributed Big Data</p>
<p>The specific predictive models which are executed in this report are discussed in further detail in their own sections.</p>
<p>==========================================</p>
</div>
<div id="section-title" class="section level2">
<h2><span class="header-section-number">2.6</span> SECTION TITLE (??)</h2>
<p>Therefore let us start by posing a question;</p>
<ul>
<li>Is there a correlation between the data points, which are outliers from principal component analysis (PCA), and 6 types of predictive modeling?</li>
</ul>
<p>This experiment is interested in determining if PCA would provide information on the false-positives and false-negatives that were an inevitable part of model building and optimization. The six predictive models that have chosen for this work are Logistic Regression, Support Vector Machines (SVM) (linear, polynomial, and radial basis function kernels), Random Forest, and a Neural Network which uses Auto-encoding.</p>
<p>It is common for Data Scientists to test their data sets for feature importance and feature selection. One test that has interested this researcher is Principal component analysis. It can be a useful tool. PCA is an unsupervised machine learning technique which “reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs.” <a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> However, the results that it provides may not be immediately intuitive to the layperson.</p>
<p>How do the advantages and disadvantages of using PCA compare with other machine learning techniques? The advantages are numerable. They include dimensionality reduction and filtering out noise inherent in the data, and it may preserve the global structure of the data. Does the global and graphical structure of the data produced by the first two principal components provide any insights into how the predictive models of Logistic Regression, Neural Networks utilizing auto-encoders, Support Vector Machines, and Random Forest? In essence, is PCA sufficiently similar to any of the applied mathematics tools of more advanced approaches? Also, this work is to teach me machine learning or predictive modeling techniques.</p>
<p>The data for this study is from the Uniprot database. From the Uniprot database was queried for two protein groups. The first group was Myoglobin, and the second was a control group comprised of human proteins not related to Hemoglobin or Myoglobin. See Figure  There have been a group of papers that are striving to classify types of proteins by their amino acid structure alone. The most straightforward classification procedures involve using the percent amino acid composition (AAC). The AAC is calculated by using the count of an amino acid over the total number in that protein.</p>
<p>Percent Amino Acid Composition:
<span class="math display">\[\begin{equation} 
\%AAC_X ~=~ \frac{N_{Amino~Acid~X}}{Total ~ N ~ of ~ AA}
\end{equation}\]</span></p>
<p>The Exploratory Data Analysis determines if features were skewed and needed must be transformed. In a random system where amino acids were chosen at random, one would expect the percent amino acid composition to be close to 5%. However, this is far from the case for the Myoglobin proteins or the control protein samples.</p>
<div class="figure">
<img src="00-data/10-images/c_m_Mean_AAC.png" alt="Mean % Amino Acid Compositions for Control &amp; Myoglobin " />
<p class="caption">Mean % Amino Acid Compositions for Control &amp; Myoglobin </p>
</div>
<div id="experimental-procedure" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Experimental Procedure</h3>
<p>The experimental procedure is broken into 3 significant steps.</p>
<div id="exploratory-data-analysis-eda" class="section level4">
<h4><span class="header-section-number">2.6.1.1</span> Exploratory Data Analysis (EDA)</h4>
<p>During EDA, the data is checked for irregularities, such as missing data, outliers among features, skewness, and visually for normality using QQ-plots. The only irregularity that posed a significant issue was the skewness of the amino acid features. Many of 20 amino acid features had a significant number of outliers, as seen by Boxplot analysis. However, only three features had skew, which might have presented a problem. Dealing with the skew of the AA was necessary since Principal Component Analysis was a significant aspect of this experiment.</p>
<p>Testing determined earlier that three amino acids (C, F, I) from the single amino acid percent composition needs transformation by using the square root function. The choice of transformations was Natural log, log base 10, squaring (<span class="math inline">\(x^2\)</span>), and using the reciprocal (<span class="math inline">\(1 / x\)</span>) of the values. The square root transformation lowered the skewness to values of less than 1.0 from high points of greater than 2 in all three cases to {-0.102739 <span class="math inline">\(\leq\)</span> skew after transformation <span class="math inline">\(\leq\)</span> 0.3478132}.</p>
<table>
<thead>
<tr class="header">
<th align="left">Amino Acid</th>
<th align="center">Initial skewness</th>
<th align="center">Skew after square root transform</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">C, Cysteine</td>
<td align="center">2.538162</td>
<td align="center">0.347813248</td>
</tr>
<tr class="even">
<td align="left">F, Phenolalanine</td>
<td align="center">2.128118</td>
<td align="center">-0.102739748</td>
</tr>
<tr class="odd">
<td align="left">I, Isoleucine</td>
<td align="center">2.192145</td>
<td align="center">0.293474879</td>
</tr>
</tbody>
</table>
<p>Three transformations take place for this dataset.</p>
<p><code>~/00-data/02-aac_dpc_values/c_m_TRANSFORMED.csv</code> and used throughout the rest of the analysis.</p>
<p>All work uses R<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>, RStudio<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> and a machine learning library/framework <code>caret</code><a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>.</p>
</div>
<div id="caret-library-for-r" class="section level4">
<h4><span class="header-section-number">2.6.1.2</span> Caret library for R</h4>
<p>The R/caret library is attractive to use for many reasons. It currently allows 238 machine learning models that use different options and data structures. <a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> The utility of caret is that it organizes the input and output into a standard format making the need for learning only one grammar and syntax. Caret also harmonizes the use of hyper-parameters. Work becomes reproducible.</p>
</div>
<div id="training-the-predictive-model" class="section level4">
<h4><span class="header-section-number">2.6.1.3</span> Training the Predictive model</h4>
<p>Setting up the training section for caret, for this experiment, can be broken into three parts.</p>
<div id="tuning-hyper-parameters" class="section level5">
<h5><span class="header-section-number">2.6.1.3.1</span> Tuning Hyper-parameters</h5>
<p>The <code>tune.grid</code> command set allows a researcher to experiment by varying the hyper-parameters of the given model to investigate optimum values. Currently, there are no algorithms that allow for the quick and robust tuning of parameters. Instead of searching, a sizable experimental space test searches an n-dimensional grid in a full factorial design if desired.</p>
<p>Although some models have many parameters, the most common one is to search along a cost hyper-parameter.</p>
<blockquote>
<p>The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. <a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
</blockquote>
<p>The cost function (a term derived from business modeling, i.e., optimizing the cost) is an estimate as to how well models predicted value fits from the actual value. A typical cost function is the squared error function.</p>
<p>Example Cost Function: <a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>
<span class="math display">\[\begin{equation} 
Cost ~=~ \left ( y_i - \hat f(x_i) \right )^2
\end{equation}\]</span></p>
<p>It may be important to search the literature to determine if other researchers have used a specific range of optimum value, which may speed a search. For example, C.W. Hsu et al. suggest using a broad range of 20 orders of magnitude of powers of 2,</p>
<p>e.g. <code>cost =</code> {<span class="math inline">\(2^{-5}, 2^{-3}, ..., 2^{15}\)</span>} for an initial gross search then switch to 4 or 5 orders of magnitude with 1/4 log steps.</p>
<p>e.g. <code>cost =</code> {<span class="math inline">\(2^{1}, 2^{1.25}, ..., 2^{5}\)</span>} for a fine search for unknown SVM using a radial basis function. <a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
</div>
<div id="k-fold-cross-validation-of-results" class="section level5">
<h5><span class="header-section-number">2.6.1.3.2</span> k-Fold Cross validation of results</h5>
<p>Another valuable option that caret has is the ability to cross-validate results.</p>
<p>Cross-validation is a statistical method used to estimate the skill of machine learning models.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
<blockquote>
<p>“The samples are randomly partitioned into k sets of roughly equal size. A model is fit using all samples except the first subset (called the first fold). The held-out samples are used for prediction by the recent model. The performance estimate measures the accuracy of the”out of bag&quot; or “held out” samples. The first subset is returned to the training set, and the procedure repeats with the second subset held out, and so on. The k resampled estimates of performance are summarized (usually with the mean and standard error) and used to understand the relationship between the tuning parameter(s) and model utility.&quot; <a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
</blockquote>
<p>Cross-validation has the advantage of using the entire dataset for training and testing, increasing the opportunity that more training samples produce a better model.</p>
<p>Example R/caret code:</p>
<pre><code>## 10 fold Cross Validation repeated 5 times
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,     # Type of Cross-Validation
                           number = 10,               # Number of splits
                           repeats = 5,               # Number of 10 times validations
                           savePredictions = &quot;final&quot;) # Save all predictions found during C.V. testing</code></pre>
</div>
<div id="train-command" class="section level5">
<h5><span class="header-section-number">2.6.1.3.3</span> Train command</h5>
<p>The training command produces an object of the model. The first line should point out the “formula,” which is modeled. The dependent variable is first. The <code>~</code> (Tilda sign) indicates a model is called. Then the desired features can be listed or abbreviated with the all (.) sign.</p>
<p>Example Train command:</p>
<pre><code>model_object &lt;- train(Class ~ .,              # READ: Class is modeled by all features.
                      data = training_set,    # data used
                      trControl = fitControl, # Train control allows Cross Validation setup
                      method = &quot;svmLinear&quot;,   # Use any method from 238 caret utilizes
                      tune.Grid = grid)       # Hyperparameter scouting and exploration</code></pre>
</div>
</div>
<div id="analysis-of-results" class="section level4">
<h4><span class="header-section-number">2.6.1.4</span> Analysis of results</h4>
<p>In binary classification, a two by two contingency table describes predicted versus actual value classifications. This table is also known as a confusion matrix for machine learning students.</p>
<table>
<thead>
<tr class="header">
<th align="center">2 x 2 Confusion Matrix</th>
<th align="center">Actual = 0</th>
<th align="center">Actual = 1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Predicted = 0</td>
<td align="center">True-Negatives</td>
<td align="center">False-Negatives</td>
</tr>
<tr class="even">
<td align="center">Predicted = 1</td>
<td align="center">False-Positives</td>
<td align="center">True-Positives</td>
</tr>
</tbody>
</table>
<p>There are many ways to describe the results further using this confusion matrix. However, Accuracy is used for all comparisons.</p>
<p><span class="math display">\[\begin{equation} 
Accuracy ~=~ \frac{TP + TN}{N_{Total}}
\end{equation}\]</span></p>
<p>The second goal of this experiment is to produce the False Positives and False-Negatives and evaluating these by comparing them to the Principal Component Analysis Biplot of the first two Principal Components.</p>

</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="https://www.acs.org/content/acs/en/careers/college-to-career/chemistry-careers/cheminformatics.html" class="uri">https://www.acs.org/content/acs/en/careers/college-to-career/chemistry-careers/cheminformatics.html</a><a href="introduction.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p><a href="https://jcheminf.biomedcentral.com/" class="uri">https://jcheminf.biomedcentral.com/</a><a href="introduction.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p><a href="https://www.usnews.com/education/best-graduate-schools/articles/2014/03/26/consider-pursuing-a-career-in-health-informatics" class="uri">https://www.usnews.com/education/best-graduate-schools/articles/2014/03/26/consider-pursuing-a-career-in-health-informatics</a><a href="introduction.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p><a href="http://omgenomics.com/what-is-bioinformatics/" class="uri">http://omgenomics.com/what-is-bioinformatics/</a><a href="introduction.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, ‘Deep Learning’, MIT Press, 2016, <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a><a href="introduction.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Max Kuhn, Kjell Johnson, Applied Predictive Modeling, Springer, <a href="ISBN:978-1-4614-6848-6" class="uri">ISBN:978-1-4614-6848-6</a>, 2013<a href="introduction.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Jeff Leek, The Elements of Data Analytic Style, A guide for people who want to analyze data., Leanpub Books, <a href="http://leanpub.com/datastyle" class="uri">http://leanpub.com/datastyle</a>, 2015<a href="introduction.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>Roger D. Peng and Elizabeth Matsui, The Art of Data Science, A Guide for Anyone Who Works with Data, Leanpub Books, <a href="http://leanpub.com/artofdatascience" class="uri">http://leanpub.com/artofdatascience</a>, 2015<a href="introduction.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" class="uri">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a><a href="introduction.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Olga Veksler, Machine Learning in Computer Vision, <a href="http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf" class="uri">http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf</a><a href="introduction.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Kevin Murphy, Machine learning a probabilistic perspective, 2012, ISBN 978-0-262-01802-9<a href="introduction.html#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p><a href="https://www.slideshare.net/teofili/machine-learning-with-apache-hama/20-KMeans_clustering_20" class="uri">https://www.slideshare.net/teofili/machine-learning-with-apache-hama/20-KMeans_clustering_20</a><a href="introduction.html#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p><a href="https://www.machinelearning.ai/machine-learning/4-big-challenges-in-machine-learning-ft-martin-jaggi-2/" class="uri">https://www.machinelearning.ai/machine-learning/4-big-challenges-in-machine-learning-ft-martin-jaggi-2/</a><a href="introduction.html#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>Jake Lever, Martin Krzywinski, Naomi Altman, Principal component analysis, Nature Methods, Vol.14 No.7, July 2017, 641-2<a href="introduction.html#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p><a href="https://cran.r-project.org/" class="uri">https://cran.r-project.org/</a><a href="introduction.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p><a href="https://rstudio.com/" class="uri">https://rstudio.com/</a><a href="introduction.html#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p><a href="http://topepo.github.io/caret/index.html" class="uri">http://topepo.github.io/caret/index.html</a><a href="introduction.html#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p><a href="http://topepo.github.io/caret/available-models.html" class="uri">http://topepo.github.io/caret/available-models.html</a><a href="introduction.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, Deep Learning, MIT Press, <a href="http://www.deeplearningbook.org" class="uri">http://www.deeplearningbook.org</a>, 2016<a href="introduction.html#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>Roberto Battiti and Mauro Brunato, The LION way. Machine Learning-Intelligent Optimization, LIONlab, University of Trento, Italy“, 2017”, <a href="http://intelligent-optimization.org/LIONbook" class="uri">http://intelligent-optimization.org/LIONbook</a><a href="introduction.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>Chih-Wei Hsu, et al., A Practical Guide to Support Vector Classification, 2016, <a href="http://www.csie.ntu.edu.tw/~cjlin" class="uri">http://www.csie.ntu.edu.tw/~cjlin</a><a href="introduction.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p><a href="https://machinelearningmastery.com/k-fold-cross-validation/" class="uri">https://machinelearningmastery.com/k-fold-cross-validation/</a><a href="introduction.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>Max Kuhn, Kjell Johnson, Applied Predictive Modeling, 2013, ISBN 978-1-4614-6848-6<a href="introduction.html#fnref23" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exploratory-data-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
