```{r include=FALSE}
Libraries <- c("knitr")
for (i in Libraries) {
    library(i, character.only = TRUE)
}
opts_chunk$set(cache = TRUE, fig.align = "center")
```

# Abstract

PCA is very useful

# Introduction

At the intersection between Applied Mathematics, Computer Science and an understanding of Biological Sciences is subset of knowledge known as Bioinformatics. Some find Bioinformatics and its more general title Data Science difficult to define. The most ubiquitous pictograph is indeed the intersection of three fields of study. Data Science is simply the overall title when more general domain knowledge is discussed. Other fields you may come across would be, the study of business or marketing begets Business Intelligence. The study of chemistry begets the field Chemoinformatics.[^11]$^,$[^12] Health or Healthcare begets the field Healthcare-Informatics.[^13]

[^11]:https://www.acs.org/content/acs/en/careers/college-to-career/chemistry-careers/cheminformatics.html

[^12]:https://jcheminf.biomedcentral.com/

[^13]:https://www.usnews.com/education/best-graduate-schools/articles/2014/03/26/consider-pursuing-a-career-in-health-informatics


```{r echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, 
                      fig.align = "center", 
                      fig.width = 6, 
                      fig.height = 4,
                      fig.cap = "Venn Diagrams of Bioinformatics Vs Data Science")
knitr::include_graphics("../00-data/10-images/Bio-DS-Venn-diagram2-1024x576.png")
```

What is Predictive Modeling?

The term ‘Predictive Modeling’ should bring to mind work in the computer science field also called Machine Learning (ML), Artificial Intelligence (AI), Data Mining, Knowledge discovery in databases (KDD), and possibly even encompassing Big Data as well.

>Indeed, these associations are appropriate and the methods implied by these terms are an integral piece of the predictive modeling process. But predictive modeling encompasses much more than the tools and techniques for uncovering patterns within data. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model’s prediction accuracy on future, yet-to-be-seen data.[^14]

[^14]:Max Kuhn, Kjell Johnson, Applied Predictive Modeling, Springer, ISBN:978-1-4614-6848-6, 2013

In the booklet entitled, 'The Elements of Data Analytic Style'[^15] there is an interesting checklist for the uninitiated into the realm of science report writing and indeed scientific thinking. A shorter more succinct listing of the steps, which I prefer, and is described by Roger Peng in his book, The Art Of Data Science. The book lists what he describes as the "Epicycle of Analysis."[^16]

[^15]:Jeff Leek, The Elements of Data Analytic Style, A guide for people who want to analyze data., Leanpub Books, http://leanpub.com/datastyle, 2015
 
[^16]:Roger D. Peng and Elizabeth Matsui, The Art of Data Science, A Guide for Anyone Who Works with Data, Leanpub Books, http://leanpub.com/artofdatascience, 2015

## The Epicycle of Analysis

1. Stating and refining the question  
1. Exploring the data  
1. Building formal statistical models  
1. Interpreting the results  
1. Communicating the results  

Therefore let us start by posing a question, Is there a correlation between the data points which are outliers from principal component analysis (PCA) and 6 types of predictive modeling. This experiment is interested in determining if PCA would provide information on the false-positives and false-negatives that were an inevitable part of model building and optimization. The six predictive models that have chosen for this work are Logit, SVM (linear, polynomial, and radial basis function kernels), Random Forest and a Neural Network which uses Auto-encoding.

It is common for Data Scientists to test their data sets for feature importance and feature selection. One test that has interested this researcher is Principal component analysis (PCA). It can be an extremely useful tool. PCA is an unsupervised machine learning technique which “reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs.”[^17] However, the results that it provides may not be immediately intuitive to the lay person.

[^17]:Jake Lever, Martin Krzywinski, Naomi Altman, Principal component analysis, NATURE METHODS, VOL.14 NO.7, JULY 2017, 641-2

How do the advantages and dis-advantages of using PCA compare with other machine learning techniques. The advantages are numerable and include dimensionality reduction, filtering out noise inherent in the data and it may preserve the global structure of the data. Does the global and graphical structure of the data produced by the first two principal components provide any insights into how the predictive models of Logistic Regression, Neural Networks utilizing auto-encoders, Support Vector Machines and Random Forest.  In essence, is PCA sufficiently similar to any of the applied mathematics tools of more advanced approaches. Also, this work is to simply teach myself machine learning or predictive modeling techniques.

The data for this study was derived from the Uniprot database. From the Uniprot database was queried for two protein groups. The first group was Myoglobin and the second was a control group comprised of human proteins not related to Hemoglobin or Myoglobin. There have been a group of papers that are striving to classify types of proteins by their amino acid structure alone. The simplest classification procedures involve using the percent amino acid composition.  This is calculated by using the count of a amino acid over the total number in that protein.

Percent Amino Acid Composition:

$$\%AAC_X ~=~ \frac{N_{Amino~Acid~X}}{Total ~ N ~ of ~ AA}$$

Other approaches include Kuo-Chen pseudo amino acid composition PseAAC.

The experimental approach which was used then did Exploratory Data Analysis to determine if the features needed to be transformed. In a random system where amino acids were chosen at random one would expect the percent amino acid composition to be close to 5%. However this is far from the case for the Myoglobin proteins or the control protein samples.

```{r echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, 
                      fig.align = "center", 
                      fig.width = 5, 
                      fig.height = 5,
                      fig.cap = "Mean % Amino Acid Compositions for Control & Myoglobin")
knitr::include_graphics("../00-data/10-images/c_m_Mean_AAC.png")
```

In general, there are four types of Predictive Modeling or machine learning approaches

1. Supervised,
2. Unsupervised,
3. Reinforcement,
4. Semi-Supervised.

## Supervised Learning

In supervised learning, data consists of observations $X_i$ (where $X$ may be a matrix of values$) that also contains a corresponding label, $y_i$. The label $y may be anyone of $L$ classes. In our case of a binary classifier, we have {‘Is myoglobin’, ‘Is Not myoglobin’}.

Data set: $(X_1, y_1), (X_2 , y_2),  . . .,  ~(X_N , y_N); ~~~y \in \{1, ..., L\}$


A machine learning algorithm determines a pattern from the input information and groups this with its title or classification. 

One example might be that we would like a machine which separates children from adults such that we will not allow children on our roller-coaster. One predictive algorithm called K-Nearest Neighbor (K-NN), looks at its nearest neighbor and takes a vote on what a like item may be like.

```{r echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, 
                      fig.align = "center", 
                      fig.width = 6, 
                      fig.height = 4,
                      fig.cap = "K-Nearest-Neighbor")
knitr::include_graphics("../00-data/10-images/K-Nearest-Neighbor.png")
```


[^2]:Stuart J. Russell, Peter Norvig (2010) Artificial Intelligence: A Modern Approach, Third Edition, Prentice Hall

Cases such as the digit recognition example, in which the aim is to assign each
input vector to one of a finite number of discrete categories, are called classification
problems. If the desired output consists of one or more continuous variables, then
the task is called regression.

Christopher M. Bishop, Pattern Recognition and Machine Learning, ISBN-10: 0-387-31073-8, 2006

========================================================================================

 The technology boils down to the use of mathematics, specifically statistics coupled with computer science.

This booklet will discuss super two of the most commonly studied facets of machine learning, Supervised and Unsupervised learning approaches.
----------------------------------

Background

---------------------------------


3 Steps of ML

1. infer / predict
2. error / loss; sometime aka cost function taken from business
3. train / learn
-----------------------------------------



----------------------------------

- Data mining is all about extracting patterns from an organization's stored data. These patterns can be used to gain insight into aspects of the organization's operations, and to predict outcomes for future situations as an aid to decision-making.

-----------------------------------

Unfortunately, there are no easy rules to follow for which models to use in all situations. The comically named *No Free Lunch Theorems for Optimization*  "demonstrates the danger of comparing [general-purpose] algorithms by their performance on a small sample of problems."[^2]


*caret* is an R language package which allows one to use the same syntax for machine learning libraries written by over 200 different researchers also in the R language.

----------------------------



##### Definitions:

1. **Supervised learning** is used whenever we want to predict a certain
outcome from a given input, and we have examples of input/output pairs.
2. In **unsupervised learning**, the learning algorithm is just shown the input data
and asked to extract knowledge from this data.[^1]

[^1]:Introduction to Machine Learning with Python (2017), Andreas C. Müller & Sarah Guido, O'Reilly press, 2017

A second set of definitions might be helpful although from an different view.

1.

-----------------------------------

- The fundamental goal of machine learning is to generalize beyond the examples in the training set.

- No matter how much data we have, it is very unlikely that we will see those exact examples again at test time.

- Doing well on the training set is easy (just memorize the examples).

- The most common mistake among machine learning beginners is to test on the training data and have the illusion of success. If the chosen classifier is then tested on new data, it is often no better than random guessing.

- HOLD back some data for more testing
- if you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it.

- cross-validation; this is very important
-------------------
1.2.1 Supervised Learning

- dataset is the collection of labeled examples, sometimes called a predictor
- xi among N is called a feature vector
- label yi can be either an element belonging to a finite set of classes {1, 2, . . . , C}, eg. {type1, type2}
- f(x1, x2, x3,...xn)= yi, {(x i , y i )}


1.2.2 Unsupervised Learning

- In unsupervised learning, the dataset is a collection of unlabeled examples {x i } N i=1 .
- For example, in clustering, the model returns the id of the cluster for each feature vector

## Big Questions

- Supervised learning
- Unsupervised learning
- Semi-Reinforcement-learning

''Unsupervised learning:'' As the name suggests, learning from data with no outcome (or supervision) is called unsupervised learning. It is a form of inferential learning based on hidden patterns and intrinsic groups in the given data. Its applications include market pattern recognition, genetic clustering, and so on.

VERY INTERESTING

- Notice that generalization being the goal has an interesting consequence for machine learning. Unlike in most other optimization problems, we don’t have access to the function we want to optimize! We have to use training error as a surrogate for test error.

- Follow up
1. Do you have the right data?
2. Do you need other data?
3. Do you have the right question?

## 4 Big Challenges in Machine Learning (ft. Martin Jaggi)

https://www.youtube.com/watch?v=v3QGgtmAZTE&t=0s&index=10&list=WL

Supervised Vs Unsupervised

Problem 1: The vast majority of information in the world in unlabeled so it would be advantageous to have a good Unsupervised machine learning algorithms to use.

Problem 2: Algorithms are very specialized, too specific.

Problem 3: Transfer learning to new environments

Problem 4: Scale, the scale of information is huge in reality and we have computers that work in gigabytes not the Exabytes that humans may have available to them. The scale of distributed Big Data...

---------------------------------------------------
```{r}
##![CV diagram](CV diagram.png)
```
--------------------------

https://vitalflux.com/top-10-solution-approaches-for-supervised-learning-problems/

-------------------------
- Three elements of a machine learning
model
-------------------------

Model = Representation + Evaluation + Optimization
Domingos, P. A few useful things to know about machine learning.
Commun. ACM. 55(10):7887 (2012).

Explain the following machine learning areas and tasks: supervised learning, unsupervised learning, reinforcement learning, classification, regression, clustering, feature selection, feature extraction, and topic modeling.

### What are the advantages and disadvantages of the various approaches?

### Why to separate training data set and test data set for validation?

Q. How do you get better at formulating questions to ask your AI?
A. Big data

## "Probably approximately correct" learning theory by Leslie Valiant

- Requirements of Learning: Cannot expect a learner to learn a concept exactly. There are only a small fraction of examples that describe our available instance/experimental space. It may be very difficult to describe the entire space with a small set.


- Cannot always expect to learn a close approximation to the target concept, the training set will not be representative of your model. It will may/contain uncommon examples.

- The only realistic expectation of a good learner is that with high probability it will learn a close approximation to the target concept/model.

- The only reason we can hope for this is the *consistent distribution assumption*.

----------------------------------------------
OVERFITTING HAS MANY FACES

- Bias and Variance
- One way to understand over-fitting is by decomposing generalization error into bias and variance. Bias is a learner’s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal.
------------------------------------------------------------


Shallow vs. Deep Learning

GET GOOD DEFINITION!!!


---------------------------------

### Parametric Approach to Dimension Reduction, Feature Reduction/Selection

Forward Selection Method Approach
- Add a new Dimension one at a time then look at goodness of fit and error statistics. if the feature is good keep it else remove it from the model.
- add new dim if error is low and the goodness of fit goes up, could use BIC of AIC.
- good for RF: logistic regression, neural network, SVM.

Backward Selection Method
- one variable at at time is deleted and the model goodness of fit is compared.

if the deletion of a feature reduces error and the goodness of fit increases that then the variable is dropped else it is kept.

- More can be found Book: The comparisons of data mining techniques for the predictive accuracy of probability of default credit card clients, Expert Systems with Application,

Yeh & Lien 2009,  36(2), 2473-2480

Dimensionality Reduction Methods Include:
1. PCA
2. SVD
3. Iterative feature Selection method(s?)

- Dim Reduction Techniques
1. Parametric or model based
  - PCA
  - Forward feature selection
  - Backward feature selection
  - Parametric methods, like iterative feature selection

Too many variables may be detrimental. For example, if there are actually thirty D that effect sales but the company cannot control all of them (such as weather) then it may not be worth trying to model them.

By reducing Dimensions it can also save space and cost.


---------------------------------------
2.7 Classification vs. Regression

- **Classification** is the problem of trying to assign a label to a previously unlabeled example.

- the classification problem is solved by usually using a collection of examples/objects *already* labeled and producing a model that can NOW take unlabeled samples and label them with the information at hand.

- Is the labeled set binary? {0, 1}
or
- Multi-class (3 or more labels or classifications)

- **Regression** is the problem of predicting a real-value label (often called a target) given an unlabeled example.

- The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can *then* take an unlabeled example as a new input and output a target value.

------------------

Classification vs Regression

ML is good for classification of categorization, regression, dimensionality reduction, development of algorithms, discovery of genetic sequences linked to diseases

- Model assumptions
The assumptions underlying nonlinear regression models of the form specified
by Equation (1.3) are:

(1) correct mean function f
(2) variance homogeneity (homoscedasticity)
(3) normally distributed measurements errors
(4) mutually independent measurement errors ε i

1. What is machine learning?

2. Name applications of machine learning.

3. Explain the following machine learning areas and tasks: supervised learning, unsupervised learning, reinforcement learning, classification, regression, clustering, feature selection, feature extraction, and topic modeling.

4. Explain the following machine learning approaches: decision tree learning, artificial neural networks,

5. advantages and disadvantages of the various approaches?

9. Why to separate training data set and test data set for validation?

--------------------------------------

# **5.2 Learning Algorithm Selection**

1. Explainability
1. In-memory vs. out-of-memory
1. Number of features and examples
1. Categorical vs. numerical features
1. Nonlinearity of the data
1. Training speed
1. Prediction speed
-------------------------------------

Max Kuhn • Kjell Johnson
Applied Predictive Modeling

Although there are other R packages that allow one to look at different models at the same time, caret has the broadest range of models that are available. Coming in at 144+.

**Excellent description of splitting, p.160**
----------------------------------------------

## Over-Fitting and Model Tuning


## Choosing Between Models, See P.78

Measuring Performance in Classification Models

------------------------------------


Parameters vs. Hyperparameters

- A hyperparameter is a property of a learning algorithm, usually (but not always) having a numerical value. That value influences the way the algorithm works. Hyperparameters aren’t learned by the algorithm itself from data. They have to be set by the data analyst before running the algorithm.

---------------------------------
SVM

# Support vector machines - SVM

## Description :
* are considered a must try—it offers one of the most robust and accurate methods among all well-known algorithms. It has a sound theoretical foundation, requires only a dozen examples for training, and is insensitive to the number of dimensions. [Top 10 Algos in DM]
* The metric for the concept of the “best” classification function can be realized geometrically.
* [Patrick Winston](https://www.youtube.com/watch?v=_PwhiWxHK8o&t=15s)
* The “soft margin” idea is aimed at extending the SVM algorithm
* [Andrew Ng](https://www.youtube.com/watch?v=hCOIMkcsm_g)


PROCEDURES

CODE start
ctrl <- trainControl(method = 'repeatedcv',
                     repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

gbmTune <- train(churn ~ .,
                 data = churnTrain,
                 metric = "ROC"
                 verbose = FALSE,
                 trControl = ctrl)
# Sets operations to 5 reps of 10X CV.

CODE end

## Expanding the Search Grid
grid <- expand.grid(.interaction.depth = seq(1,7, by=2),
                    .n.trees = seq(100, 1000, by=50),
                    .shrinkage = c(0.01, 0.1))

-------------------------------------------
3 pieces of Advice
- Be Patient
- Be Creative
- Be Practical

## Three Pieces of Advice

You probably spent quite some time reading these chapters and perhaps also following along with the code examples. In the hope that it maximizes the return on this investment and increases the probability that you’ll continue to incorporate the command line into your data science workflow, we would like to offer you three pieces of advice: (1) be patient, (2) be creative, and (3) be practical. In the next three subsections we elaborate on each piece of advice.

### Be Patient

The first piece of advice that we can give is to be patient. Working with data on the command line is different from using a programming language, and therefore it requires a different mindset.

Moreover, the command-line tools themselves are not without their quirks and inconsistencies. This is partly because they have been developed by many different people, over the course of multiple decades. If you ever find yourself at a loss regarding their mind-dazzling options, don’t forget to use --help, man, or your favorite search engine to learn more.

Still, especially in the beginning, it can be a frustrating experience. Trust us, you will become more proficient as you practice using the command line and its tools. The command line has been around for many decades, and will be around for many more to come. It is a worthwhile investment.

### Be Creative

The second, related piece of advice is to be creative. The command line is very flexible. By combining the command-line tools, you can accomplish more than you might think.

We encourage you to not immediately fall back onto your programming language. And when you do have to use a programming language, think about whether the code can be generalized or reused in some way. If so, consider creating your own command-line tool with that code using the steps we discussed in [Chapter 4](#chapter-4-creating-reusable-command-line-tools). If you believe your command-line tool may be beneficial for others, you could even go one step further by making it open source.

### Be Practical

The third piece of advice is to be practical. Being practical is related to being creative, but deserves a separate explanation. In the previous subsection, we mentioned that you should not immediately fall back to a programming language. Of course, the command line has its limits. Throughout the book, we have emphasized that the command line should be regarded as a companion approach to doing data science.

We’ve discussed four steps for doing data science at the command line. In practice, the applicability of the command line is higher for step 1 than it is for step 4. You should use whatever approach works best for the task at hand. And it’s perfectly fine to mix and match approaches at any point in your workflow. The command line is wonderful at being integrated with other approaches, programming languages, and statistical environments. There’s a certain trade-off with each approach, and part of becoming proficient at the command line is to learn when to use which.

In conclusion, when you’re patient, creative, and practical, the command line will make you a more efficient and productive data scientist.



-------------------------

USEFUL ????????????????????
Given an example of star light measured by flux, we would approach the problem by Maximum Likelihood. 
```{r}
# ![](EML for Freq.v.Bayesian.png)
# ![](EML for Freq.v.Bayesian.2.png)
```
When you have multiple measurements you multiple the measurements together. 

Bayesians are interested in the posterior probability

$P(F_{true} | D) = P(D | F_{true})\* P(F_{true}) over P(D)$


------------------------------------------

Key points in a basic EDA:

* **Data types** - categorical variables, numerical variables, nominal
* **Outliers**
* **Missing values**
* Distributions (numerically and graphically) for both, numerical and categorical variables.Fall into patterns????
* (Steps and Key points from: https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/)

- **Assessing data quality**: Correctness and completeness of data is necessary to get the model to work in the way that it is supposed to. This includes checking the consistency, the attribute values and types, the missing values, and finding the outliers.  (Bioinformatics with R Cookbook by Paurush Praveen Sinha)

- **Feature selection**: We need to get the features that show a dependency between the data instance and the objective (for example, class labels in the case of classification). Depending on the data, objective, and learning algorithm, all features might not be required to create a mode; they either add noise, redundancy, or cause computational inefficiency for the model. Therefore, we need to identify useful features that can define the objective function. For example, avoiding a redundant feature or correlated feature is a good call.  (Bioinformatics with R Cookbook by Paurush Praveen Sinha)

Dimensionality reduction — helps to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data.

- Model evaluation

- Some widely used model evaluation parameters for classification models (including cross validation) are as follows:

- Confusion matrix (accuracy, precision, recall, and F1-score)

Confusion Matrix and Statistics

          Reference
Prediction  yes   no
       yes  155    5
       no    69 1438


===============

Tree methods are good if;

1. You have lots of predictor variables and data.

2. Your test set values are within the range of values found within the training set (for both predictors and response). Trees tend to suck at extrapolative predictions although they have some success with panel data.

3. Suitable predictor variables are de-noised (e.g. wavelet transformation, etc).
SVM are likely better than tree methods if;

1. The dataset is smaller with less predictor variables.

2. It's a time series.

3. Extrapolative predictions are needed (opposed to interpolative predictions).

============

https://martin-thoma.com/comparing-classifiers/

================


- 35-free-online-books

https://vitalflux.com/machine-learning-list-of-35-free-online-books/

